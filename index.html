<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://qingfengzhou.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/" class="article-date">
  <time class="dt-published" datetime="2023-07-05T04:28:09.000Z" itemprop="datePublished">2023-07-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/">kyuubi_simple_usage</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#kyuubi-%E6%98%AF%E4%BB%80%E4%B9%88">kyuubi 是什么</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83-%E9%83%A8%E7%BD%B2kyuubi-080-rc2">环境 部署kyuubi-0.8.0-rc2</a><ul>
<li><a href="#compile">Compile</a></li>
<li><a href="#install">install</a><ul>
<li><a href="#%E5%8F%98%E9%87%8F%E8%AE%BE%E7%BD%AE">变量设置</a></li>
<li><a href="#sparkkyuubiauthenticationnone">spark.kyuubi.authentication&#x3D;NONE</a><ul>
<li><a href="#%E9%97%AE%E9%A2%98">问题</a></li>
</ul>
</li>
<li><a href="#sparkkyuubiauthenticationkerberos">spark.kyuubi.authentication&#x3D;kerberos</a></li>
<li><a href="#kerberos-ha">kerberos + ha</a></li>
</ul>
</li>
<li><a href="#test">Test</a><ul>
<li><a href="#connections">Connections</a></li>
<li><a href="#problems">Problems</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%8E%AF%E5%A2%83-%E9%83%A8%E7%BD%B2kyuubi-120">环境 部署kyuubi-1.2.0</a><ul>
<li><a href="#zookeeper">Zookeeper</a></li>
<li><a href="#ha">HA</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#hive">Hive</a></li>
<li><a href="#source">Source</a><ul>
<li><a href="#kyuubiserver">KyuubiServer</a></li>
<li><a href="#sparksqlengine">SparkSQLEngine</a></li>
<li><a href="#servicediscovery">ServiceDiscovery</a></li>
</ul>
</li>
<li><a href="#debug">Debug</a><ul>
<li><a href="#server-debug">server debug</a></li>
<li><a href="#app-debug">App debug</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<!-- tocstop -->

<h2><span id="kyuubi-是什么">kyuubi 是什么</span></h2><p>Kyuubi is a distributed multi-tenant JDBC server for large-scale data processing and analytics, built on top of Apache Spark。</p>
<p>Kyuubi has enhanced the Thrift JDBC&#x2F;ODBC Server in some ways for solving these existing problems, as shown in the following table.</p>
<p>简单的来说，是spark thrift server 的增强版，增加了权限管控、动态资源扩展、多租户等等。</p>
<h2><span id="环境-部署kyuubi-080-rc2">环境 部署kyuubi-0.8.0-rc2</span></h2><h3><span id="compile">Compile</span></h3><p><a target="_blank" rel="noopener" href="https://github.com/yaooqinn/kyuubi/tree/v0.8.0-rc2">Download</a></p>
<p>Tag &gt; v0.8.0-rc2 以后，编译需要spark3.0, </p>
<p>Tag &lt;&#x3D; v0.8.0-rc2 , 编译可在pom.xml文件中指定spark 2.*版本，也可以直接使用<a target="_blank" rel="noopener" href="https://github.com/yaooqinn/kyuubi/releases/tag/v0.8.0-rc2">github realease 包</a></p>
<p>由于生产中使用了spark 2.4, 实际测试编译 使用了<strong>v0.8.0-rc2 版本</strong>进行编译。</p>
<p>.&#x2F;build&#x2F;mvn clean package -DskipTests</p>
<p>.&#x2F;build&#x2F;dist –tgz （.&#x2F;build&#x2F;dist –tgz -Pspark-2.4.0 这里指定了spark 版本，但是没有生效，可在pom文件指定）</p>
<p>编译过程中，出现jar包pentaho-aggdesigner-algorithm&#x2F;5.1.5-jhyde 找不到，可以手动<a target="_blank" rel="noopener" href="http://conjars.org/repo/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/">下载</a>至本地maven仓库中</p>
<h3><span id="install">install</span></h3><h4><span id="变量设置">变量设置</span></h4><p>在 kyuubi_home&#x2F;bin&#x2F;kyuubi-env.sh中， 修改spark_home:</p>
<p>export SPARK_HOME&#x3D;&#x2F;Users&#x2F;Kent&#x2F;Documents&#x2F;spark</p>
<h4><span id="sparkkyuubiauthenticationx3dnone">spark.kyuubi.authentication&#x3D;NONE</span></h4><p>1、无任何认证的情况，启动kyuubi 服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.backend.session.check.interval=1s \</span><br><span class="line">--conf spark.kyuubi.backend.session.idle.timeout=0min</span><br><span class="line"></span><br><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.backend.session.check.interval=5s \</span><br><span class="line">--conf spark.kyuubi.backend.session.idle.timeout=1min</span><br><span class="line"></span><br><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.backend.session.check.interval=1s \</span><br><span class="line">--conf spark.kyuubi.backend.session.idle.timeout=0min</span><br></pre></td></tr></table></figure>



<p>2、使用beeline 连接</p>
<p>cd   $spark_home (spark version &#x3D; spark-2.4.0-bin-hadoop2.7)</p>
<p>bin&#x2F;beeline</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;192.168.1.110:10010</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;10.26.119.144:10010</p>
<p>hive ( or username)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://10.26.119.144:10010/;hive.server2.proxy.user=zhouqingfeng#spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://10.26.119.144:10010/;hive.server2.proxy.user=zhouqingfeng#spark.executor.instances=3;spark.executor.cores=1;spark.executor.memory=512m</span><br></pre></td></tr></table></figure>

<h5><span id="问题">问题</span></h5><p>1、这里hive 的版本是1.2.1，版本太高，貌似也会有连接不上的问题。</p>
<p>2、使用beeline 连接!connect jdbc:hive2:&#x2F;&#x2F;localhost:10010 一直提示连上不上: Connection refused (state&#x3D;08S01,code&#x3D;0)</p>
<p>最后使用命令查看端口 10010，lsof -i tcp:10010，发现服务是正常的， 切换成端口显示的Ip地址后，就可以正常连接了。  !connect jdbc:hive2:&#x2F;&#x2F;localhost:10010  -》  !connect jdbc:hive2:&#x2F;&#x2F;192.168.1.110:10010</p>
<h4><span id="sparkkyuubiauthenticationx3dkerberos">spark.kyuubi.authentication&#x3D;kerberos</span></h4><p>hadoop yarn 开启了kerberos 认证的情况：</p>
<p>–kerberos<br> Kyuubi requires a principal and keytab file specified in $SPARK_HOME&#x2F;conf&#x2F;spark-defaults.conf. </p>
<p>spark.yarn.principal – Kerberos principal for Kyuubi server.<br>spark.yarn.keytab – Keytab for Kyuubi server principal.</p>
<p>可以在 SPARK_HOME&#x2F;conf&#x2F;spark-defaults.conf 增加spark.yarn.principal 和 spark.yarn.keytab两个参数。</p>
<p>也可以，在启动kyuubi 服务的时候 命令配置参数。</p>
<p>1、启动kyuubi 服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--principal    principal1 \</span><br><span class="line">--keytab       Keytab1 \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=KERBEROS</span><br></pre></td></tr></table></figure>



<p>2、使用beeline 连接</p>
<p>cd   $spark_home (spark version &#x3D; spark-2.4.0-bin-hadoop2.7)</p>
<p>bin&#x2F;beeline</p>
<p>!connect  jdbc:hive2:&#x2F;&#x2F;10.20.30.111:10000&#x2F;test_zq;principal&#x3D;hive&#x2F;<a href="mailto:&#98;&#x64;&#x70;&#50;&#64;&#x54;&#69;&#83;&#x54;&#46;&#x43;&#x4f;&#77;">&#98;&#x64;&#x70;&#50;&#64;&#x54;&#69;&#83;&#x54;&#46;&#x43;&#x4f;&#77;</a>;authentication&#x3D;kerberos</p>
<p>hive</p>
<p><strong>beeline 每个连接可以设置不同的spark 参数</strong>：</p>
<p>jdbc:hive2:&#x2F;&#x2F;<host>:<port>&#x2F;;hive.server2.proxy.user&#x3D;tom#spark.yarn.queue&#x3D;theque;spark.executor.instances&#x3D;3;spark.executor.cores&#x3D;3;spark.executor.memory&#x3D;10g</port></host></p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;   -n hive </p>
<h4><span id="kerberos-ha">kerberos + ha</span></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--num-executors 3 \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--principal    principal1 \</span><br><span class="line">--keytab       Keytab1 \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=KERBEROS \</span><br><span class="line">--conf spark.kyuubi.ha.enabled=true \</span><br><span class="line">--conf spark.kyuubi.ha.zk.quorum=zk1:port1,zk2:port2 \</span><br><span class="line">--conf spark.kyuubi.ha.zk.namespace=kyuubiserver \</span><br><span class="line">--conf spark.kyuubi.ha.mode=load-balance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.ha.enabled=true \</span><br><span class="line">--conf spark.kyuubi.ha.zk.quorum=10.20.145.31:2181 \</span><br><span class="line">--conf spark.kyuubi.ha.zk.namespace=kyuubiserver \</span><br><span class="line">--conf spark.kyuubi.ha.mode=load-balance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">./bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=y</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>jdbc url</strong></p>
<p>jdbc:hive2:&#x2F;&#x2F;zk1:port1,zk2:port2&#x2F;test_zq;serviceDiscoveryMode&#x3D;zooKeeper;zooKeeperNamespace&#x3D;kyuubiserver;principal&#x3D;hive&#x2F;<a href="mailto:&#98;&#x64;&#112;&#x32;&#64;&#x54;&#69;&#x53;&#84;&#x2e;&#x43;&#x4f;&#77;">&#98;&#x64;&#112;&#x32;&#64;&#x54;&#69;&#x53;&#84;&#x2e;&#x43;&#x4f;&#77;</a>;authentication&#x3D;kerberos</p>
<p>username: hive or others</p>
<h3><span id="test">Test</span></h3><h4><span id="connections">Connections</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://10.20.30.111:10010/;hive.server2.proxy.user=tom#spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://10.20.30.111:10010/;hive.server2.proxy.user=tom#spark.executor.instances=4;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://10.20.30.111:10010/;hive.server2.proxy.user=bob#spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br></pre></td></tr></table></figure>



<h4><span id="problems">Problems</span></h4><p>修改同一个用户提交多个作业并发测试的问题：问题定位是SparkEnv导致，多个线程混用了同一个SparkEnv</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 3, bdp-1.rdc.com, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)</span><br><span class="line">        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:89)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)</span><br><span class="line">        at org.apache.spark.scheduler.Task.run(Task.scala:121)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at scala.collection.immutable.List.foreach(List.scala:392)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)</span><br><span class="line">        at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)</span><br><span class="line">        ... 14 more</span><br><span class="line"></span><br><span class="line">Driver stacktrace:</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)</span><br><span class="line">        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">        at scala.Option.foreach(Option.scala:257)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)</span><br><span class="line">        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)</span><br><span class="line">        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)</span><br><span class="line">        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)</span><br><span class="line">        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)</span><br><span class="line">        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)</span><br><span class="line">        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">        at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">        at org.apache.spark.rdd.RDD.collect(RDD.scala:944)</span><br><span class="line">        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)</span><br><span class="line">        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)</span><br><span class="line">        at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2783)</span><br><span class="line">        at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2783)</span><br><span class="line">        at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)</span><br><span class="line">        at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)</span><br><span class="line">        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)</span><br><span class="line">        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)</span><br><span class="line">        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)</span><br><span class="line">        at org.apache.spark.sql.Dataset.collect(Dataset.scala:2783)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementInClientMode.execute(ExecuteStatementInClientMode.scala:184)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1$$anon$2.run(ExecuteStatementOperation.scala:74)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1$$anon$2.run(ExecuteStatementOperation.scala:70)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1.run(ExecuteStatementOperation.scala:70)</span><br><span class="line">        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">        at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)</span><br><span class="line">        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:89)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)</span><br><span class="line">        at org.apache.spark.scheduler.Task.run(Task.scala:121)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)</span><br><span class="line">        ... 3 more</span><br><span class="line">Caused by: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at scala.collection.immutable.List.foreach(List.scala:392)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)</span><br><span class="line">        at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="环境-部署kyuubi-120">环境 部署kyuubi-1.2.0</span></h2><p>kyuubi-1.2.0-bin-spark-3.0-hadoop2.7</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">bin/kyuubi start</span><br><span class="line">bin/kyuubi stop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/kyuubi run (print in the foreground)</span><br><span class="line"></span><br><span class="line">jdbc:hive2://localhost:10009/</span><br><span class="line"></span><br><span class="line">beeline</span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://localhost:10009/;#spark.master=yarn;spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line">bin/beeline -u &#x27;jdbc:hive2://localhost:10009/&#x27; -n zhouqingfeng</span><br><span class="line"></span><br><span class="line">//不支持在单个任务上配置kerberos | spark.kerberos.keytab and spark.kerberos.principal should not //use now.</span><br><span class="line"></span><br><span class="line">kinit -kt /home/demo/hive.keytab  spark/hd137@TEST.COM</span><br><span class="line"></span><br><span class="line">//submit to yarn</span><br><span class="line"></span><br><span class="line">jdbc:hive2://localhost:10009/;hive.server2.proxy.user=zhouqingfeng#spark.master=yarn;spark.submit.deployMode=client;spark.executor.instances=1;spark.executor.cores=1;spark.executor.memory=512m;spark.executor.heartbeatInterval=1000s;spark.network.timeout=1001s&quot;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="zookeeper">Zookeeper</span></h3><p><strong>第一步</strong></p>
<p>找到thrift jdbc 连接地址：connection url (可配置多个KyuubiServer，对应多个jdbc url实现服务的Ha)</p>
<p>ls &#x2F;kyuubi</p>
<p>[serviceUri&#x3D;localhost:10009;version&#x3D;1.2.0;sequence&#x3D;0000000000]</p>
<p><strong>第二步:</strong></p>
<p>找到user对应的spark_submit应用的监听端口，user对应的sql 会由第一步转交给该spark_submit应用处理</p>
<p>ls &#x2F;kyuubi_USER&#x2F;zhouqingfeng</p>
<p>[serviceUri&#x3D;localhost:57486;version&#x3D;1.2.0;sequence&#x3D;0000000001]</p>
<h3><span id="ha">HA</span></h3><p>jdbc:hive2:&#x2F;&#x2F;zk1:port1,zk2:port2&#x2F;test_zq;serviceDiscoveryMode&#x3D;zooKeeper;zooKeeperNamespace&#x3D;kyuubiserver;principal&#x3D;hive&#x2F;<a href="mailto:&#x62;&#100;&#112;&#x32;&#x40;&#x54;&#69;&#83;&#84;&#x2e;&#67;&#x4f;&#77;">&#x62;&#100;&#112;&#x32;&#x40;&#x54;&#69;&#83;&#84;&#x2e;&#67;&#x4f;&#77;</a>;authentication&#x3D;kerberos</p>
<p>kyuubi.ha.enabled</p>
<p>kyuubi.ha.zookeeper.quorum</p>
<p>kyuubi.ha.zookeeper.client.port</p>
<p>kyuubi.ha.zookeeper.namespace</p>
<p>–conf spark.kyuubi.ha.enabled&#x3D;true <br>–conf spark.kyuubi.ha.zk.quorum&#x3D;zk1:port1,zk2:port2 <br>–conf spark.kyuubi.ha.zk.namespace&#x3D;kyuubiserver \</p>
<p>–conf spark.kyuubi.ha.enabled&#x3D;true \</p>
<h3><span id="configuration">Configuration</span></h3><p>$SPARK_HOME&#x2F;conf  -&gt; $Kyuubi_HOME&#x2F;conf -&gt; jdbc url  (low -&gt; high)</p>
<p> <strong>kyuubi.session.engine.idle.timeout</strong></p>
<p>engine timeout, the engine will self-terminate when it’s not accessed for this duration</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kyuubi.engine.share.level  //CONNECTION  USER  SERVER</span><br><span class="line"></span><br><span class="line">Engines will be shared in different levels, available configs are:</span><br><span class="line">CONNECTION: engine will not be shared but only used by the current client connection</span><br><span class="line">USER: engine will be shared by all sessions created by a unique username, see also kyuubi.engine.share.level.sub.domain</span><br><span class="line">SERVER: the App will be shared by Kyuubi servers</span><br></pre></td></tr></table></figure>

<h3><span id="hive">Hive</span></h3><p>For example, Spark 3.0 was released with a builtin Hive client (2.3.7), so, ideally, the version of server should &gt;&#x3D; 2.3.x.</p>
<p>To prevent this problem, we can use Spark’s <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html#interacting-with-different-versions-of-hive-metastore">Interacting with Different Versions of Hive Metastore</a></p>
<h3><span id="source">Source</span></h3><h4><span id="kyuubiserver">KyuubiServer</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">KyuubiServer  -&gt; KyuubiBackendService + FrontendService</span><br><span class="line"></span><br><span class="line">1、KinitAuxiliaryService   -&gt; kyuubiserver kerberos 认证</span><br><span class="line">2、MetricsSystem  -&gt; 指标收集</span><br><span class="line">3、KyuubiBackendService -&gt; kyuubi 后端</span><br><span class="line">4、FrontendService  -&gt; kyuubi前端接收用户jdbc connection (TServerSocket)</span><br><span class="line">5、KyuubiServiceDiscovery -&gt; A service for service discovery used by kyuubi server side.</span><br><span class="line">(创建启动zkclient, 与zkserver 建立连接， 创建节点对应kyuubi服务)</span><br><span class="line"></span><br><span class="line">(1)</span><br><span class="line">21/07/13 14:23:00 INFO service.KinitAuxiliaryService: Service[KinitAuxiliaryService] is initialized.</span><br><span class="line">(2)</span><br><span class="line">21/07/13 14:30:04 INFO metrics.JsonReporterService: Service[JsonReporterService] is initialized.</span><br><span class="line">21/07/13 14:41:19 INFO metrics.MetricsSystem: Service[MetricsSystem] is initialized.</span><br><span class="line">(3)</span><br><span class="line">KyuubiBackendService</span><br><span class="line"></span><br><span class="line">-&gt; AbstractBackendService -&gt; KyuubiSessionManager -&gt; KyuubiOperationManager</span><br><span class="line"></span><br><span class="line">backend.server.exec.pool.size</span><br><span class="line">Number of threads in the operation execution thread pool of Kyuubi server</span><br><span class="line"></span><br><span class="line">backend.engine.exec.pool.size</span><br><span class="line">Number of threads in the operation execution thread pool of SQL engine applications</span><br><span class="line"></span><br><span class="line">initialized：</span><br><span class="line">21/07/13 15:00:54 INFO util.ThreadUtils: KyuubiSessionManager-exec-pool: pool size: 100, wait queue size: 100, thread keepalive time: 60000 m</span><br><span class="line">21/07/13 15:05:22 INFO operation.KyuubiOperationManager: Service[KyuubiOperationManager] is initialized</span><br><span class="line">21/07/13 15:06:30 INFO session.KyuubiSessionManager: Service[KyuubiSessionManager] is initialized.</span><br><span class="line">21/07/13 15:09:07 INFO server.KyuubiBackendService: Service[KyuubiBackendService] is initialized.</span><br><span class="line">(4) 21/07/13 15:23:18 INFO service.FrontendService: Initializing FrontendService on host localhost at port 10009 with [9, 999] worker threads</span><br><span class="line">21/07/13 15:23:52 INFO service.FrontendService: Service[FrontendService] is initialized.</span><br><span class="line">(5)</span><br><span class="line">21/07/13 15:45:26 INFO client.KyuubiServiceDiscovery: Service[ServiceDiscovery] is initialized.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">started:</span><br><span class="line">21/07/13 15:46:42 INFO service.KinitAuxiliaryService: Service[KinitAuxiliaryService] is started.</span><br><span class="line">21/07/13 15:59:33 INFO metrics.JsonReporterService: Service[JsonReporterService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:06:57 INFO operation.KyuubiOperationManager: Service[KyuubiOperationManager] is started.</span><br><span class="line">21/07/13 16:07:13 INFO session.KyuubiSessionManager: Service[KyuubiSessionManager] is started.</span><br><span class="line">21/07/13 16:07:13 INFO server.KyuubiBackendService: Service[KyuubiBackendService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:19:14 INFO service.FrontendService: Service[FrontendService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:23:48 INFO client.ServiceDiscovery: Created a /kyuubi/serviceUri=localhost:10009;version=1.2.0;sequence=0000000000 on ZooKeeper for KyuubiServer uri: localhost:10009</span><br><span class="line">21/07/13 16:24:41 INFO client.KyuubiServiceDiscovery: Service[ServiceDiscovery] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:25:27 INFO server.KyuubiServer: Service[KyuubiServer] is started.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="sparksqlengine">SparkSQLEngine</span></h4><p>提交jar到集群，获取资源 -&gt; 初始化服务  -&gt; 启动服务(FrontendService 对外暴露的地址注册到zk)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">SparkSQLEngine -&gt; SparkSQLBackendService + FrontendService + EngineServiceDiscovery</span><br><span class="line">(命令行独立进程里先提交任务， 再启动driver端上述三个服务)</span><br><span class="line"></span><br><span class="line">21/07/13 20:26:15 INFO ThreadUtils: SparkSQLSessionManager-exec-pool: pool size: 100, wait queue size: 100, thread keepalive time: 60000 ms</span><br><span class="line">21/07/13 20:27:22 INFO SparkSQLOperationManager: Service[SparkSQLOperationManager] is initialized.</span><br><span class="line">21/07/13 20:27:24 INFO SparkSQLSessionManager: Service[SparkSQLSessionManager] is initialized.</span><br><span class="line">21/07/13 20:27:25 INFO SparkSQLBackendService: Service[SparkSQLBackendService] is initialized.</span><br><span class="line">21/07/13 20:27:39 INFO FrontendService: Initializing FrontendService on host localhost at port 50764 with [9, 999] worker threads</span><br><span class="line">21/07/13 20:27:59 INFO FrontendService: Service[FrontendService] is initialized.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">21/07/13 20:28:29 INFO EngineServiceDiscovery: Service[ServiceDiscovery] is initialized.</span><br><span class="line">21/07/13 20:28:37 INFO SparkSQLEngine: Service[SparkSQLEngine] is initialized.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">21/07/13 20:34:33 INFO SparkSQLOperationManager: Service[SparkSQLOperationManager] is started.</span><br><span class="line">21/07/13 20:34:33 INFO SparkSQLSessionManager: Service[SparkSQLSessionManager] is started.</span><br><span class="line">21/07/13 20:34:33 INFO SparkSQLBackendService: Service[SparkSQLBackendService] is started.</span><br><span class="line">21/07/13 20:34:36 INFO FrontendService: Service[FrontendService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 20:35:12 INFO EngineServiceDiscovery: Zookeeper client connection state changed to: RECONNECTED</span><br><span class="line">21/07/13 20:35:12 INFO ServiceDiscovery: Created a /kyuubi_USER/zhouqingfeng/serviceUri=localhost:50764;version=1.2.0;sequence=0000000000 on ZooKeeper for KyuubiServer uri: localhost:50764</span><br><span class="line"></span><br><span class="line">21/07/13 20:35:12 INFO EngineServiceDiscovery: Service[ServiceDiscovery] is started.</span><br><span class="line">21/07/13 20:35:12 INFO SparkSQLEngine: Service[SparkSQLEngine] is started.</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SparkProcessBuilder</span><br></pre></td></tr></table></figure>



<h4><span id="servicediscovery">ServiceDiscovery</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ServiceDiscovery  服务发现</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">KyuubiServiceDiscovery</span><br><span class="line">A service for service discovery used by kyuubi server side. 找到kyuubiserver(默认10009端口)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EngineServiceDiscovery   找到对应的spark-submit 任务 (地址+端口监听spark服务)</span><br><span class="line">A service for service discovery used by engine side. </span><br></pre></td></tr></table></figure>



<h3><span id="debug">Debug</span></h3><p>（<a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/guides/jpda/conninv.html#Transports">jpda调试工具</a>）</p>
<h4><span id="server-debug">server debug</span></h4><p>Kyuubi 启动脚本kyuubi添加：</p>
<p>-Xdebug -Xrunjdwp:transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5006</p>
<p>cmd&#x3D;”${RUNNER}  -Xdebug -Xrunjdwp:transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5006 ${KYUUBI_JAVA_OPTS}  -cp ${KYUUBI_CLASSPATH} $CLASS”</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5006</p>
<p>启动server,  debug idea remote, ok!</p>
<h4><span id="app-debug">App debug</span></h4><p>change kyuubi-defaults.conf.template to  kyuubi-defaults.conf:</p>
<p>kyuubi-defaults.conf add:</p>
<p>spark.driver.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>spark.executor.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5007</p>
<p>正常启动server,</p>
<p>发起一个jdbc 连接，</p>
<p>debug idea remote, ok!</p>
<h2><span id="references">References</span></h2><p><a target="_blank" rel="noopener" href="https://github.com/yaooqinn/kyuubi/tree/master">Github</a></p>
<p><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">spark docs</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1440251">如何在CDH5中使用Spark2.4 Thrift</a></p>
<p><a target="_blank" rel="noopener" href="https://yaooqinn.github.io/kyuubi/docs/authentication.html">kyuubi 文档 适用与0.8版本(包括) 以前</a></p>
<p><a target="_blank" rel="noopener" href="https://kyuubi.readthedocs.io/en/stable/quick_start/quick_start.html#installation">kyuubi文档 使用于spark3.0 以后</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/354654936">知乎作者总结</a></p>
<p><a target="_blank" rel="noopener" href="https://kyuubi.readthedocs.io/en/stable/tools/debugging.html">1.* 文档</a></p>
<p><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties">Spark official doc</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/" data-id="cljp81lem0000239afmc93krp" data-title="kyuubi_simple_usage" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/spark3学习笔记20230704" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T03:08:56.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/">spark3学习笔记20230704</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#example">Example</a><ul>
<li><a href="#wordcount">wordcount</a></li>
<li><a href="#sparklauncher">SparkLauncher</a></li>
</ul>
</li>
<li><a href="#dataset">DataSet</a><ul>
<li><a href="#dataset-api">Dataset API</a><ul>
<li><a href="#common">Common</a></li>
<li><a href="#cube-%E6%95%B0%E6%8D%AE%E7%AB%8B%E6%96%B9">Cube 数据立方</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sparksql">SparkSql</a><ul>
<li><a href="#group-by-%E5%88%86%E7%BB%84">Group by 分组</a><ul>
<li><a href="#cube%E7%94%A8%E6%B3%95">cube用法</a></li>
</ul>
</li>
<li><a href="#orgapachesparksqlfunctions">org.apache.spark.sql.functions</a><ul>
<li><a href="#collect_list">collect_list</a></li>
<li><a href="#min">min</a></li>
<li><a href="#percentile_approx">percentile_approx</a></li>
<li><a href="#kurtosis">kurtosis</a></li>
<li><a href="#product">Product</a></li>
<li><a href="#skewness">skewness</a></li>
<li><a href="#stddev_pop">stddev_pop</a></li>
<li><a href="#stddev">stddev</a></li>
<li><a href="#var_pop">var_pop</a></li>
<li><a href="#var_samp">var_samp</a></li>
<li><a href="#variance">variance</a></li>
<li><a href="#collection">Collection</a></li>
<li><a href="#%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0">日期函数</a></li>
<li><a href="#others">Others</a></li>
<li><a href="#%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%87%BD%E6%95%B0">字符串函数</a></li>
<li><a href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0">窗口函数</a></li>
<li><a href="#scalar-function">Scalar Function</a><ul>
<li><a href="#udf">UDF</a></li>
</ul>
</li>
<li><a href="#aggregate-function">Aggregate Function</a><ul>
<li><a href="#udaf">UDAF</a></li>
</ul>
</li>
<li><a href="#hive-udf">Hive UDF</a><ul>
<li><a href="#spark-%E5%AE%98%E6%96%B9hive-udf-example">Spark 官方hive udf Example</a></li>
<li><a href="#hive-udf-%E5%9C%A8hive%E4%B8%AD%E4%BD%BF%E7%94%A8">Hive UDF 在Hive中使用</a></li>
<li><a href="#hive-udf-%E5%9C%A8spark-%E4%B8%AD%E4%BD%BF%E7%94%A8">HIVE UDF 在Spark 中使用</a></li>
<li><a href="#hive-udf%E5%85%B6%E4%BB%96%E6%96%87%E6%A1%A3">HIVE UDF其他文档</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#dataset-%E7%94%9F%E6%88%90">Dataset 生成</a><ul>
<li><a href="#%E9%9B%86%E5%90%88%E7%94%9F%E6%88%90dataset">集合生成Dataset</a></li>
<li><a href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90dataset">加载数据集生成Dataset</a></li>
<li><a href="#rdd-%E7%94%9F%E6%88%90dataset">RDD 生成Dataset</a><ul>
<li><a href="#inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</a></li>
<li><a href="#%E7%BC%96%E7%A0%81%E6%8C%87%E5%AE%9Aschema">编码指定schema</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-%E6%95%B0%E6%8D%AE%E6%BA%90">Spark 数据源</a><ul>
<li><a href="#bucket-%E5%92%8C-partition">Bucket 和 Partition</a><ul>
<li><a href="#%E5%88%86%E6%A1%B6%E5%88%92%E5%88%86">分桶划分</a></li>
<li><a href="#%E5%88%86%E6%A1%B6%E6%B5%8B%E8%AF%95">分桶测试</a></li>
<li><a href="#%E5%88%86%E6%A1%B6%E7%9B%AE%E7%9A%84">分桶目的</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="example">Example</span></h2><h3><span id="wordcount">wordcount</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val textFile = spark.read.textFile(&quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/README.md&quot;)</span><br><span class="line"></span><br><span class="line">//    textFile.show()</span><br><span class="line">//    textFile.count()</span><br><span class="line">//    textFile.first()</span><br><span class="line">    //word最多的行有多少个word</span><br><span class="line">//    textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; if (a &gt; b) a else b)</span><br><span class="line">    //identity : scala predef预定义的一个函数，返回值等于传入参数</span><br><span class="line">    val wordCounts = textFile</span><br><span class="line">            .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">            .groupByKey(identity)</span><br><span class="line">            .count()</span><br><span class="line">            .selectExpr(&quot;key as value&quot;, &quot;`count(1)` as count&quot;)</span><br><span class="line"></span><br><span class="line">    wordCounts.printSchema()</span><br><span class="line">    wordCounts.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /* wordCounts2 等价于 上面的 wordCounts，</span><br><span class="line">       等价于sql中的 select value, count(1) count from tb1 group by value</span><br><span class="line">       等价于mapreduce中的wordcount</span><br><span class="line"></span><br><span class="line">    val wordCounts2 = textFile</span><br><span class="line">      .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">      .groupBy(&quot;value&quot;)</span><br><span class="line">      .count()</span><br><span class="line"></span><br><span class="line">    wordCounts2.show()</span><br><span class="line">     */</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="sparklauncher">SparkLauncher</span></h3><p>java代码里提交spark任务到指定集群，</p>
<p>（官方说法：The <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/launcher/package-summary.html">org.apache.spark.launcher</a> package provides classes for launching Spark jobs as child processes using a simple Java API.）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.launcher.SparkAppHandle;</span><br><span class="line">import org.apache.spark.launcher.SparkLauncher;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line"></span><br><span class="line">public class TestSparkLauncher &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; envParams = new HashMap&lt;&gt;();</span><br><span class="line">//        envParams.put(&quot;YARN_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">//        envParams.put(&quot;HADOOP_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_HOME&quot;, &quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;, &quot;1&quot;);</span><br><span class="line">//        envParams.put(&quot;JAVA_HOME&quot;, &quot;/usr/java&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SparkLauncher launcher = new SparkLauncher(envParams)</span><br><span class="line">                .setAppResource(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;)</span><br><span class="line">                .setMainClass(&quot;org.apache.spark.examples.SparkPi&quot;)</span><br><span class="line">                .setMaster(&quot;local[2]&quot;)</span><br><span class="line">                .setDeployMode(&quot;client&quot;)</span><br><span class="line">                .addAppArgs(&quot;20&quot;);</span><br><span class="line"></span><br><span class="line">        String appName = &quot;sparklancherTest&quot;;</span><br><span class="line">        launcher.setConf(&quot;spark.executor.memory&quot;, &quot;1g&quot;);</span><br><span class="line">        launcher.setAppName( appName );</span><br><span class="line"></span><br><span class="line">//        launcher.addJar(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;);</span><br><span class="line">//        launcher.redirectOutput(new File(&quot;/Users/zhouqingfeng/Desktop/mydirect/data/spark/redir/test1&quot;) );</span><br><span class="line"></span><br><span class="line">        SparkAppHandle sparkHandle = launcher.startApplication();</span><br><span class="line"></span><br><span class="line">        while(!sparkHandle.getState().isFinal()) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(1000);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="dataset">DataSet</span></h2><p>dataframe是dataset的一种特殊形式，所有元素类型被泛化为row（untyped）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type DataFrame = Dataset[org.apache.spark.sql.Row]</span><br></pre></td></tr></table></figure>





<h3><span id="dataset-api">Dataset API</span></h3><h4><span id="common">Common</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetUsageExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df  = spark.read.json(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">    val ds1 = df.as[Person]</span><br><span class="line"></span><br><span class="line">    ds1.cache()</span><br><span class="line"></span><br><span class="line">//    ds1.checkpoint()</span><br><span class="line">    ds1.explain(true)</span><br><span class="line"></span><br><span class="line">    val ds2 = ds1.where(&quot;name = &#x27;Andy&#x27;&quot;)</span><br><span class="line">    //hint 提示词</span><br><span class="line">    ds1.join(ds2.hint(&quot;broadcast&quot;))</span><br><span class="line"></span><br><span class="line">    //ds1.repartition($&quot;age&quot;).write.save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/out2/&quot;)</span><br><span class="line"></span><br><span class="line">    //添加一列</span><br><span class="line">    ds1.withColumn(&quot;age1&quot;, $&quot;age&quot; + 1 ).show</span><br><span class="line"></span><br><span class="line">    //重命名</span><br><span class="line">    ds1.select($&quot;name&quot;.as(&quot;name1&quot;))</span><br><span class="line"></span><br><span class="line">    //删除一列</span><br><span class="line">    ds1.withColumn(&quot;age1&quot;, $&quot;age&quot; + 1).drop(&quot;age&quot;)</span><br><span class="line"></span><br><span class="line">    //join</span><br><span class="line">    val ds3 = ds1</span><br><span class="line">    ds1.join(ds3, ds1.col(&quot;age&quot;) === ds3.col(&quot;age&quot;) &amp;&amp; ds1.col(&quot;name&quot;) === ds3.col(&quot;name&quot;), &quot;inner&quot;)</span><br><span class="line">       .show()</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    ds1.agg( max($&quot;age&quot;) ).show()</span><br><span class="line"></span><br><span class="line">    //statistics for numeric and string columns</span><br><span class="line">    ds1.describe(&quot;age&quot;)</span><br><span class="line">    ds1.summary()</span><br><span class="line"></span><br><span class="line">    // 替换字段中的null为11</span><br><span class="line">    ds1.na.fill(11).show</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="cube-数据立方">Cube 数据立方</span></h4><p>Cube  + rollup + pivot透视</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetCubeExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val df1 = Seq((&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)</span><br><span class="line">                 ).toDF(&quot;dept&quot;, &quot;sex&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line">    //rollup 分组,  这里等价于 group by 1 (不分组) + group by dept + group by dept, sex,  agg指明分组后的聚合函数</span><br><span class="line">    df1.rollup(&quot;dept&quot;, &quot;sex&quot;).agg(sum(&quot;salary&quot;).as(&quot;salary_sum&quot;)).show(20)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    分组聚合结果如下</span><br><span class="line">    +----+----+----------+</span><br><span class="line">    |dept| sex|salary_sum|</span><br><span class="line">    +----+----+----------+</span><br><span class="line">    |sale|null|       180|</span><br><span class="line">    |sale|   m|        85|</span><br><span class="line">    | dev|   m|       232|</span><br><span class="line">    | dev|null|       349|</span><br><span class="line">    |null|null|       529|</span><br><span class="line">    |sale|   f|        95|</span><br><span class="line">    | dev|   f|       117|</span><br><span class="line">    +----+----+----------+</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    //cube 分组,  表示取dept和sex两个维度，任意维度组合 分组</span><br><span class="line">    // 这里等价于 group by 1 (不分组) + group by dept + group by sex + group by dept, sex</span><br><span class="line">    df1.cube(&quot;dept&quot;, &quot;sex&quot;).agg(&quot;salary&quot; -&gt; &quot;sum&quot;).show(20)</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    分组聚合结果如下</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">    |dept| sex|sum(salary)|</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">    |sale|null|        180|</span><br><span class="line">    |null|   m|        317|</span><br><span class="line">    |sale|   m|         85|</span><br><span class="line">    | dev|   m|        232|</span><br><span class="line">    | dev|null|        349|</span><br><span class="line">    |null|null|        529|</span><br><span class="line">    |null|   f|        212|</span><br><span class="line">    |sale|   f|         95|</span><br><span class="line">    | dev|   f|        117|</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    //分组之后的 数据透视pivot, group by dept分组之后, 以sex为透视列, sex的取值(m\f)作为透视结果表中列的名字</span><br><span class="line">    val df_pivot = df1.groupBy(&quot;dept&quot;)</span><br><span class="line">      .pivot(&quot;sex&quot;, Seq(&quot;m&quot;, &quot;f&quot;))</span><br><span class="line">      .sum(&quot;salary&quot;)</span><br><span class="line">    df_pivot.show()</span><br><span class="line">    /*</span><br><span class="line">    分组之后的透视结果如下, sex</span><br><span class="line">    +----+---+---+</span><br><span class="line">    |dept|  m|  f|</span><br><span class="line">    +----+---+---+</span><br><span class="line">    | dev|232|117|</span><br><span class="line">    |sale| 85| 95|</span><br><span class="line">    +----+---+---+</span><br><span class="line"></span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">    //数据反透视，数据透视的逆向</span><br><span class="line">    val df_unpivot = df_pivot.unpivot(Array($&quot;dept&quot;), Array($&quot;m&quot;, $&quot;f&quot;), &quot;sex&quot;, &quot;sum&quot;)</span><br><span class="line">    df_unpivot.show()</span><br><span class="line">    /*</span><br><span class="line">    逆向透视结果如下,</span><br><span class="line">    +----+---+---+</span><br><span class="line">    |dept|sex|sum|</span><br><span class="line">    +----+---+---+</span><br><span class="line">    | dev|  m|232|</span><br><span class="line">    | dev|  f|117|</span><br><span class="line">    |sale|  m| 85|</span><br><span class="line">    |sale|  f| 95|</span><br><span class="line">    +----+---+---+</span><br><span class="line"></span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>参考资料</strong>：</p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">spark dataset api</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html">org.apache.spark.sql.functions</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html">spark中的透视函数</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36785151">excel数据透视表制作</a></p>
<h2><span id="sparksql">SparkSql</span></h2><h3><span id="group-by-分组">Group by 分组</span></h3><h4><span id="cube用法">cube用法</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">GROUP BY city, car_model WITH ROLLUP</span><br><span class="line">GROUP BY city, car_model WITH CUBE</span><br><span class="line">GROUP BY CUBE(city, car_model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE dealer (id INT, city STRING, car_model STRING, quantity INT);</span><br><span class="line">INSERT INTO dealer VALUES</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda Civic&#x27;, 10),</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda Accord&#x27;, 15),</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda CRV&#x27;, 7),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda Civic&#x27;, 20),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda Accord&#x27;, 10),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda CRV&#x27;, 3),</span><br><span class="line">    (300, &#x27;San Jose&#x27;, &#x27;Honda Civic&#x27;, 5),</span><br><span class="line">    (300, &#x27;San Jose&#x27;, &#x27;Honda Accord&#x27;, 8);</span><br><span class="line"></span><br><span class="line">-- Sum of quantity per dealership. Group by `id`.</span><br><span class="line">SELECT id, sum(quantity) FROM dealer GROUP BY id ORDER BY id;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           32|</span><br><span class="line">|200|           33|</span><br><span class="line">|300|           13|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Use column position in GROUP by clause.</span><br><span class="line">SELECT id, sum(quantity) FROM dealer GROUP BY 1 ORDER BY 1;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           32|</span><br><span class="line">|200|           33|</span><br><span class="line">|300|           13|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Multiple aggregations.</span><br><span class="line">-- 1. Sum of quantity per dealership.</span><br><span class="line">-- 2. Max quantity per dealership.</span><br><span class="line">SELECT id, sum(quantity) AS sum, max(quantity) AS max FROM dealer GROUP BY id ORDER BY id;</span><br><span class="line">+---+---+---+</span><br><span class="line">| id|sum|max|</span><br><span class="line">+---+---+---+</span><br><span class="line">|100| 32| 15|</span><br><span class="line">|200| 33| 20|</span><br><span class="line">|300| 13|  8|</span><br><span class="line">+---+---+---+</span><br><span class="line"></span><br><span class="line">-- Count the number of distinct dealer cities per car_model.</span><br><span class="line">SELECT car_model, count(DISTINCT city) AS count FROM dealer GROUP BY car_model;</span><br><span class="line">+------------+-----+</span><br><span class="line">|   car_model|count|</span><br><span class="line">+------------+-----+</span><br><span class="line">| Honda Civic|    3|</span><br><span class="line">|   Honda CRV|    2|</span><br><span class="line">|Honda Accord|    3|</span><br><span class="line">+------------+-----+</span><br><span class="line"></span><br><span class="line">-- Sum of only &#x27;Honda Civic&#x27; and &#x27;Honda CRV&#x27; quantities per dealership.</span><br><span class="line">SELECT id, sum(quantity) FILTER (</span><br><span class="line">            WHERE car_model IN (&#x27;Honda Civic&#x27;, &#x27;Honda CRV&#x27;)</span><br><span class="line">        ) AS `sum(quantity)` FROM dealer</span><br><span class="line">    GROUP BY id ORDER BY id;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           17|</span><br><span class="line">|200|           23|</span><br><span class="line">|300|            5|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Aggregations using multiple sets of grouping columns in a single statement.</span><br><span class="line">-- Following performs aggregations based on four sets of grouping columns.</span><br><span class="line">-- 1. city, car_model</span><br><span class="line">-- 2. city</span><br><span class="line">-- 3. car_model</span><br><span class="line">-- 4. Empty grouping set. Returns quantities for all city and car models.</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">    ORDER BY city;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|     null| HondaAccord| 33|</span><br><span class="line">|     null|    HondaCRV| 10|</span><br><span class="line">|     null|  HondaCivic| 35|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line">-- Group by processing with `ROLLUP` clause.</span><br><span class="line">-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), ())</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY city, car_model WITH ROLLUP</span><br><span class="line">    ORDER BY city, car_model;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line">-- Group by processing with `CUBE` clause.</span><br><span class="line">-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY city, car_model WITH CUBE</span><br><span class="line">    ORDER BY city, car_model;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|     null| HondaAccord| 33|</span><br><span class="line">|     null|    HondaCRV| 10|</span><br><span class="line">|     null|  HondaCivic| 35|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--Prepare data for ignore nulls example</span><br><span class="line">CREATE TABLE person (id INT, name STRING, age INT);</span><br><span class="line">INSERT INTO person VALUES</span><br><span class="line">    (100, &#x27;Mary&#x27;, NULL),</span><br><span class="line">    (200, &#x27;John&#x27;, 30),</span><br><span class="line">    (300, &#x27;Mike&#x27;, 80),</span><br><span class="line">    (400, &#x27;Dan&#x27;, 50);</span><br><span class="line"></span><br><span class="line">--Select the first row in column age</span><br><span class="line">SELECT FIRST(age) FROM person;</span><br><span class="line">+--------------------+</span><br><span class="line">| first(age, false)  |</span><br><span class="line">+--------------------+</span><br><span class="line">| NULL               |</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">--Get the first row in column `age` ignore nulls,last row in column `id` and sum of column `id`.</span><br><span class="line">SELECT FIRST(age IGNORE NULLS), LAST(id), SUM(id) FROM person;</span><br><span class="line">+-------------------+------------------+----------+</span><br><span class="line">| first(age, true)  | last(id, false)  | sum(id)  |</span><br><span class="line">+-------------------+------------------+----------+</span><br><span class="line">| 30                | 400              | 1000     |</span><br><span class="line">+-------------------+------------------+----------+</span><br></pre></td></tr></table></figure>



<h3><span id="orgapachesparksqlfunctions">org.apache.spark.sql.functions</span></h3><p>sql: <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/sql/index.html">https://spark.apache.org/docs/latest/api/sql/index.html</a></p>
<p>Api: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html</span><br></pre></td></tr></table></figure>

<p>资料：<a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1">https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1</a></p>
<h4><span id="collect_list">collect_list</span></h4><p>collect_list | collect_set   返回数组ArrayType</p>
<p>ds1.groupBy(“name”).agg(collect_list(“age”).as(“lis”)).show</p>
<h4><span id="min">min</span></h4><p>min_by(c1, c2)  </p>
<p>返回c2取最小值时，c1对应的值</p>
<p>val df1 &#x3D; Seq((1, 322), (2, 211)).toDF(“val”, “id”)</p>
<p>df1.agg(min_by($”val”, $”id”)).show</p>
<h4><span id="percentile_approx">percentile_approx</span></h4><p>percentile_approx百分位数：</p>
<p>val df1 &#x3D; Seq(1, 2, 3,4,5,6,7,8,9,10).toDF(“val”)</p>
<p>df1.agg(percentile_approx( $”val”, lit(0.1), lit(1000))).show</p>
<h4><span id="kurtosis">kurtosis</span></h4><p>kurtosis）又称<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B3%B0%E6%80%81%E7%B3%BB%E6%95%B0/0?fromModule=lemma_inlink">峰态系数</a>。表征<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%A6%82%E7%8E%87/0?fromModule=lemma_inlink">概率</a><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%AF%86%E5%BA%A6%E5%88%86%E5%B8%83%E6%9B%B2%E7%BA%BF/485777?fromModule=lemma_inlink">密度分布曲线</a>在<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B9%B3%E5%9D%87%E5%80%BC/0?fromModule=lemma_inlink">平均值</a>处<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B3%B0%E5%80%BC/11008657?fromModule=lemma_inlink">峰值</a>高低的特征数。直观看来，峰度反映了峰部的尖度</p>
<h4><span id="product">Product</span></h4><p>分组内元素的乘积</p>
<p>val df1 &#x3D; Seq((1, 322), (2, 211)).toDF(“val”, “id”)</p>
<p>df1.agg(product($”id”)).show</p>
<h4><span id="skewness">skewness</span></h4><p>数据倾斜度</p>
<p>val df1 &#x3D; Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(“val”, “id”)</p>
<p>df1.agg(skewness($”val”)).show</p>
<p>df1.agg(skewness($”id”)).show</p>
<h4><span id="stddev_pop">stddev_pop</span></h4><p>整体标准差 seigema (方差&#x3D;标准差的平方 )</p>
<h4><span id="stddev">stddev</span></h4><p>样本标准差</p>
<h4><span id="var_pop">var_pop</span></h4><p>总体方差</p>
<h4><span id="var_samp">var_samp</span></h4><p>样本方差</p>
<h4><span id="variance">variance</span></h4><p>样本方差</p>
<h4><span id="collection">Collection</span></h4><p>集合函数， 作用对象是一个集合，也就是说，列对应数据类型是一个集合（array, list等）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">aggregate </span><br><span class="line"></span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy(&quot;val&quot;).agg(collect_list(&quot;id&quot;).as(&quot;lis&quot;)).select(aggregate(col(&quot;lis&quot;), lit(0), (acc, x) =&gt; acc + x).as(&quot;lis_sum&quot;), $&quot;val&quot;).show</span><br><span class="line"></span><br><span class="line">array_append</span><br><span class="line">数组追加</span><br><span class="line"></span><br><span class="line">array_distinct</span><br><span class="line">数组去重</span><br><span class="line"></span><br><span class="line">array_join</span><br><span class="line">数组元素  连接:Concatenates the elements of column using the delimiter</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(array_join($&quot;ids&quot;, &quot;||&quot;)).show</span><br><span class="line"></span><br><span class="line">exists  </span><br><span class="line">数组中是否存在满足条件的元素</span><br><span class="line">exists(column: Column, f: (Column) ⇒ Column)</span><br><span class="line">df.select(exists(col(&quot;i&quot;), _ % 2 === 0))</span><br><span class="line"></span><br><span class="line">explode(e: Column): Column</span><br><span class="line">一行变多行，数组中的每个元素对应一个新的行</span><br><span class="line">Creates a new row for each element in the given array or map column</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(explode($&quot;ids&quot;).as(&quot;id&quot;)).show</span><br><span class="line"></span><br><span class="line">filter</span><br><span class="line">数组元素过滤</span><br><span class="line">df.select(filter(col(&quot;s&quot;), (x, i) =&gt; i % 2 === 0)) </span><br><span class="line">i表示对应索引</span><br><span class="line">((col, index) =&gt; predicate, the Boolean predicate to filter the input column given the index. Indices start at 0.)</span><br><span class="line"></span><br><span class="line">get</span><br><span class="line">get(column: Column, index: Column)</span><br><span class="line">返回数组指定索引处的元素 Returns element of array at given (0-based) index.</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(get($&quot;ids&quot;, lit(1))).show</span><br><span class="line"></span><br><span class="line">get_json_object(e: Column, path: String)</span><br><span class="line">解析json字符串, 返回key对应值</span><br><span class="line">val df1 = Seq(&quot;&#123;\&quot;a\&quot;:1, \&quot;b\&quot;:2&#125;&quot;, &quot;&#123;\&quot;a\&quot;:11, \&quot;b\&quot;:22&#125;&quot; ).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(get_json_object($&quot;val&quot;, &quot;$.b&quot;)).show</span><br><span class="line"></span><br><span class="line">from_json(e: Column, schema: String, options: Map[String, String]): Column</span><br><span class="line">解析json字符串, 返回一个map (Parses a column containing a JSON string into a MapType)</span><br><span class="line">val df1 = Seq(&quot;&#123;\&quot;a\&quot;:1, \&quot;b\&quot;:2&#125;&quot;, &quot;&#123;\&quot;a\&quot;:11, \&quot;b\&quot;:22&#125;&quot; ).toDF(&quot;val&quot;)</span><br><span class="line">import scala.collection.JavaConverters._</span><br><span class="line">df1.select(from_json($&quot;val&quot;, &quot;a int, b int&quot;, Map.empty[String, String].asJava)).show</span><br><span class="line">df1.select(from_json($&quot;val&quot;, &quot;a int, b int&quot;, Map.empty[String, String].asJava).as(&quot;map1&quot;)).select($&quot;map1.b&quot;)</span><br><span class="line"></span><br><span class="line">json_tuple</span><br><span class="line">json_tuple(json: Column, fields: String*): Column</span><br><span class="line">返回tuple, 输入指定多个字段</span><br><span class="line">Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.</span><br><span class="line">df1.select(lit(0).as(&quot;cc11&quot;), json_tuple($&quot;val&quot;, &quot;b&quot;, &quot;a&quot;)).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">map_contains_key</span><br><span class="line">判断map是否包含key</span><br><span class="line">val df1 = Seq(Map(&quot;a&quot; -&gt; &quot;a&quot;, &quot;b&quot; -&gt; &quot;b&quot;), Map(&quot;aa&quot; -&gt; &quot;aa&quot;, &quot;bb&quot; -&gt; &quot;bb&quot;)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(map_contains_key($&quot;val&quot;, &quot;a&quot;)).show</span><br><span class="line"></span><br><span class="line">map_filter</span><br><span class="line">map进行过滤</span><br><span class="line">df1.select(map_filter(col(&quot;val&quot;), (k, v) =&gt; k !== &#x27;a&#x27;)).show</span><br><span class="line"></span><br><span class="line">map_zip_with</span><br><span class="line">两个map合并</span><br><span class="line"></span><br><span class="line">posexplode</span><br><span class="line">类似explode，一行变多行</span><br><span class="line">df1.select($&quot;val&quot;, posexplode($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">sequence </span><br><span class="line">产生一个序列</span><br><span class="line">df1.select(sequence(lit(1), lit(3)))</span><br><span class="line"></span><br><span class="line">shuffle</span><br><span class="line">产生一个随机排列的数组</span><br><span class="line">val df1 = Seq(Array(1, 2, 3), Array(4,5,6,7)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(shuffle($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">slice</span><br><span class="line">数组切片</span><br><span class="line">df1.select(slice($&quot;val&quot;, 1, 2)).show</span><br><span class="line"></span><br><span class="line">sort_array(e: Column, asc: Boolean)</span><br><span class="line">数组排序</span><br><span class="line"></span><br><span class="line">to_json</span><br><span class="line">Converts a column containing a StructType, ArrayType or a MapType into a JSON string</span><br><span class="line">val df1 = Seq(Map(&quot;a&quot; -&gt; &quot;a&quot;, &quot;b&quot; -&gt; &quot;b&quot;), Map(&quot;aa&quot; -&gt; &quot;aa&quot;, &quot;bb&quot; -&gt; &quot;bb&quot;)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(to_json($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">zip_with</span><br><span class="line">按元素合并两个数组</span><br><span class="line">df.select(zip_with(df1(&quot;val1&quot;), df1(&quot;val2&quot;), (x, y) =&gt; x + y))</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h4><span id="日期函数">日期函数</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">val df1 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line"></span><br><span class="line">df1.select($&quot;val&quot;, current_date().as(&quot;curd&quot;), current_timestamp.as(&quot;curt&quot;)).select(add_months($&quot;curd&quot;, 2)).show</span><br><span class="line"></span><br><span class="line">select(date_add($&quot;curd&quot;, 2))</span><br><span class="line"></span><br><span class="line">date_sub</span><br><span class="line">date_diff</span><br><span class="line"></span><br><span class="line">date_format</span><br><span class="line"></span><br><span class="line">窗口函数 </span><br><span class="line">window(timeColumn: Column, windowDuration: String) 滚动窗口  窗口长度 size固定, slide = size</span><br><span class="line">window(timeColumn: Column, windowDuration: String, slideDuration: String) 滑动窗口</span><br><span class="line">滑动窗口以一个步长（Slide）不断向前滑动，窗口的长度size固定, slide &lt; size</span><br><span class="line"></span><br><span class="line">session_window(timeColumn: Column, gapDuration: String) 会话窗口</span><br><span class="line">两个窗口之间有一个间隙，被称为Session Gap。当一个窗口在大于Session Gap的时间内没有接收到新数据时，窗口将关闭。在这种模式下，窗口的长度是可变的，每个窗口的开始和结束时间并不是确定的。</span><br></pre></td></tr></table></figure>

<h4><span id="others">Others</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def coalesce(e: Column*): Column</span><br><span class="line">返回非空的列对应值</span><br><span class="line">Returns the first column that is not null, or null if all inputs are null.</span><br><span class="line"></span><br><span class="line">def col(colName: String): Column</span><br><span class="line">取对应列</span><br><span class="line"></span><br><span class="line">expr(expr: String): Column</span><br><span class="line">表达式转换为列</span><br><span class="line">df.groupBy(expr(&quot;length(word)&quot;)).count()</span><br><span class="line"></span><br><span class="line">lit(literal: Any): Column</span><br><span class="line">创建一个常量类型的列</span><br><span class="line">Creates a Column of literal value.</span><br><span class="line"></span><br><span class="line">monotonically_increasing_id()</span><br><span class="line">产生一个单调递增列，保证唯一性，但整体不保证是连续的</span><br><span class="line">val df1 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line">df1.select($&quot;val&quot;, monotonically_increasing_id()).show</span><br><span class="line"></span><br><span class="line">negate</span><br><span class="line">取负值</span><br><span class="line">df1.select(negate($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">rand()</span><br><span class="line">随机函数</span><br><span class="line"></span><br><span class="line">_*使用</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select(df1.columns.map(c =&gt; col(c)):_*)</span><br><span class="line"></span><br><span class="line">when使用</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select(when(col(&quot;id&quot;) &gt; 1, 1).otherwise(0), $&quot;id&quot;).show</span><br><span class="line"></span><br><span class="line">struct(colName: String, colNames: String*)</span><br><span class="line">Creates a new struct column  创建一个struct列</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select($&quot;id&quot;, struct(&quot;id&quot;, &quot;mark&quot;)).printSchema</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4><span id="字符串函数">字符串函数</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html</span><br></pre></td></tr></table></figure>

<p>concat_ws(sep: String, exprs: <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html">Column</a>*)</p>
<p>字符串连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select($&quot;id&quot;, concat_ws(&quot;|&quot;, $&quot;id&quot;, $&quot;mark&quot;)).show</span><br><span class="line"></span><br><span class="line">instr(str: Column, substring: String)</span><br><span class="line">查找substring 位置，如果未找到返回0</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="窗口函数">窗口函数</span></h4><p>关于hive的窗口函数 是类似的，<a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2015/04/190.htm">参考资料hive分析窗口函数</a></p>
<p><a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1">spark窗口函数example</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">row_number()  ( row_number() over(partition by a order by b))</span><br><span class="line">dense_rank()   1 2 2  3  排名rank不会跳级</span><br><span class="line">rank()         1 2 2 4 排名rank会跳级，如果有相等排名</span><br><span class="line">percent_rank()</span><br><span class="line">cume_dist()  //累加分布</span><br><span class="line">ntile()   //窗口分组</span><br><span class="line">lag</span><br><span class="line">lead</span><br><span class="line">nth_value  //当前这一行能看到的窗口对应的第n个值</span><br></pre></td></tr></table></figure>





<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.expressions.Window</span><br><span class="line"></span><br><span class="line">object DatasetWindowExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Michael&quot;, &quot;Sales&quot;, 4600),</span><br><span class="line">      (&quot;Robert&quot;, &quot;Sales&quot;, 4100),</span><br><span class="line">      (&quot;Maria&quot;, &quot;Finance&quot;, 3000),</span><br><span class="line">      (&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Scott&quot;, &quot;Finance&quot;, 3300),</span><br><span class="line">      (&quot;Jen&quot;, &quot;Finance&quot;, 3900),</span><br><span class="line">      (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000),</span><br><span class="line">      (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000),</span><br><span class="line">      (&quot;Saif&quot;, &quot;Sales&quot;, 4100)</span><br><span class="line">    )</span><br><span class="line">    val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //row_number</span><br><span class="line">    val windowSpec  = Window.partitionBy(&quot;department&quot;).orderBy($&quot;salary&quot;)</span><br><span class="line">    df.withColumn(&quot;row_number&quot;,row_number().over(windowSpec))</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line">    df.withColumn(&quot;dense_rank&quot;,dense_rank().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    df.withColumn(&quot;rank&quot;,rank().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //returns the relative rank (i.e. percentile) of rows within a window partition. 相对排名(取值0到1)</span><br><span class="line">    df.withColumn(&quot;percent_rank&quot;,percent_rank().over(windowSpec)).show()</span><br><span class="line"></span><br><span class="line">    //ntile(2) 表示将窗口 2等分, 对应的分组id，前一部分数据对应分组id 1，后一部分数据对应分组id 2</span><br><span class="line">    df.withColumn(&quot;ntile&quot;,ntile(2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    // 窗口累加值分布</span><br><span class="line">    //   * Window function: returns the cumulative distribution of values within a window partition,</span><br><span class="line">    //   * i.e. the fraction of rows that are below the current row.</span><br><span class="line">    df.withColumn(&quot;cume_dist&quot;,cume_dist().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //当前行之前 2 行 对应列的值</span><br><span class="line">    df.withColumn(&quot;lag&quot;,lag(&quot;salary&quot;,2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //当前行之后 2 行 对应列的值</span><br><span class="line">    df.withColumn(&quot;lead&quot;,lead(&quot;salary&quot;,2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    val windowSpecAgg  = Window.partitionBy(&quot;department&quot;)</span><br><span class="line">    val aggDF = df.withColumn(&quot;row&quot;,row_number.over(windowSpec))</span><br><span class="line">      .withColumn(&quot;avg&quot;, avg(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;sum&quot;, sum(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;min&quot;, min(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;max&quot;, max(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .where(col(&quot;row&quot;)===1).select(&quot;department&quot;,&quot;avg&quot;,&quot;sum&quot;,&quot;min&quot;,&quot;max&quot;)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.groupBy(&quot;department&quot;).agg(</span><br><span class="line">      avg($&quot;salary&quot;).as(&quot;avg&quot;),</span><br><span class="line">      sum($&quot;salary&quot;).as(&quot;sum&quot;),</span><br><span class="line">      min($&quot;salary&quot;).as(&quot;min&quot;),</span><br><span class="line">      max($&quot;salary&quot;).as(&quot;max&quot;)).show</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="scalar-function">Scalar Function</span></h4><p>标量函数</p>
<p>一行返回一个值</p>
<p>Scalar functions are functions that return a single value per row</p>
<p>系统内置( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions.html#scalar-functions">Built-in Scalar Functions</a>) +  用户定义UDF ( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html">User Defined Scalar Functions</a>.)</p>
<h5><span id="udf">UDF</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.udf</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark SQL UDF scalar example&quot;)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">// Define and register a zero-argument non-deterministic UDF</span><br><span class="line">// UDF is deterministic by default, i.e. produces the same result for the same input.</span><br><span class="line">val random = udf(() =&gt; Math.random())</span><br><span class="line">spark.udf.register(&quot;random&quot;, random.asNondeterministic())</span><br><span class="line">spark.sql(&quot;SELECT random()&quot;).show()</span><br><span class="line">// +-------+</span><br><span class="line">// |UDF()  |</span><br><span class="line">// +-------+</span><br><span class="line">// |xxxxxxx|</span><br><span class="line">// +-------+</span><br><span class="line"></span><br><span class="line">// Define and register a one-argument UDF</span><br><span class="line">val plusOne = udf((x: Int) =&gt; x + 1)</span><br><span class="line">spark.udf.register(&quot;plusOne&quot;, plusOne)</span><br><span class="line">spark.sql(&quot;SELECT plusOne(5)&quot;).show()</span><br><span class="line">// +------+</span><br><span class="line">// |UDF(5)|</span><br><span class="line">// +------+</span><br><span class="line">// |     6|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Define a two-argument UDF and register it with Spark in one step</span><br><span class="line">spark.udf.register(&quot;strLenScala&quot;, (_: String).length + (_: Int))</span><br><span class="line">spark.sql(&quot;SELECT strLenScala(&#x27;test&#x27;, 1)&quot;).show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |strLenScala(test, 1)|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |                   5|</span><br><span class="line">// +--------------------+</span><br><span class="line"></span><br><span class="line">// UDF in a WHERE clause</span><br><span class="line">spark.udf.register(&quot;oneArgFilter&quot;, (n: Int) =&gt; &#123; n &gt; 5 &#125;)</span><br><span class="line"></span><br><span class="line">//注册成临时表</span><br><span class="line">spark.range(1, 10).createOrReplaceTempView(&quot;test&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM test WHERE oneArgFilter(id)&quot;).show()</span><br><span class="line">// +---+</span><br><span class="line">// | id|</span><br><span class="line">// +---+</span><br><span class="line">// |  6|</span><br><span class="line">// |  7|</span><br><span class="line">// |  8|</span><br><span class="line">// |  9|</span><br><span class="line">// +---+</span><br></pre></td></tr></table></figure>



<h4><span id="aggregate-function">Aggregate  Function</span></h4><p>聚合函数</p>
<p>多行返回一个值</p>
<p>return a single value on a group of rows</p>
<p>系统内置( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions">Built-in Aggregation Functions</a>) + 用户定义UDAF(<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-aggregate.html">User Defined Aggregate Functions</a>.)</p>
<h5><span id="udaf">UDAF</span></h5><p>Reduce  汇总、汇聚</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Type-Safe User-Defined Aggregate Functions</span><br></pre></td></tr></table></figure>

<p>针对dataset操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.expressions.Aggregator</span><br><span class="line"></span><br><span class="line">case class Employee(name: String, salary: Long)</span><br><span class="line">case class Average(var sum: Long, var count: Long)</span><br><span class="line"></span><br><span class="line">object MyAverage extends Aggregator[Employee, Average, Double] &#123;</span><br><span class="line">  // A zero value for this aggregation. Should satisfy the property that any b + zero = b</span><br><span class="line">  def zero: Average = Average(0L, 0L)</span><br><span class="line">  // Combine two values to produce a new value. For performance, the function may modify `buffer`</span><br><span class="line">  // and return it instead of constructing a new object</span><br><span class="line">  def reduce(buffer: Average, employee: Employee): Average = &#123;</span><br><span class="line">    buffer.sum += employee.salary</span><br><span class="line">    buffer.count += 1</span><br><span class="line">    buffer</span><br><span class="line">  &#125;</span><br><span class="line">  // Merge two intermediate values</span><br><span class="line">  def merge(b1: Average, b2: Average): Average = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  // Transform the output of the reduction</span><br><span class="line">  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count</span><br><span class="line">  // Specifies the Encoder for the intermediate value type</span><br><span class="line">  def bufferEncoder: Encoder[Average] = Encoders.product</span><br><span class="line">  // Specifies the Encoder for the final output value type</span><br><span class="line">  def outputEncoder: Encoder[Double] = Encoders.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val ds = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;).as[Employee]</span><br><span class="line">ds.show()</span><br><span class="line">// +-------+------+</span><br><span class="line">// |   name|salary|</span><br><span class="line">// +-------+------+</span><br><span class="line">// |Michael|  3000|</span><br><span class="line">// |   Andy|  4500|</span><br><span class="line">// | Justin|  3500|</span><br><span class="line">// |  Berta|  4000|</span><br><span class="line">// +-------+------+</span><br><span class="line"></span><br><span class="line">// Convert the function to a `TypedColumn` and give it a name</span><br><span class="line">val averageSalary = MyAverage.toColumn.name(&quot;average_salary&quot;)</span><br><span class="line">val result = ds.select(averageSalary)</span><br><span class="line">result.show()</span><br><span class="line">// +--------------+</span><br><span class="line">// |average_salary|</span><br><span class="line">// +--------------+</span><br><span class="line">// |        3750.0|</span><br><span class="line">// +--------------+</span><br></pre></td></tr></table></figure>

<p>针对DataFrame操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Untyped User-Defined Aggregate Functions</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.expressions.Aggregator</span><br><span class="line">import org.apache.spark.sql.functions</span><br><span class="line"></span><br><span class="line">case class Average(var sum: Long, var count: Long)</span><br><span class="line"></span><br><span class="line">object MyAverage extends Aggregator[Long, Average, Double] &#123;</span><br><span class="line">  // A zero value for this aggregation. Should satisfy the property that any b + zero = b</span><br><span class="line">  def zero: Average = Average(0L, 0L)</span><br><span class="line">  // Combine two values to produce a new value. For performance, the function may modify `buffer`</span><br><span class="line">  // and return it instead of constructing a new object</span><br><span class="line">  def reduce(buffer: Average, data: Long): Average = &#123;</span><br><span class="line">    buffer.sum += data</span><br><span class="line">    buffer.count += 1</span><br><span class="line">    buffer</span><br><span class="line">  &#125;</span><br><span class="line">  // Merge two intermediate values</span><br><span class="line">  def merge(b1: Average, b2: Average): Average = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  // Transform the output of the reduction</span><br><span class="line">  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count</span><br><span class="line">  // Specifies the Encoder for the intermediate value type</span><br><span class="line">  def bufferEncoder: Encoder[Average] = Encoders.product</span><br><span class="line">  // Specifies the Encoder for the final output value type</span><br><span class="line">  def outputEncoder: Encoder[Double] = Encoders.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Register the function to access it</span><br><span class="line">spark.udf.register(&quot;myAverage&quot;, functions.udaf(MyAverage))</span><br><span class="line"></span><br><span class="line">val df = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;)</span><br><span class="line">df.createOrReplaceTempView(&quot;employees&quot;)</span><br><span class="line">df.show()</span><br><span class="line">// +-------+------+</span><br><span class="line">// |   name|salary|</span><br><span class="line">// +-------+------+</span><br><span class="line">// |Michael|  3000|</span><br><span class="line">// |   Andy|  4500|</span><br><span class="line">// | Justin|  3500|</span><br><span class="line">// |  Berta|  4000|</span><br><span class="line">// +-------+------+</span><br><span class="line"></span><br><span class="line">val result = spark.sql(&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;)</span><br><span class="line">result.show()</span><br><span class="line">// +--------------+</span><br><span class="line">// |average_salary|</span><br><span class="line">// +--------------+</span><br><span class="line">// |        3750.0|</span><br><span class="line">// +--------------+</span><br></pre></td></tr></table></figure>

<h4><span id="hive-udf">Hive UDF</span></h4><p>Spark SQL supports integration of Hive UDFs, UDAFs and UDTFs. Similar to Spark UDFs and UDAFs, Hive UDFs work on a single row as input and generate a single row as output, while Hive UDAFs operate on multiple rows and return a single aggregated row as a result. In addition, Hive also supports UDTFs (User Defined Tabular Functions) that act on one row as input and return multiple rows as output. </p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-hive.html">spark 官方</a></p>
<h5><span id="spark-官方hive-udf-example">Spark 官方hive udf Example</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">-- Register `GenericUDFAbs` and use it in Spark SQL.</span><br><span class="line">-- Note that, if you use your own programmed one, you need to add a JAR containing it</span><br><span class="line">-- into a classpath,</span><br><span class="line">-- e.g., ADD JAR yourHiveUDF.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION testUDF AS &#x27;org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs&#x27;;</span><br><span class="line"></span><br><span class="line">SELECT * FROM t;</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| -1.0|</span><br><span class="line">|  2.0|</span><br><span class="line">| -3.0|</span><br><span class="line">+-----+</span><br><span class="line"></span><br><span class="line">SELECT testUDF(value) FROM t;</span><br><span class="line">+--------------+</span><br><span class="line">|testUDF(value)|</span><br><span class="line">+--------------+</span><br><span class="line">|           1.0|</span><br><span class="line">|           2.0|</span><br><span class="line">|           3.0|</span><br><span class="line">+--------------+</span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf-在hive中使用">Hive UDF 在Hive中使用</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1 创建一个class 继承UDF class，实现evaluate 方法</span><br><span class="line">(you need to create a new class that extends UDF, with one or more methods named evaluate)</span><br><span class="line"></span><br><span class="line">2 把项目打成jar包，将jar包添加到hive classpath</span><br><span class="line">(After compiling your code to a jar, you need to add this to the Hive classpath)</span><br><span class="line"></span><br><span class="line">3 注册一个函数(可以是临时的、永久的) 对应用户定义的class (register your function as described)</span><br><span class="line">之后，就可以像内置函数一样，使用udf了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive0.13似乎支持，注册UDF时 指定用户jar path， for example:</span><br><span class="line"></span><br><span class="line">As of Hive 0.13, UDFs also have the option of being able to specify required jars in the CREATE FUNCTION statement:</span><br><span class="line">`CREATE FUNCTION myfunc AS ``&#x27;myclass&#x27;` `USING JAR ``&#x27;hdfs:///path/to/jar&#x27;``;`</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf-在spark-中使用">HIVE  UDF 在Spark 中使用</span></h5><p>定义class GeohashUtilty 继承udf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class GeohashUtilty extends UDF &#123;</span><br><span class="line"></span><br><span class="line">    public String evaluate(double lat, double lon, int precision) &#123;</span><br><span class="line">        return getGeoHashString(lat, lon, precision);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //get geohash string with precision n</span><br><span class="line">    public static String getGeoHashString(double lat, double lon, int precision) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            GeoHash geoHash = GeoHash.withCharacterPrecision(lat, lon, precision);</span><br><span class="line">            String geoHashString = geoHash.toBase32();</span><br><span class="line">            return geoHashString;</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(1) 经纬度获取geohash值，用于判断哪些点在同一个区域内</span><br><span class="line">spark sql 用法:</span><br><span class="line">add jar /home/etl/until/GenericUDF-1.0-jar-with-dependencies.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION geohashVal AS &#x27;com.weidai.udf.generic.GeohashUtilty&#x27;;</span><br><span class="line">select geohashVal( 28.561104, 121.186142, 8 );</span><br><span class="line">最后一位表示hash值的精度，既字符串长度，上述结果为字符串 wtn4mxmh</span><br><span class="line"></span><br><span class="line">当两个点的geohash值相同，表示这两个点在同一个区域内</span><br><span class="line">geohash值取8位长度，表示区域范围38.2*19m，</span><br><span class="line">geohash值取7位长度，表示区域范围152.9*152.4m，</span><br><span class="line">geohash值取6位长度，表示区域范围1200*609m，</span><br><span class="line">geohash值取5位长度，表示区域范围4.9km*4.9km，</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(2) 经纬度 转换为 地址 （调用百度api, 调用次数有限制，一天不要超过10万次）</span><br><span class="line">spark sql  用法:</span><br><span class="line">add jar /home/etl/until/GenericUDF-1.0-jar-with-dependencies.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION latlonToAddr AS &#x27;com.weidai.udf.generic.LatLonToAddress&#x27;;</span><br><span class="line">select latlonToAddr( 28.561104, 121.186142 );</span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf其他文档">HIVE UDF其他文档</span></h5><p><strong>hive SQL 目前支持 三种用户自定义的function: UDF, UDAF, UDTF</strong></p>
<p><strong>UDF</strong>: 输入对应一行，输出对应一行，一对一关系</p>
<p>UDFs works on a single row in a table and produces a single row as output. Its one to one relationship between input and output of a function， e.g Hive built in TRIM() function</p>
<p><strong>UDAF</strong>：<strong>User Defined Aggregate Function</strong></p>
<p>输入多行，输出1行，关系对应是多对1</p>
<p>User defined aggregate functions works on more than one row and gives single row as output. e.g Hive built in MAX() or COUNT() functions. here the relation is many to one</p>
<p><strong>UDTF</strong></p>
<p>输入一行，输出多行，关系对应是1对多。UDTF can be used to split a column into multiple column。 </p>
<p>User defined tabular function works on one row as input and returns multiple rows as output. So here the relation in one to many. e.g Hive built in EXPLODE() function(输入一行，输出多行). </p>
<p><strong>references</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.linkedin.com/pulse/hive-functions-udfudaf-udtf-examples-gaurav-singh">linked in的一篇blog：Hive Functions – UDF,UDAF and UDTF with Examples</a></p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inOperators">hive wiki: hive built-in function</a></p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">Hive wiki: Creating Custom UDFs</a></p>
<h2><span id="dataset-生成">Dataset 生成</span></h2><h3><span id="集合生成dataset">集合生成Dataset</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val df1 = Seq((&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)).toDF(&quot;dept&quot;, &quot;sex&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line">val df2 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line"></span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">// Encoders are created for case classes</span><br><span class="line">val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"></span><br><span class="line">val rdd1 = spark.range(1, 10)</span><br><span class="line">#生成rdd1, rdd1: org.apache.spark.sql.Dataset[Long] = [id: bigint]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3><span id="加载数据集生成dataset">加载数据集生成Dataset</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span><br><span class="line">import spark.implicits._</span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDS = spark.read.json(path).as[Person]</span><br><span class="line">peopleDS.show()</span><br></pre></td></tr></table></figure>



<h3><span id="rdd-生成dataset">RDD 生成Dataset</span></h3><h4><span id="inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于反射推测schema</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">object DatasetExample2 &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val peopleDF  = spark.sparkContext</span><br><span class="line">      .textFile(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.txt&quot;)</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)).toDF()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">    //dsl api</span><br><span class="line">    peopleDF.select( $&quot;name&quot;, $&quot;age&quot;, concat_ws(&quot;&quot;, lit(&quot;Name: &quot;), $&quot;name&quot;) ).show()</span><br><span class="line"></span><br><span class="line">    //函数式api The columns of a row in the result can be accessed by field index</span><br><span class="line">    peopleDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line"></span><br><span class="line">    //函数式api The columns of a row in the result can be accessed by field name</span><br><span class="line">    peopleDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span><br><span class="line">    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]</span><br><span class="line">    // Primitive types and case classes can be also defined as</span><br><span class="line">    // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span><br><span class="line"></span><br><span class="line">    // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span><br><span class="line">    peopleDF.map(teenager =&gt; teenager.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="编码指定schema">编码指定schema</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Row, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;</span><br><span class="line"></span><br><span class="line">object DatasetExample3 &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val peopleRDD  = spark.sparkContext</span><br><span class="line">      .textFile(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.txt&quot;)</span><br><span class="line"></span><br><span class="line">    // The schema is encoded in a string</span><br><span class="line">    val schemaString = &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">    // Generate the schema based on the string of schema</span><br><span class="line">    val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">      .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">    val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">    // Convert records of the RDD (people) to Rows</span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(0), attributes(1).trim))</span><br><span class="line"></span><br><span class="line">    // Apply the schema to the RDD</span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.printSchema()</span><br><span class="line"></span><br><span class="line">    peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">    // SQL can be run over a temporary view created using DataFrames</span><br><span class="line">    val results = spark.sql(&quot;SELECT name FROM people&quot;)</span><br><span class="line"></span><br><span class="line">    // The results of SQL queries are DataFrames and support all the normal RDD operations</span><br><span class="line">    // The columns of a row in the result can be accessed by field index or by field name</span><br><span class="line">    results.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="spark-数据源">Spark 数据源</span></h2><p>DataSource</p>
<p>通用写法指定fileformat，比如parquet|orc|json|csv|avro，默认读写格式都为parquet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># read format | write format</span><br><span class="line">val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># read option | write option</span><br><span class="line">val peopleDFCsv = spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;examples/src/main/resources/people.csv&quot;)</span><br><span class="line">  </span><br><span class="line">usersDF.write.format(&quot;orc&quot;)</span><br><span class="line">  .option(&quot;orc.bloom.filter.columns&quot;, &quot;favorite_color&quot;)</span><br><span class="line">  .option(&quot;orc.dictionary.key.threshold&quot;, &quot;1.0&quot;)</span><br><span class="line">  .option(&quot;orc.column.encoding.direct&quot;, &quot;name&quot;)</span><br><span class="line">  .save(&quot;users_with_options.orc&quot;)  </span><br><span class="line"></span><br><span class="line"># 直接通过sql 读取文件</span><br><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;)</span><br><span class="line"></span><br><span class="line">SELECT * FROM parquet.`file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.parquet`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#saveAsTable</span><br><span class="line">val df  = spark.read.parquet(&quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">df.write.mode(&quot;overwrite&quot;).saveAsTable(&quot;test_zhou.user1&quot;)</span><br><span class="line">spark.table(&quot;test_zhou.user1&quot;).show</span><br><span class="line">spark.sql(&quot;drop table test_zhou.user1&quot;)   </span><br><span class="line">#默认保存warehouse配置路径，drop table删除表之后，数据对应路径也被删除(类似hive内部表)</span><br><span class="line"></span><br><span class="line">//在write option指定path, drop table删除表之后，数据对应路径不会被删除, 数据被保留, 类似hive外部表</span><br><span class="line">//在这种方式下, 表的分区信息需要手动同步, 执行sql: MSCK REPAIR TABLE tableName</span><br><span class="line">df.write.mode(&quot;overwrite&quot;).option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;).saveAsTable(&quot;test_zhou.user1&quot;)</span><br><span class="line">spark.table(&quot;test_zhou.user1&quot;).show</span><br><span class="line">spark.sql(&quot;drop table test_zhou.user1&quot;)   </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="bucket-和-partition">Bucket 和 Partition</span></h3><p>分桶只能作用与持久化的表 Bucketing and sorting are applicable only to persistent tables</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.fs.Path</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.expressions.Window</span><br><span class="line"></span><br><span class="line">object DatasetPartitionExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local[3]&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">//      .config(&quot;spark.sql.catalogImplementation&quot;, &quot;hive&quot;)</span><br><span class="line">      .enableHiveSupport()  //&quot;spark.sql.catalogImplementation&quot;  &quot;hive&quot;</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.hadoopConfiguration.addResource(new Path(&quot;/Users/zhouqingfeng/Desktop/mydirect/github/distributedStudy/spark/sourcecode/projects/spark_all_test/src/main/resources/hive-site.xml&quot;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Michael&quot;, &quot;Sales&quot;, 4600),</span><br><span class="line">      (&quot;Robert&quot;, &quot;Sales&quot;, 4100),</span><br><span class="line">      (&quot;Maria&quot;, &quot;Finance&quot;, 3000),</span><br><span class="line">      (&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Scott&quot;, &quot;Finance&quot;, 3300),</span><br><span class="line">      (&quot;Jen&quot;, &quot;Finance&quot;, 3900),</span><br><span class="line">      (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000),</span><br><span class="line">      (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000),</span><br><span class="line">      (&quot;Saif&quot;, &quot;Sales&quot;, 4100)</span><br><span class="line">    )</span><br><span class="line">    val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.write</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line">    </span><br><span class="line">    //按照分桶个数和分桶列 进行分桶</span><br><span class="line">    df.write</span><br><span class="line">      .bucketBy(3, &quot;department&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line"></span><br><span class="line">		//分区</span><br><span class="line">    df.write</span><br><span class="line">      .partitionBy(&quot;department&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .format(&quot;parquet&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		//分区之后 分桶</span><br><span class="line">    df.write</span><br><span class="line">      .partitionBy(&quot;department&quot;)</span><br><span class="line">      .bucketBy(3, &quot;salary&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .format(&quot;parquet&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">    df.write</span><br><span class="line">      .partitionBy(&quot;department&quot;)</span><br><span class="line">      .bucketBy(3, &quot;salary&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .format(&quot;csv&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line">      </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="分桶划分">分桶划分</span></h4><p>计算桶字段hash值， 然后mod取模， 如果结果为负，再加上桶数(保证结果为正)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># value为根据hash算法计算的hash值, n为桶数</span><br><span class="line">b = value mod n</span><br><span class="line">if b &lt; 0:</span><br><span class="line">  b = (b + n) mod n</span><br><span class="line"></span><br><span class="line"># 测试</span><br><span class="line">from pyspark.sql.functions import hash, col, expr</span><br><span class="line">(</span><br><span class="line">  spark.range(100) # this will create a DataFrame with one column id</span><br><span class="line">  .withColumn(&quot;hash&quot;, hash(col(&quot;id&quot;)))</span><br><span class="line">  .withColumn(&quot;bucket&quot;, expr(&quot;pmod(hash, 8)&quot;))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4><span id="分桶测试">分桶测试</span></h4><pre><code>import org.apache.spark.sql.functions._
val simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000),
  (&quot;Michael&quot;, &quot;Sales&quot;, 4600),
  (&quot;Robert&quot;, &quot;Sales&quot;, 4100),
  (&quot;Maria&quot;, &quot;Finance&quot;, 3000),
  (&quot;James&quot;, &quot;Sales&quot;, 3000),
  (&quot;Scott&quot;, &quot;Finance&quot;, 3300),
  (&quot;Jen&quot;, &quot;Finance&quot;, 3900),
  (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000),
  (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000),
  (&quot;Saif&quot;, &quot;Sales&quot;, 4100)
)
val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)

        //分区之后 分桶
df.write
      .partitionBy(&quot;department&quot;)
      .bucketBy(3, &quot;salary&quot;)
      .mode(&quot;overwrite&quot;)
      .format(&quot;parquet&quot;)
      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)
      .saveAsTable(&quot;test_zhou.emp&quot;)

# 桶号计算方式：如何计算每行数据所属的桶id
spark.table(&quot;test_zhou.emp&quot;).withColumn(&quot;hash&quot;, hash(col(&quot;salary&quot;))).withColumn(&quot;bucket&quot;, expr(&quot;pmod(hash, 3)&quot;)).orderBy(&quot;department&quot;).withColumn(&quot;mod&quot;, expr(&quot;mod(hash, 3)&quot;)).orderBy(&quot;department&quot;).show
</code></pre>
<p>结果产生三个分区，每个分区最多产生三个桶(可能少于3)，桶号可通过文件名或计算得到</p>
<p>三个分区：</p>
<p>对应三个目录</p>
<img src="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/截屏2023-07-07 17.43.39.png">



<p>文件名查看桶号：</p>
<p>最后面的00001、00002表示桶号</p>
<img src="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/截屏2023-07-07 17.44.01.png">



<p>![截屏2023-07-07 17.45.02](&#x2F;Users&#x2F;zhouqingfeng&#x2F;Desktop&#x2F;mydirect&#x2F;gitblog&#x2F;cont&#x2F;source&#x2F;_posts&#x2F;bigdata&#x2F;spark&#x2F;spark3&#x2F;spark3学习笔记20230704&#x2F;截屏2023-07-07 17.45.02.png)</p>
<p>桶号计算方式如上，查询结果如下，bucket列对应桶号</p>
<img src="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/截屏2023-07-07 17.44.15.png">



<h4><span id="分桶目的">分桶目的</span></h4><p>1、避免在join和聚合查询中，产生shuffle操作</p>
<p>2、数据过滤查询中，应用桶裁剪，减少数据IO开销 (io读取和传输)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">From: https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53</span><br><span class="line">分桶优势：</span><br><span class="line">There are two main areas where bucketing can help, the first one is to avoid shuffle in queries with joins and aggregations, the second one is to reduce the I/O with a feature called bucket pruning.</span><br></pre></td></tr></table></figure>



<h4><span id="其他">其他</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#查看分桶信息</span><br><span class="line">spark.sql(&quot;DESCRIBE EXTENDED table_name&quot;).show(n=100, true)</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wypblog/article/details/108786770">和hive分桶区别</a></p>
<p>1、Spark 分桶和 Hive 分桶采用不同的 hash 算法。Hive 用的是 HiveHash，而 Spark 用的是 Murmur3，所以数据的分布是不一样的</p>
<p>2、Hive 在生成分桶的时候会额外进行一个 Reduce 操作，以保证相同分桶的数据都存储在一个文件中。而 Spark SQL 在写分桶文件时不需要 Shuffle 操作，这样就会导致每个分桶最多产生 M 个文件(task个数)</p>
<p>（spark的每个task针对每个桶多会产生一个文件(0个或1个)，结果是每个桶会有多个文件；而hive会自动进行reduce，将多个分桶文件合并）</p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53">spark 分桶详解，非常非常清晰、全面</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/" data-id="cljo12rbr0000k99a2y8w8wtx" data-title="spark3学习笔记20230704" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/hadoop/hadoop_env/hadoop本地环境搭建" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T02:13:53.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">hadoop本地环境搭建</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#env">Env</a></li>
<li><a href="#install-hadoop-277">Install hadoop 2.7.7</a><ul>
<li><a href="#start">Start</a></li>
<li><a href="#others">Others</a></li>
</ul>
</li>
<li><a href="#install-mysql">Install mysql</a></li>
<li><a href="#install-hive">Install hive</a><ul>
<li><a href="#start-1">Start</a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#install-hbase">Install HBase</a><ul>
<li><a href="#pseudo-distributed-local-install">Pseudo-Distributed Local Install</a></li>
<li><a href="#simple-usage">Simple Usage</a></li>
<li><a href="#phoenix">phoenix</a></li>
</ul>
</li>
<li><a href="#install-zookeeper">Install Zookeeper</a></li>
<li><a href="#install-kafka">Install Kafka</a></li>
<li><a href="#install-spark">Install Spark</a><ul>
<li><a href="#spark-local">Spark local</a></li>
<li><a href="#spark-yarn-deploy">Spark yarn deploy</a></li>
<li><a href="#spark-to-hbase">spark to  hbase</a></li>
</ul>
</li>
<li><a href="#hive-to-or-from-hbase">Hive to (or from) Hbase</a><ul>
<li><a href="#hive-to-hbase">Hive to hbase</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="env">Env</span></h2><p>macOS 12.4</p>
<p>java version “1.8.0_102”</p>
<p>Spark3.4.1  hadoop2.7.7</p>
<h2><span id="install-hadoop-277">Install hadoop 2.7.7</span></h2><p>&#x2F;Users&#x2F;***&#x2F;Desktop&#x2F;software&#x2F;hadoop-2.7.7 </p>
<p>Site:<a target="_blank" rel="noopener" href="http://apache.communilink.net/hadoop/common/hadoop-2.7.7/">http://apache.communilink.net/hadoop/common/hadoop-2.7.7/</a></p>
<p>ref1:<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/de7eb61c983a">https://www.jianshu.com/p/de7eb61c983a</a></p>
<p>ref2:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/landed/p/6831758.html">https://www.cnblogs.com/landed/p/6831758.html</a></p>
<h3><span id="start">Start</span></h3><p>.&#x2F;sbin&#x2F;start-all.sh</p>
<p> .&#x2F;sbin&#x2F;mr-jobhistory-daemon.sh  start historyserver</p>
<p>localhost:50070</p>
<p>localhost:8088</p>
<h3><span id="others">Others</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># turn off hdfs safemode</span><br><span class="line">hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure>



<h2><span id="install-mysql">Install mysql</span></h2><p>使用docker安装简单更方便</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql-8.0.12-macos10.13-x86_64.dmg</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MYSQL_HOME=/usr/local/mysql</span><br><span class="line">export MYSQL_HOME</span><br><span class="line"></span><br><span class="line">PATH=.:$MYSQL_HOME/bin:$PATH</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure>

<p>mysql –version</p>
<p>mysql -uroot -p</p>
<h2><span id="install-hive">Install hive</span></h2><p>cd &#x2F;Users&#x2F;***&#x2F;Desktop&#x2F;software&#x2F;apache-hive-2.3.3-bin</p>
<p>download mirror: <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/">https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/</a></p>
<p>vi conf&#x2F;hive-site.xml</p>
<pre><code>#使用MySQL中的hive数据库，如果没有就创建一个
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
      &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
      &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
    &lt;/property&gt;

#连接mysql驱动
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
      &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
    &lt;/property&gt;

#用户名
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
      &lt;value&gt;hive&lt;/value&gt;
      &lt;description&gt;username to use against metastore database&lt;/description&gt;
    &lt;/property&gt;

#密码:
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
      &lt;value&gt;hive&lt;/value&gt;
      &lt;description&gt;password to use against metastore database&lt;/description&gt;
    &lt;/property&gt;

#日志目录
    &lt;property&gt;
      &lt;name&gt;hive.querylog.location&lt;/name&gt;
      &lt;value&gt;/Users/zhouqingfeng/Desktop/software/tmp/hive/querylog&lt;/value&gt;
      &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;
    &lt;/property&gt;


&lt;property&gt;
  &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
  &lt;value&gt;/Users/**/Desktop/software/tmp/hive/scratchdir&lt;/value&gt;
  &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
  &lt;value&gt;/Users/**/Desktop/software/tmp/hive/resoucesDir&lt;/value&gt;
  &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
  &lt;value&gt;/Users/**/Desktop/software/tmp/hive/operationlog&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<hr>
<h3><span id="start">Start</span></h3><p>启动hive metastore:</p>
<p>首先启动.&#x2F;hive –service metastore</p>
<p>hive client:</p>
<p>.&#x2F;hive</p>
<p>使用hiveserver2，然后使用beeline链接hive</p>
<p>.&#x2F;hive –service hiveserver2  |  hiveserver2   | lsof -i tcp:10000</p>
<p>beeline -u “jdbc:hive2:&#x2F;&#x2F;localhost:10000” </p>
<p>beeline -u “jdbc:hive2:&#x2F;&#x2F;192.168.43.65:10000”</p>
<p>hiveserver2 web url: <a target="_blank" rel="noopener" href="http://localhost:10002/">http://localhost:10002/</a></p>
<p>set hive.execution.engine &#x3D; mr;</p>
<p>beeline远程客户端配置：</p>
<p>修改conf&#x2F;hive-env.sh: HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.7.7</p>
<p>修改hadoop_home配置文件：&#x2F;opt&#x2F;hadoop-2.7.7&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh</p>
<h3><span id="references">References</span></h3><p><a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2016/01/600.htm">hiveserver2用户名和密码设置</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qiaojialin/article/details/55506439">https://blog.csdn.net/qiaojialin/article/details/55506439</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/micrari/p/7067968.html">https://www.cnblogs.com/micrari/p/7067968.html</a></p>
<h2><span id="install-hbase">Install  HBase</span></h2><h3><span id="pseudo-distributed-local-install">Pseudo-Distributed Local Install</span></h3><p>Hbase-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export  JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home </span><br></pre></td></tr></table></figure>

<p>Hbase-site.xml</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;hbase.rootdir&lt;/name&gt;
 &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>bin&#x2F;start-hbase.sh</p>
<p>bin&#x2F;stop-hbase.sh</p>
<p>Ref:<a target="_blank" rel="noopener" href="https://hbase.apache.org/book.html">https://hbase.apache.org/book.html</a></p>
<h3><span id="simple-usage">Simple Usage</span></h3><p>create  ‘test’, ‘cf1’</p>
<p>list  ‘test’  &#x2F;&#x2F;Use the <code>list</code> command to confirm your table exists</p>
<p>describe ‘test’</p>
<p>&#x2F;&#x2F;insert</p>
<p>put ‘test’,  ‘row1’,  ‘cf1:c1’,  ‘c1’</p>
<p>put ‘test’, ‘row1’, ‘cf1:a’, ‘value1’ </p>
<p>put ‘test’,  ‘row1’,   ‘cf1:c2’,  ‘c2’</p>
<p>put ‘test’,  ‘row2’,   ‘cf1:c3’,  ‘value1’</p>
<p>&#x2F;&#x2F;select</p>
<p>scan ‘test’</p>
<p>get ‘test’,  ‘row1’</p>
<p>get ‘test’,  ‘row1’,  ‘cf1’</p>
<p>get ‘test’,  ‘row1’,  ‘cf1:c1’</p>
<h3><span id="phoenix">phoenix</span></h3><h2><span id="install-zookeeper">Install Zookeeper</span></h2><p><a target="_blank" rel="noopener" href="http://apache.website-solution.net/zookeeper/">http://apache.website-solution.net/zookeeper/</a></p>
<h2><span id="install-kafka">Install Kafka</span></h2><h2><span id="install-spark">Install Spark</span></h2><h3><span id="spark-local">Spark  local</span></h3><h3><span id="spark-yarn-deploy">Spark  yarn deploy</span></h3><p>1、<a href="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/#spark-envsh">转spark 环境搭建</a></p>
<p>2、配置 spark history server:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vi conf/spark-defaults.conf</span><br><span class="line"></span><br><span class="line">spark.driver.memory              2g</span><br><span class="line"></span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://localhost:9000/tmp/spark-history-server</span><br><span class="line">spark.history.fs.logDirectory    hdfs://localhost:9000/tmp/spark-history-server</span><br><span class="line"></span><br><span class="line">spark.yarn.jars                  hdfs://localhost:9000/spark3/jars/*</span><br><span class="line">spark.yarn.historyServer.address localhost:18080</span><br><span class="line"></span><br><span class="line">./sbin/start-history-server.sh </span><br></pre></td></tr></table></figure>

<p>3、测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>



<h3><span id="spark-to-hbase">spark  to  hbase</span></h3><p>ETL</p>
<p>mysql  -&gt;  hive -&gt; spark sql  ?</p>
<p>machine learning  ?</p>
<p>spark + machine learning  ?</p>
<p>spark + AI ?</p>
<h2><span id="hive-to-or-from-hbase">Hive to (or from) Hbase</span></h2><h3><span id="hive-to-hbase">Hive to hbase</span></h3><p>create hive:</p>
<p>CREATE  TABLE tb2 (<br>name  string,<br>age int,<br>addr string<br>)<br>COMMENT ‘test table’</p>
<p>ROW FORMAT DELIMITED</p>
<p>FIELDS TERMINATED BY ‘,’</p>
<p>STORED AS textfile;</p>
<p>load data to hive:</p>
<p>hive  -e  “LOAD DATA LOCAL INPATH ‘&#x2F;Users&#x2F;zhouqingfeng&#x2F;Desktop&#x2F;software&#x2F;tmp&#x2F;hive&#x2F;definedata&#x2F;tb2&#x2F;tb2.txt’ OVERWRITE INTO TABLE default.tb2” </p>
<p>create another hive table:</p>
<p>hbase: create  ‘test_tb2’,  ‘cf’</p>
<p>create external table tb2_cp (<br>name  string,<br>age int,<br>addr string<br>) </p>
<p>STORED BY ‘org.apache.hadoop.hive.hbase.HBaseStorageHandler’</p>
<p>WITH SERDEPROPERTIES (“hbase.columns.mapping” &#x3D; “:key,  cf:age,  cf:addr”)</p>
<p>TBLPROPERTIES (“hbase.table.name” &#x3D; “test_tb2”);</p>
<p>insert into table:  this is slow because of mapreduce job</p>
<p>insert into table  tb2_cp select * from tb2;  </p>
<h3><span id="reference">Reference</span></h3><p><a target="_blank" rel="noopener" href="http://bigdataprogrammers.com/data-migration-from-hive-to-hbase/">hive to hbase</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" data-id="cljnns25s0000sq9aca136t58" data-title="hadoop本地环境搭建" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_env/spark环境搭建" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T01:48:42.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdate-spark-spark-env/">bigdate/spark/spark_env</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">spark环境搭建</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#hive">Hive</a></li>
<li><a href="#install-spark">Install Spark</a><ul>
<li><a href="#spark-local">Spark local</a></li>
<li><a href="#spark-standalone">spark standalone</a></li>
</ul>
</li>
<li><a href="#spark-yarn-deploy">Spark yarn deploy</a><ul>
<li><a href="#spark-envsh">spark-env.sh</a></li>
<li><a href="#spark-defaultsconf">spark-defaults.conf</a></li>
</ul>
<ul>
<li><a href="#spark-connect-hive">Spark connect Hive</a></li>
<li><a href="#spark-to-hbase">spark to  hbase</a></li>
</ul>
</li>
<li><a href="#spark%E6%B5%8B%E8%AF%95">Spark测试</a><ul>
<li><a href="#spark-local">spark local</a></li>
<li><a href="#spark-standalone">spark standalone</a></li>
<li><a href="#spark-yarn">spark yarn</a></li>
<li><a href="#spark-thrift-sever">Spark Thrift Sever</a></li>
</ul>
</li>
<li><a href="#spark-debug">Spark Debug</a><ul>
<li><a href="#yarn">YARN</a><ul>
<li><a href="#driver">Driver</a></li>
<li><a href="#executor">Executor</a></li>
</ul>
</li>
<li><a href="#standalone">Standalone</a><ul>
<li><a href="#master">Master</a></li>
<li><a href="#worker">Worker</a></li>
</ul>
</li>
<li><a href="#problems">Problems</a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#sparksumit-parameters">SparkSumit Parameters</a><ul>
<li><a href="#jars">jars</a></li>
<li><a href="#files">files</a><ul>
<li><a href="#usage1">usage1</a></li>
<li><a href="#usage2">usage2</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-compile">Spark Compile</a><ul>
<li><a href="#spark2">Spark2</a></li>
<li><a href="#spark3">Spark3</a></li>
</ul>
</li>
<li><a href="#spark3-1">Spark3</a><ul>
<li><a href="#connect-hive">connect hive</a></li>
<li><a href="#spark2%E5%8D%87%E7%BA%A7%E5%88%B0spark3">spark2升级到spark3</a><ul>
<li><a href="#fasterxml%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98">fasterxml不兼容问题</a></li>
<li><a href="#codehaus%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98">codehaus报错问题</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-windows-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">Spark Windows 环境搭建</a></li>
<li><a href="#spark%E6%8F%90%E4%BA%A4%E6%9C%AC%E5%9C%B0jar%E5%8C%85%E5%88%B0%E8%BF%9C%E7%A8%8Bhadoop%E9%9B%86%E7%BE%A4">spark提交本地jar包到远程Hadoop集群</a><ul>
<li><a href="#linuxmac-remote-cluster">Linux+mac + remote cluster</a></li>
<li><a href="#windows-remote-kerberos-cluster">Windows + remote kerberos cluster</a></li>
</ul>
</li>
<li><a href="#spark-%E8%BF%9E%E6%8E%A5tdh-hive">Spark 连接tdh hive</a><ul>
<li><a href="#classnotfound">ClassnotFound</a></li>
<li><a href="#jdk%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98">jdk版本问题</a></li>
<li><a href="#kerberos">kerberos</a></li>
<li><a href="#%E6%97%B6%E9%92%9F%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98">时钟同步问题</a></li>
<li><a href="#%E6%97%B6%E5%8C%BA%E8%AE%BE%E7%BD%AE%E9%97%AE%E9%A2%98">时区设置问题</a></li>
<li><a href="#tdh-hive%E9%85%8D%E7%BD%AE%E5%B1%9E%E6%80%A7">tdh hive配置属性</a></li>
<li><a href="#client-%E7%AB%AF%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1-unknownhost-%E5%BC%82%E5%B8%B8">Client 端提交任务 unknownhost 异常</a></li>
<li><a href="#%E5%A4%96%E7%BD%91tdh60-%E5%8F%A6%E4%B8%80%E4%B8%AA%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95">外网tdh6.0 另一个集群测试</a></li>
<li><a href="#spark-%E8%BF%9E%E6%8E%A5-inceptor-%E7%9A%84%E6%96%B9%E6%B3%95">spark 连接 inceptor 的方法</a></li>
<li><a href="#%E6%96%87%E6%A1%A3%E6%89%8B%E5%86%8C">文档手册</a></li>
</ul>
</li>
<li><a href="#hive-on-spark">hive on spark</a></li>
<li><a href="#hive-partition">hive partition</a><ul>
<li><a href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA">动态分区</a></li>
</ul>
</li>
<li><a href="#hive-hplsql">hive HPLSQL</a></li>
<li><a href="#others">Others</a><ul>
<li><a href="#spark-3-%E6%96%B0%E7%89%B9%E6%80%A7httpssparkapacheorgreleasesspark-release-3-0-0htmlspma2c6h128736390070a07c17h6eldk">Spark 3 新特性</a></li>
<li><a href="#spark-sql%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6">spark sql小文件合并</a></li>
</ul>
</li>
<li><a href="#references-1">References</a></li>
</ul>
<!-- tocstop -->

<h2><span id="hive">Hive</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nohup ./bin/hive --service metastore &amp;</span><br><span class="line">nohup ./bin/hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>



<h2><span id="install-spark">Install Spark</span></h2><h3><span id="spark-local">Spark  local</span></h3><h3><span id="spark-standalone">spark  standalone</span></h3><p>vi  $spark_home&#x2F;conf&#x2F;slaves</p>
<p>sh $spark_home&#x2F;sbin&#x2F;start-all.sh</p>
<h2><span id="spark-yarn-deploy">Spark  yarn deploy</span></h2><h4><span id="spark-envsh">spark-env.sh</span></h4><p>vi   $spark_home&#x2F;conf&#x2F;spark-env.sh</p>
<p>HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop<br>YARN_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop</p>
<p>core-site.xml</p>
<p>hive-site.xml</p>
<p> hdfs-site.xml </p>
<p> yarn-site.xml</p>
<p>export  HADOOP_USER_NAME&#x3D;hdfs<br>export  SPARK_HOME&#x3D;&#x2F;opt&#x2F;spark&#x2F;test&#x2F;spark-2.4.0-bin-hadoop2.7<br>HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;spark&#x2F;test&#x2F;spark-2.4.0-bin-hadoop2.7&#x2F;conf<br>YARN_CONF_DIR&#x3D;&#x2F;opt&#x2F;spark&#x2F;test&#x2F;spark-2.4.0-bin-hadoop2.7&#x2F;conf</p>
<p><strong>注意</strong></p>
<p>cp  mysql-connector-java-8.0.12.jar   $spark_home&#x2F;jars&#x2F;</p>
<h4><span id="spark-defaultsconf">spark-defaults.conf</span></h4><p>cp spark-defaults.conf.template spark-defaults.conf</p>
<p>vi $spark_home&#x2F;conf&#x2F;spark-defaults.conf</p>
<p>#上传jar包到指定hdfs位置</p>
<p>spark.yarn.jars&#x3D;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;spark&#x2F;jars&#x2F;jars&#x2F;*</p>
<p>spark.sql.shuffle.partitions.num 200 (修改默认属性值)</p>
<h3><span id="spark-connect-hive">Spark connect Hive</span></h3><p>Hive 1.2</p>
<p>spark-2.4.0-bin-hadoop2.7</p>
<p>cp  $hive_home&#x2F;conf&#x2F;hive-site.xml  $spark_home&#x2F;conf&#x2F;</p>
<p><strong>Error</strong>: Hive Schema version 1.2.0 does not match metastore’s schema version 2.3.0</p>
<p>修改hive-site.xml  hive.metastore.schema.verification 为false</p>
<h3><span id="spark-to-hbase">spark  to  hbase</span></h3><p>ETL</p>
<p>mysql  -&gt;  hive -&gt; spark sql  ?</p>
<p>machine learning  ?</p>
<p>spark + machine learning  ?</p>
<p>spark + AI ?</p>
<h2><span id="spark测试">Spark测试</span></h2><h3><span id="spark-local">spark local</span></h3><p>.&#x2F;bin&#x2F;spark-submit –class org.apache.spark.examples.SparkPi <br>    –master local[2] <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g <br>    examples&#x2F;jars&#x2F;spark-examples*.jar <br>    10</p>
<p>.&#x2F;bin&#x2F;spark-shell <br>    –master local[2] <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g</p>
<p>val ds1 &#x3D; Seq(1,2,3).toDS<br>ds1.count</p>
<p>.&#x2F;bin&#x2F;spark-sql <br>    –master local[2] <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g</p>
<h3><span id="spark-standalone">spark standalone</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  spark://Jon:7077 \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --total-executor-cores  2 \</span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-shell \</span><br><span class="line">    --master spark://Jon:7077 \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory  1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --total-executor-cores  2</span><br><span class="line"></span><br><span class="line">val ds1 = Seq(1,2,3).toDS</span><br><span class="line">ds1.count</span><br><span class="line"></span><br><span class="line">./bin/spark-shell \</span><br><span class="line">    --master spark://Jon:7077 \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --total-executor-cores  2</span><br></pre></td></tr></table></figure>



<h3><span id="spark-yarn">spark yarn</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  2 \</span><br><span class="line">   --executor-cores 1</span><br><span class="line"></span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 4 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line"></span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 4 \</span><br><span class="line">   hdfs://localhost:9000/spark/tmp/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line">   </span><br><span class="line">   ./bin/spark-submit --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 2 \</span><br><span class="line">   --principal *** \</span><br><span class="line">   --keytab  ***   \</span><br><span class="line">   --files /opt/test/test.json </span><br><span class="line">   bdata-compute-spark-jdbc-***.jar \</span><br><span class="line">   test.json</span><br><span class="line">   </span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh  test.sh  &gt; test.sh.log    2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./bin/spark-submit --class com.zhou.spark.sql.hive.TestHive  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test123  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">/Users/zhouqingfeng/Desktop/mydirect/github/distributedStudy/spark/sourcecode/projects/spark_all_test/target/spark_all_test-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/home/etl/bdata/env/spark_env/bin/spark-submit \</span><br><span class="line">    --class com.zhou.spark.sql.hive.TestHive  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test1234567  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 8g \</span><br><span class="line">    --executor-memory 6g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    --conf spark.sql.warehouse.dir=hdfs://10.20.29.103:8020/user/hive/warehouse   \</span><br><span class="line">    ./spark_all_test-jar-with-dependencies.jar \</span><br><span class="line">    thrift://node103:9083 nameservice1 namenode101  namenode161  node103:8020  node104:8020</span><br><span class="line">		</span><br><span class="line">1、读取hive大表不跨集群		</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">    --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test1234567  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 8g \</span><br><span class="line">    --executor-memory 6g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    --conf spark.sql.warehouse.dir=hdfs://10.20.30.112:8020/user/hive/warehouse   \</span><br><span class="line">    ./beta1192_bdata-compute-spark-jdbc-1.0.1.jar \</span><br><span class="line">    ***.json  thrift://bdp-1.rdc.com:9083,thrift://bdp-2.rdc.com:9083</span><br><span class="line">参数1: json文件</span><br><span class="line">参数2: thrift server地址，对应是hive-site.xml参数hive.metastore.uris</span><br><span class="line"></span><br><span class="line">2、读取hive大表跨集群：hive所在集群和spark计算所在集群是两个集群</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">    --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test1234567  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 8g \</span><br><span class="line">    --executor-memory 6g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    --conf spark.sql.warehouse.dir=hdfs://10.20.29.103:8020/user/hive/warehouse   \</span><br><span class="line">    ./beta1192_bdata-compute-spark-jdbc-1.0.1.jar \</span><br><span class="line">    /home/etl/test/testHive.json  thrift://node103:9083 nameservice1 namenode101  namenode161  node103:8020  node104:8020</span><br><span class="line">		</span><br><span class="line">参数1: json文件</span><br><span class="line">参数2: thrift server地址，对应是hive-site.xml参数hive.metastore.uris		</span><br><span class="line">参数3: nameservice1  namespace名字，对应是core-site.xml参数fs.defaultFS的值hdfs://nameservice1</span><br><span class="line">参数4: namenode1，namenode1名字，对应是hdfs-site.xml参数dfs.ha.namenodes.nameservice1</span><br><span class="line">参数5: namenode2，namenode2名字，对应是hdfs-site.xml参数dfs.ha.namenodes.nameservice1</span><br><span class="line">参数6: namenode1地址，对应是hdfs-site.xml参数dfs.namenode.rpc-address.nameservice1.namenode1</span><br><span class="line">参数7: namenode2地址，对应是hdfs-site.xml参数dfs.namenode.rpc-address.nameservice1.namenode2</span><br></pre></td></tr></table></figure>

<p>.&#x2F;bin&#x2F;spark-shell <br>    –master yarn <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g <br>    –executor-cores 1</p>
<p>val ds1 &#x3D; Seq(1,2,3).toDS<br>ds1.count</p>
<p>.&#x2F;bin&#x2F;spark-sql <br>    –master yarn <br>    –deploy-mode client <br>    –driver-memory 1g <br>    –executor-memory 1g <br>    –executor-cores 1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 --num-executors 2</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>





<h3><span id="spark-thrift-sever">Spark Thrift Sever</span></h3><p>.&#x2F;sbin&#x2F;start-thriftserver.sh <br>    –master yarn <br>    –deploy-mode client <br>    –driver-memory 512m <br>    –executor-memory 512m <br>    –executor-cores 1   –num-executors 3</p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;   -n hive </p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;10.26.119.144:10000&#x2F; </p>
<p>或者</p>
<p>.&#x2F;bin&#x2F;beeline<br>!connect jdbc:hive2:&#x2F;&#x2F;localhost:10000</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;10.26.119.144:10000</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;10.20.30.111:10000</p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;bdp-2:10000&#x2F; </p>
<p>&#x2F;&#x2F;开启kerberos</p>
<p>!connect  jdbc:hive2:&#x2F;&#x2F;10.20.30.111:10000&#x2F;test_zq;principal&#x3D;hive&#x2F;<a href="mailto:&#98;&#100;&#112;&#x32;&#64;&#x54;&#69;&#83;&#x54;&#46;&#x43;&#79;&#x4d;">&#98;&#100;&#112;&#x32;&#64;&#x54;&#69;&#83;&#x54;&#46;&#x43;&#79;&#x4d;</a>;authentication&#x3D;kerberos</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://bdp-3:10000/default</span><br></pre></td></tr></table></figure>

<p>用户可以远程连接thrift server, 提交spark sql</p>
<h2><span id="spark-debug">Spark Debug</span></h2><h3><span id="yarn">YARN</span></h3><h4><span id="driver">Driver</span></h4><p>spark.driver.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>（1）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   --driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5009 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line">   </span><br><span class="line">   ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   --driver-java-options &quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5009&quot; \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5009</p>
<p>(3) 加断点：</p>
<p>起始类：org.apache.spark.deploy.SparkSubmit</p>
<p>(4) debug idea remote!</p>
<h4><span id="executor">Executor</span></h4><p>spark.executor.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>（1）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  1 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   --conf &quot;spark.executor.extraJavaOptions=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5010&quot; \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5010</p>
<p>(3) 加断点：</p>
<p>起始类：CoarseGrainedExecutorBackend.main( )</p>
<p>可以在ShuffleMapTask 和ResultTask 添加，executor启动后，执行这两类任务</p>
<p>(4) debug idea remote!</p>
<h3><span id="standalone">Standalone</span></h3><h4><span id="master">Master</span></h4><p>(1)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#调试Master，在master节点的spark-env.sh中添加SPARK_MASTER_OPTS变量</span><br><span class="line">export SPARK_MASTER_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10000&quot;</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;10000</p>
<p>(3) 加断点：</p>
<p>起始类：org.apache.spark.deploy.master.Master</p>
<p>(4) debug idea remote!</p>
<h4><span id="worker">Worker</span></h4><p>(1)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#调试Worker，在worker节点的spark-env.sh中添加SPARK_WORKER_OPTS变量</span><br><span class="line">export SPARK_WORKER_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10001&quot;</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;10001</p>
<p>(3) 加断点：</p>
<p>起始类：org.apache.spark.deploy.worker.Worker</p>
<p>(4) debug idea remote!</p>
<h3><span id="problems">Problems</span></h3><p>执行调试的源码一定要和对应的包(测试包&#x2F;测试脚本)一致</p>
<h3><span id="references">References</span></h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/breg/p/8427199.html">https://www.cnblogs.com/breg/p/8427199.html</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a69d46a6b923?tdsourcetag=s_pcqq_aiomsg">https://www.jianshu.com/p/a69d46a6b923?tdsourcetag=s_pcqq_aiomsg</a></p>
<p><a target="_blank" rel="noopener" href="https://medium.com/agile-lab-engineering/spark-remote-debugging-371a1a8c44a8">spark remote debugging</a></p>
<p><a target="_blank" rel="noopener" href="https://www.iteblog.com/archives/1192.html">过往记忆</a></p>
<h2><span id="sparksumit-parameters">SparkSumit Parameters</span></h2><h3><span id="jars">jars</span></h3><p>Comma-separated list of jars to include on the driver<br>                              and executor classpaths.</p>
<p>多个依赖jar包 以, 分割，jar包路径设置为client端本地文件路径即可，最终所有依赖jar 会放置在driver端和executor端的classpath路径下。</p>
<h3><span id="files">files</span></h3><p>Comma-separated list of files to be placed in the working<br>                              directory of each executor. File paths of these files<br>                              in executors can be accessed via SparkFiles.get(fileName).</p>
<p>多个依赖文件以,, 分割，jar包路径设置为client端本地文件路径，最终所有依赖文件 会放置在每个executor节点的临时文件路径下，executor可以通过SparkFiles.get(fileName) 读取到对应文件名字的依赖文件。（类似于广播变量，广播该文件到每一个执行节点，）</p>
<h4><span id="usage1">usage1</span></h4><p>每个Executor 节点获取文件，local | yarn client| yarn cluster 都支持：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jdbcDF.foreach(new ForeachFunction() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void call(Object row) throws Exception &#123;</span><br><span class="line">        System.out.println(row.toString());</span><br><span class="line"></span><br><span class="line">        FileReader fr = new FileReader(SparkFiles.get(fileName));</span><br><span class="line">        BufferedReader br = new BufferedReader(fr);</span><br><span class="line">        String line = br.readLine();</span><br><span class="line">        while(line != null) &#123;</span><br><span class="line">            System.out.println(line);</span><br><span class="line">            line = br.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h4><span id="usage2">usage2</span></h4><p>直接通过filename 获取文件，不能通过SparkFiles.get(fileName) 获取文件，</p>
<p><strong>只能用在yarn cluster模式</strong>，driver端直接通过filename 获取文件(driver节点也对应一个集群节点)，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 2 \</span><br><span class="line">   --principal *** \</span><br><span class="line">   --keytab  ***   \</span><br><span class="line">   --files /opt/test/test.json </span><br><span class="line">   bdata-compute-spark-jdbc-***.jar \</span><br><span class="line">   test.json</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   //yarn cluster 模式，driver端直接通过filename:test.json 获取到对应文件 </span><br><span class="line">   //test.json被广播到所有executor节点上，</span><br><span class="line">   FileReader fr = new FileReader(&quot;test.json&quot;);</span><br><span class="line">   BufferedReader br = new BufferedReader(fr);</span><br><span class="line">   String line = br.readLine();</span><br><span class="line">   while(line != null) &#123;</span><br><span class="line">   System.out.println(line);</span><br><span class="line">   line = br.readLine();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>





<h2><span id="spark-compile">Spark Compile</span></h2><h3><span id="spark2">Spark2</span></h3><p>.&#x2F;dev&#x2F;make-distribution.sh –name spark-2.4.7-bin-hadoop2.7  –tgz  -Phadoop-2.7 -Phive -Phive-thriftserver -Pyarn</p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.7/building-spark.html">https://spark.apache.org/docs/2.4.7/building-spark.html</a></p>
<h3><span id="spark3">Spark3</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./dev/make-distribution.sh --name spark-3.0.1-bin-hadoop2.7.7  --tgz  -Phive -Phive-thriftserver  -Pyarn</span><br><span class="line"></span><br><span class="line">修改hive 版本1.2.2 编译失败</span><br></pre></td></tr></table></figure>



<h2><span id="spark3">Spark3</span></h2><h3><span id="connect-hive">connect hive</span></h3><p>spark包的 hive版本和线上Hive版本不一致，出现Invalid method name: ‘get_table_req’</p>
<p>(hive 版本需要 &gt; 2.3)</p>
<p>Solutions: </p>
<p>1、<a target="_blank" rel="noopener" href="https://github.com/apache/spark/pull/27161">https://github.com/apache/spark/pull/27161</a></p>
<p>add configuration: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.sql.hive.metastore.version=1.2.2 </span><br><span class="line">--conf spark.sql.hive.metastore.jars=/root/hive-1.2.2-lib/*</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shenyunsese/article/details/111299659">替换spark3 jars，未验证成功</a></p>
<p>2、重新编译Spark3，指定Hive版本，compile failed</p>
<h3><span id="spark2升级到spark3">spark2升级到spark3</span></h3><p>spark2升级到 Spark3 版本3.0.3，scala2.12.10，hadoop版本2.6.4</p>
<h4><span id="fasterxml不兼容问题">fasterxml不兼容问题</span></h4><p>Fasterxml.jackson.module 升级到jackson-module-scala_2.12:2.10.5</p>
<h4><span id="codehaus报错问题">codehaus报错问题</span></h4><p>添加dependency，org.codehaus.janino:janino:3.0.8  和 org.codehaus.janino:commons-compiler:3.0.8</p>
<h2><span id="spark-windows-环境搭建">Spark Windows 环境搭建</span></h2><p>hadoop 本地2.7 spark local模式，连接cdh6.3.2  hive（cdh开启了kerberos验证）</p>
<p>1、下载hadoop包，如hadoop-2.7.7.tar.gz</p>
<p>2、下载winutils.exe，放置在hadoop_home&#x2F;bin目录下</p>
<p>3、spark main class 添加环境变量 HADOOP_HOME&#x3D;D:\software\hadoop2.7;HADOOP_USER_NAME&#x3D;HDFS(或者对应其他用户)</p>
<p>4、copy core-site.xml hdfs-site.xml hive-site.xml yarn-site.xml 到项目的resources目录</p>
<p>注意：一定要查看是否生效，或者出于调试考虑，为了保障一定能生效，可以手动加载这几个配置文件，如下所示：</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“core-site.xml”))</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“hdfs-site.xml”))</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“hive-site.xml”))</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“yarn-site.xml”))</p>
<p>5、Spark driver class：</p>
<p>由于sparksession 初始化的时候，会连接hive thrift server，获取Hive Catalog</p>
<p>同时由于hive 开启了kerberos验证，因此 sparksession 初始化前，需要先进行kerberos验证：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(&quot;java.security.krb5.conf&quot;,  &quot;&quot;)  //c:/users/zhouqf/cdh6/krb5.con</span><br><span class="line">val conf = new Configuration</span><br><span class="line">conf.set(&quot;fs.trash.interval&quot;, &quot;14400&quot;)  //不会更改hdfs实际参数</span><br><span class="line">conf.set(&quot;hadoop.security.authentication&quot;,  &quot;Kerberos&quot;)</span><br><span class="line">import org.apache.hadoop.security.UserGroupInformation  //(hadoop-common包)</span><br><span class="line">UserGroupInformation.setConfiguration(conf)</span><br><span class="line">//hive/bdp2@TEST.COM   c:/users/zhouqf/cdh6/hive.keytab </span><br><span class="line">UserGroupInformation.loginUserFromKeytab(&quot;principal&quot;,  &quot;keytabfile&quot;) </span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p>6、需要修改本地 windows机器上的文件权限，报错提示:&#x2F;tmp&#x2F;hive 没有权限</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/34196302/the-root-scratch-dir-tmp-hive-on-hdfs-should-be-writable-current-permissions">The root scratch dir: &#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rw-rw-rw- (on Windows)</a></p>
<p>假如winutils.exe 所在目录为D:\winutils\bin\winutils.exe，</p>
<p>在windows 上可登陆powershell 执行命令，cd  \winutils\bin,  然后.\winutils.exe  chmod 777 D:\tmp\hive</p>
<p>spark中的三种参数配置：</p>
<p>spark自身相关，如spark.sql.warehouse.dir</p>
<p>hadoop相关：–conf spark.hadoop.abc.def&#x3D;xyz   represents adding hadoop property “abc.def&#x3D;xyz”,</p>
<p>Hive 相关：–conf spark.hive.abc&#x3D;xyz  represents adding hive property “hive.abc&#x3D;xyz”.</p>
<h2><span id="spark提交本地jar包到远程hadoop集群">spark提交本地jar包到远程Hadoop集群</span></h2><h3><span id="linuxmac-remote-cluster">Linux+mac + remote cluster</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.launcher.SparkAppHandle;</span><br><span class="line">import org.apache.spark.launcher.SparkLauncher;</span><br><span class="line"></span><br><span class="line">import java.util.HashMap;</span><br><span class="line"></span><br><span class="line">public class TestSparkLauncher &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; envParams = new HashMap&lt;&gt;();</span><br><span class="line">//        envParams.put(&quot;YARN_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">//        envParams.put(&quot;HADOOP_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_HOME&quot;, &quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;, &quot;1&quot;);</span><br><span class="line">//        envParams.put(&quot;JAVA_HOME&quot;, &quot;/usr/java&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SparkLauncher launcher = new SparkLauncher(envParams)</span><br><span class="line">                .setAppResource(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;)</span><br><span class="line">                .setMainClass(&quot;org.apache.spark.examples.SparkPi&quot;)</span><br><span class="line">                .setMaster(&quot;yarn&quot;)</span><br><span class="line">                .setDeployMode(&quot;client&quot;)</span><br><span class="line">                .addAppArgs(&quot;20&quot;);</span><br><span class="line"></span><br><span class="line">        String appName = &quot;sparklancherTest&quot;;</span><br><span class="line">        launcher.setConf(&quot;spark.executor.memory&quot;, &quot;1g&quot;);</span><br><span class="line">        launcher.setAppName( appName );</span><br><span class="line">        SparkAppHandle sparkHandle = launcher.startApplication();</span><br><span class="line"></span><br><span class="line">        while(!sparkHandle.getState().isFinal()) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(1000);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="windows-remote-kerberos-cluster">Windows + remote kerberos cluster</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public class TestSparkLauncher &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; envParams = new HashMap&lt;&gt;();</span><br><span class="line">        envParams.put(&quot;YARN_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">//        envParams.put(&quot;HADOOP_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_HOME&quot;, &quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;, &quot;1&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_SUBMIT_OPTS&quot;, &quot;-Djava.security.krb5.conf=D:/projects/spark-compute/conf/krb5.conf&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SparkLauncher launcher = new SparkLauncher(envParams)</span><br><span class="line">                .setAppResource(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;)</span><br><span class="line">                .setMainClass(&quot;org.apache.spark.examples.SparkPi&quot;)</span><br><span class="line">                .setMaster(&quot;yarn&quot;)</span><br><span class="line">                .setDeployMode(&quot;client&quot;)</span><br><span class="line">                .addAppArgs(&quot;20&quot;);</span><br><span class="line">        launcher.addSparkArg(&quot;--keytab&quot;, &quot;D:/projects/spark-compute/conf/hive.keytab&quot;);</span><br><span class="line">        launcher.addSparkArg(&quot;--principal&quot;, &quot;hive@TEST.COM&quot;);        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String appName = &quot;sparklancherTest&quot;;</span><br><span class="line">        launcher.setConf(&quot;spark.executor.memory&quot;, &quot;1g&quot;);</span><br><span class="line">        launcher.setAppName( appName );</span><br><span class="line">        SparkAppHandle sparkHandle = launcher.startApplication();</span><br><span class="line"></span><br><span class="line">        while(!sparkHandle.getState().isFinal()) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(1000);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="spark-连接tdh-hive">Spark 连接tdh hive</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">tdh版本6.2</span><br><span class="line">本地测试需要在resouces添加hive-site.xml、core-site.xml、hdfs-site.xml</span><br><span class="line">本地连接时，core-site.xml 放置在本地，也需要修改属性 为本地目录hadoop.security.group.mapping.ldap.bind.password.file=/etc/hdfs1/conf/ldap-conn-pass.txt</span><br><span class="line"></span><br><span class="line">10.20.30.50  root   BData.COM </span><br><span class="line">spark_home:  /opt/bdata/env/spark_env</span><br><span class="line"></span><br><span class="line">./bin/spark-sql  \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client  \</span><br><span class="line">--executor-memory 1G  \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">--executor-cores 1  \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--conf spark.yarn.appMasterEnv.JAVA_HOME=&quot;/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--conf &quot;spark.executorEnv.JAVA_HOME=/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--hiveconf hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider \</span><br><span class="line">--principal hive@TDH  \</span><br><span class="line">--keytab /home/tdh/hive.keytab</span><br><span class="line"></span><br><span class="line">//spark-submit</span><br><span class="line"></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">--class com.**.TaskRunner \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client  \</span><br><span class="line">--executor-memory 1G  \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">--executor-cores 1  \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--conf spark.yarn.appMasterEnv.JAVA_HOME=&quot;/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--conf &quot;spark.executorEnv.JAVA_HOME=/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--conf spark.hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider \</span><br><span class="line">--principal hive@TDH  \</span><br><span class="line">--keytab /home/tdh/hive.keytab</span><br></pre></td></tr></table></figure>

<h3><span id="classnotfound">ClassnotFound</span></h3><p>跳过tdh guardian 权限管理，</p>
<p>配置hive 参数hive.security.authorization.manager&#x3D;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</p>
<h3><span id="jdk版本问题">jdk版本问题</span></h3><p>Tdh6.2 需要指定JDK版本 &gt;&#x3D; 1.8.0_241</p>
<p>–conf spark.yarn.appMasterEnv.JAVA_HOME&#x3D;”&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_241” <br>–conf “spark.executorEnv.JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_241” \</p>
<h3><span id="kerberos">kerberos</span></h3><p>如果需要指定krb5.conf, 配置   –conf spark.driver.extraJavaOptions&#x3D;-Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;tdh-test&#x2F;krb5.conf </p>
<h3><span id="时钟同步问题">时钟同步问题</span></h3><p>如下表示同步当前server时间与192.168.60.14服务器一致</p>
<p>cd &#x2F;bin&#x2F;<br>systemctl stop ntpd.service<br>ntpdate  192.168.60.14<br>systemctl start  ntpd.service<br>systemctl status  ntpd.service  </p>
<h3><span id="时区设置问题">时区设置问题</span></h3><p>mv &#x2F;etc&#x2F;localtime &#x2F;etc&#x2F;localtime.bak<br>ln -s &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai  &#x2F;etc&#x2F;localtime </p>
<p>删除软连接： unlink   &#x2F;etc&#x2F;localtime</p>
<h3><span id="tdh-hive配置属性">tdh hive配置属性</span></h3><p>定义hive conf property,覆盖hive-site.xml属性</p>
<p>Spark-submit:</p>
<p>–conf spark.hive.security.authorization.manager&#x3D;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</p>
<p>Spark-sql:</p>
<p>–hiveconf hive.security.authorization.manager&#x3D;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</p>
<h3><span id="client-端提交任务-unknownhost-异常">Client 端提交任务 unknownhost 异常</span></h3><p>Spark-submit 提交yarn，注册生成applicationmaster之后，去跟driver(client模式，driver位于client端)交互，但是总是无法识别driver对应Hostname，查看centos  &#x2F;etc&#x2F;hosts是配置过hostname的，而且ping 命令是通的，好久思考之后，发现问题还是因为driver端&#x2F;etc&#x2F;hosts配置的问题引起的，修改之后，问题就解决了。</p>
<p><strong>错误写法</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1  localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">10.20.29.125  mysql-125</span><br></pre></td></tr></table></figure>

<p><strong>正确写法</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1  mysql-125  localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="外网tdh60-另一个集群测试">外网tdh6.0 另一个集群测试</span></h3><p>使用spark-submit 测试：</p>
<p>Spark-submit 命令 </p>
<p>（1）修改hive-site.xml属性 hive.security.authorization.manager</p>
<p>–conf spark.hadoop.hive.security.authorization.manager&#x3D;”org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider”</p>
<p>这样写才能生效，使用以下写法报错了，似乎不行。</p>
<p>–conf spark.hive.security.authorization.manager&#x3D;”org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider”</p>
<p>（2）</p>
<p>copy  core-site.xml对应属性文件到client 对应目录，要与core-site.xml中的目录一致<br>–conf  net.topology.script.file.name&#x3D;&#x2F;usr&#x2F;lib&#x2F;transwarp&#x2F;scripts&#x2F;rack_map.sh<br>–conf  hadoop.security.group.mapping.ldap.bind.password.file&#x3D;&#x2F;etc&#x2F;hdfs1&#x2F;conf&#x2F;ldap-conn-pass.txt</p>
<h3><span id="spark-连接-inceptor-的方法">spark 连接 inceptor 的方法</span></h3><p>官方文档：</p>
<p><a target="_blank" rel="noopener" href="https://nj.transwarp.cn:8180/?p=3382">https://nj.transwarp.cn:8180/?p=3382</a></p>
<h3><span id="文档手册">文档手册</span></h3><p><a target="_blank" rel="noopener" href="http://support.transwarp.cn/t/topic/3262">http://support.transwarp.cn/t/topic/3262</a></p>
<h2><span id="hive-on-spark">hive on spark</span></h2><p>spark2.4</p>
<p>1、spark源码编译编译： .&#x2F;dev&#x2F;make-distribution.sh –name “hadoop2-without-hive” –tgz “-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided,orc-provided” （必须拥有<strong>不</strong>包含Hive jar 的Spark版本 。Spark的发行版本为了兼顾Spark SQL都会包含有Hive相关的jar,所以我们需要通过源码重新编译,去重相关的jar.）</p>
<p>2、配置Hive</p>
<p>cp scala-library-2.11.8.jar   $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>cp spark-core_2.11-2.0.2.jar  $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>cp spark-network-common_2.11-2.0.2.jar  $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>cp spark-unsafe_2.11-2.4.7.jar   $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>hive-site.xml  修改配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.execution.engine&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;spark&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.yarn.jars&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://localhost:9000/spark/jars/jars2/*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、hive on spark 任务参数：</p>
<p>set hive.execution.engine&#x3D;spark;  （      Expects one of [mr, tez, spark]    ）</p>
<p>set spark.executor.memory&#x3D;6g;<br>set spark.executor.cores&#x3D;3;<br>set spark.executor.instances&#x3D;40;</p>
<p>set spark.master&#x3D;yarn-client; </p>
<p>set spark.serializer&#x3D;org.apache.spark.serializer.KryoSerializer;</p>
<p>4、任务详细日志查看</p>
<p>hive  –hiveconf  hive.root.logger&#x3D;DEBUG,console  -e  “select count(1) from   test_db1.test_tb1”</p>
<p>5、references:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/pucao_cug/article/details/72783688">https://blog.csdn.net/pucao_cug/article/details/72783688</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/339da2b6d480">https://www.jianshu.com/p/339da2b6d480</a></p>
<h2><span id="hive-partition">hive partition</span></h2><h3><span id="动态分区">动态分区</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">create table test_part2(id int, name string) </span><br><span class="line">partitioned by (country string, province string );</span><br><span class="line"></span><br><span class="line">--静态分区 指定分区字段的值</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;sh&#x27;)  select id, name from test_part1 where id = 1 and name = &#x27;lisa&#x27;;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;)  values(2, &#x27;dal&#x27;)</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;) select 2 as id, &#x27;cate&#x27; as name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--半动态分区(一部分静态分区 + 一部分动态分区)</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--全动态分区</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-sql \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;tez&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-shell   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;insert into table test_zhou.test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;newyork&#x27; as province&quot;)</span><br></pre></td></tr></table></figure>



<h2><span id="hive-hplsql">hive HPLSQL</span></h2><p>1、配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;m1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>





<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">hplsql-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.default&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hive2conn&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The default connection profile&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.hiveconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.jdbc.HiveDriver;jdbc:hive://&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Hive embedded JDBC (not requiring HiveServer)&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 配置项hive.execution.engine默认设置为mr,若使用spark作为引擎时,则设置为spark --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.init.hiveconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;</span><br><span class="line">     set mapred.job.queue.name=default;</span><br><span class="line">     set hive.execution.engine=spark; </span><br><span class="line">     use default;</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Statements for execute after connection to the database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.convert.hiveconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Convert SQL statements before execution&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.hive2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hive.jdbc.HiveDriver;jdbc:hive2://localhost:10000;zhouqingfeng;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;HiveServer2 JDBC connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 配置项hive.execution.engine默认设置为mr,若使用spark作为引擎时,则设置为spark --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.init.hive2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;</span><br><span class="line">     set mapred.job.queue.name=default;</span><br><span class="line">     set hive.execution.engine=spark; </span><br><span class="line">     use default;</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Statements for execute after connection to the database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.convert.hive2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Convert SQL statements before execution&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.db2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.ibm.db2.jcc.DB2Driver;jdbc:db2://localhost:50001/dbname;user;password&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;IBM DB2 connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.tdconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.teradata.jdbc.TeraDriver;jdbc:teradata://localhost/database=dbname,logmech=ldap;user;password&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Teradata connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.mysqlconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver;jdbc:mysql://localhost/test;user;password&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;MySQL connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.dual.table&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;default.dual&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Single row, single column table for internal operations&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.insert.values&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;native&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;How to execute INSERT VALUES statement: native (default) and select&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.onerror&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;exception&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Error handling behavior: exception (default), seterror and stop&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.temp.tables&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;native&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Temporary tables: native (default) and managed&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.temp.tables.schema&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Schema for managed temporary tables&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.temp.tables.location&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/zhouqingfeng/tmp&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;LOcation for managed temporary tables in HDFS&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 下面两项需要按实际情况修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">&lt;value&gt;localhost&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2、启动HiveServer2和Metastore服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh <span class="variable">$HIVE_HOME</span>/bin/hive  --service  metastore</span><br><span class="line">sh <span class="variable">$HIVE_HOME</span>/bin/hive  --service  hiveserver2</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、test:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/hplsql -e &quot;CURRENT_DATE+1&quot; </span><br><span class="line">./bin/hplsql  -e &#x27;select count(1) from test_db1.test_tb2&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>4、reference:</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/4d2035377753">https://www.jianshu.com/p/4d2035377753</a></p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/37831928/apache-hplsql-permission-denied-while-running-hplsql-query">https://stackoverflow.com/questions/37831928/apache-hplsql-permission-denied-while-running-hplsql-query</a></p>
<p><a target="_blank" rel="noopener" href="http://www.hplsql.org/if">http://www.hplsql.org/if</a></p>
<p>5、usage:</p>
<p><strong>define a function:</strong></p>
<p>cat   test_sql&#x2F;test_func1.sql</p>
<p>CREATE FUNCTION hello(text STRING)<br> RETURNS STRING<br>BEGIN<br> RETURN ‘Hello, ‘ || text || ‘!’;<br>END;</p>
<p>– Invoke the function<br>PRINT hello(‘world’);</p>
<p>hplsql -f  test_sql&#x2F;test_func1.sql</p>
<p><strong>define a <a target="_blank" rel="noopener" href="http://www.hplsql.org/udf-sproc">Procedures</a></strong> :</p>
<p>cat   test_sql&#x2F;test_proc1.sql</p>
<p>CREATE PROCEDURE set_message(IN name STRING, OUT result STRING)<br>BEGIN<br> SET result &#x3D; ‘Hello, ‘ || name || ‘!’;<br>END;</p>
<p>– Call the procedure and print the results<br>DECLARE str STRING;<br>CALL set_message(‘Jack’, str);<br>PRINT str;</p>
<p>hplsql -f  test_sql&#x2F;test_proc1.sql </p>
<p><strong>EXECUTE Statement</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DECLARE cnt INT;</span><br><span class="line">EXECUTE &#x27;SELECT COUNT(1) FROM test_db1.test_tb1&#x27; INTO cnt;</span><br><span class="line">PRINT cnt</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>hplsql  -e  “DECLARE cnt INT;EXECUTE ‘SELECT COUNT(1) FROM test_db1.test_tb1’ INTO cnt;PRINT cnt”</p>
<h2><span id="others">Others</span></h2><h3><span id="spark-3-新特性"></span></h3><p><strong>1、<a target="_blank" rel="noopener" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html?spm=a2c6h.12873639.0.0.70a07c17h6ELDK">自适应查询执行（Adaptive Query Execution）</a></strong></p>
<p>spark.sql.adaptive.enabled &#x3D; true</p>
<p> As soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc.</p>
<p>In Spark 3.0, the AQE framework is shipped with three features:</p>
<ul>
<li>Dynamically coalescing shuffle partitions</li>
<li>Dynamically switching join strategies</li>
<li>Dynamically optimizing skew joins</li>
</ul>
<p>允许spark 在运行过程中 根据已经执行完的stage的统计信息，动态优化调整逻辑执行计划和物理执行计划，主要包括以下三个方面的优化：</p>
<p>动态合并shuffle partition的数目；</p>
<p>动态调整 join 策略，sortmergejoin -》 broadcasthashjoin</p>
<p>动态优化产生倾斜的Join</p>
<p><strong>2、动态分区修剪（Dynamic Partition Pruning）</strong></p>
<p>spark.sql.optimizer.dynamicPartitionPruning.enabled </p>
<p>所谓的动态分区裁剪就是基于运行时（run time）推断出来的信息来进一步进行分区裁剪</p>
<p>3、对深度学习的增强，包括支持GPU计算等</p>
<p>4、对k8s更好的整合</p>
<p>5、数据湖delta lake更好的整合</p>
<p>6、For the Scala API, Spark 3.0.1 uses Scala 2.12. Python 2 and Python 3 prior to version 3.6 support is deprecated as of Spark 3.0.0.</p>
<h3><span id="spark-sql小文件合并">spark sql小文件合并</span></h3><p>(1) 对于原始数据进行按照分区字段进行shuffle，可以规避小文件问题。但有可能引入数据倾斜的问题；</p>
<p>(2) sql中引入 distribute by ，指定分区字段或分区表达式</p>
<p>(3) 已知倾斜key的情况，将数据分为两部分处理，倾斜部分按rand()函数 重分区，未倾斜部分常规处理</p>
<p>(4) 对于Spark 2.4 以上版本的用户，sql中 可以使用<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/SPARK-24940">HINT提示</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert into select /*+ REPARTITION(2) */  id,name  from tb1 where id &gt; 0</span><br><span class="line">insert into select /*+ COALESCE(2) */  id,name  from tb1 where id &gt; 0</span><br></pre></td></tr></table></figure>

<p>(5) spark3.0以上开启自适应查询执行：</p>
<p>spark.sql.adaptive.enabled&#x3D;true;</p>
<p>spark.sql.adaptive.coalescePartitions.enabled&#x3D;true;</p>
<p>spark.sql.adaptive.coalescePartitions.parallelismFirst &#x3D; false;</p>
<p>spark.sql.adaptive.advisoryPartitionSizeInBytes &#x3D; 64m;</p>
<h2><span id="references">References</span></h2><p><a href>waterdrop 如何更好的使用spark</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/apache/incubator-seatunnel">apache seatunnel</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" data-id="cljnnbsuy0000gl9a87717mmh" data-title="spark环境搭建" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-tools/draw/在线画图/processon" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T01:30:56.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/">processon</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2><span id="工具">工具</span></h2><p><a target="_blank" rel="noopener" href="https://www.processon.com/">免费在线流程图思维导图</a>:<a target="_blank" rel="noopener" href="https://www.processon.com/">https://www.processon.com/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/" data-id="cljnm9mja0000jg9a8gxe7mpu" data-title="processon" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/prometheus/prometheus使用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T17:43:13.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/">prometheus使用</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#prometheus">prometheus</a></li>
<li><a href="#grafana">Grafana</a><ul>
<li><a href="#dashboard">Dashboard</a></li>
</ul>
</li>
<li><a href="#%E9%87%87%E9%9B%86%E5%99%A8">采集器</a><ul>
<li><a href="#node_exporter">node_exporter</a></li>
<li><a href="#cadvisor">cadvisor</a></li>
<li><a href="#elasticsearch_exporter">elasticsearch_exporter</a></li>
<li><a href="#blackbox_exporter">blackbox_exporter</a><ul>
<li><a href="#refrerences">Refrerences</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="prometheus">prometheus</span></h2><p>.&#x2F;prometheus –config.file&#x3D;prometheus.yml</p>
<p><a target="_blank" rel="noopener" href="http://localhost:9090/">http://localhost:9090</a></p>
<h2><span id="grafana">Grafana</span></h2><h3><span id="dashboard">Dashboard</span></h3><p><a target="_blank" rel="noopener" href="https://grafana.com/grafana/dashboards/">Grafana 各种dashboard</a>:<a target="_blank" rel="noopener" href="https://grafana.com/grafana/dashboards/">https://grafana.com/grafana/dashboards/</a></p>
<p>bin&#x2F;grafana-server</p>
<p><a target="_blank" rel="noopener" href="http://localhost:3000/">http://localhost:3000/</a></p>
<h2><span id="采集器">采集器</span></h2><h3><span id="node_exporter">node_exporter</span></h3><p>curl  <a target="_blank" rel="noopener" href="http://localhost:9100/metrics">http://localhost:9100/metrics</a></p>
<h3><span id="cadvisor">cadvisor</span></h3><p><a target="_blank" rel="noopener" href="https://github.com/google/cadvisor">容器监控</a>：cAdvisor (Container Advisor) provides container users an understanding of the resource usage and performance characteristics of their running containers.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md">Monitoring cAdvisor with Prometheus</a></p>
<p><a target="_blank" rel="noopener" href="https://grafana.com/grafana/dashboards/14282-cadvisor-exporter/">cadvisor Grafana dashboard</a></p>
<h3><span id="elasticsearch_exporter">elasticsearch_exporter</span></h3><p><a target="_blank" rel="noopener" href="https://github.com/prometheus-community/elasticsearch_exporter">Exporter</a></p>
<p><a target="_blank" rel="noopener" href="https://grafana.com/grafana/dashboards/14191-elasticsearch-overview/">Grafana</a></p>
<h3><span id="blackbox_exporter">blackbox_exporter</span></h3><p><a target="_blank" rel="noopener" href="https://yunlzheng.gitbook.io/prometheus-book/part-ii-prometheus-jin-jie/exporter/commonly-eporter-usage/install_blackbox_exporter#zi-ding-yi-http-qing-qiu">黑盒探测和使用，文档写的太好了，通俗易懂</a></p>
<p><strong>目的</strong>：一句话总结：通过http探测或tcp探测 手段 检测远程服务站点是不是可用的</p>
<p>白盒监控：过对监控指标的观察能够预判可能出现的问题，从而对潜在的不确定因素进行优化。</p>
<p>黑盒监控即以用户的身份测试服务的外部可见性，常见的黑盒监控包括HTTP探针、TCP探针等用于检测站点或者服务的可访问性，以及访问效率等。</p>
<p>区别：黑盒监控相较于白盒监控最大的不同在于黑盒监控是以故障为导向当故障发生时，黑盒监控能快速发现故障，而白盒监控则侧重于主动发现或者预测潜在的问题。一个完善的监控目标是要能够从白盒的角度发现潜在问题，能够在黑盒的角度快速发现已经发生的问题。</p>
<h4><span id="refrerences">Refrerences</span></h4><p><a target="_blank" rel="noopener" href="https://yunlzheng.gitbook.io/prometheus-book/part-ii-prometheus-jin-jie/exporter/commonly-eporter-usage/install_blackbox_exporter#zi-ding-yi-http-qing-qiu">prometheus-book</a> <strong>重点推荐prometheus学习</strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/" data-id="cljn5iqgg00007j9a6mcm3wwz" data-title="prometheus使用" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/ansible/ansible使用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T15:58:16.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-ansible/">system/linux/ansible</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/">ansible使用</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">环境搭建</a><ul>
<li><a href="#%E5%9F%BA%E4%BA%8Ealpine-%E9%95%9C%E5%83%8F%E6%90%AD%E5%BB%BA">基于alpine 镜像搭建</a><ul>
<li><a href="#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4">安装步骤</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E4%BA%8Ecentos-7-%E9%95%9C%E5%83%8F%E6%90%AD%E5%BB%BA">基于centos 7 镜像搭建</a><ul>
<li><a href="#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4-1">安装步骤</a></li>
</ul>
</li>
<li><a href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A4%9A%E7%BB%93%E7%82%B9">虚拟机多结点</a><ul>
<li><a href="#virturebox%E7%BD%91%E5%8D%A1%E9%85%8D%E7%BD%AE">virturebox网卡配置</a></li>
<li><a href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE">虚拟机网络配置</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-1">参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8">入门使用</a><ul>
<li><a href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">配置文件</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97">常用模块</a><ul>
<li><a href="#shell-%E6%A8%A1%E5%9D%97">shell 模块</a></li>
<li><a href="#script-%E6%A8%A1%E5%9D%97">script 模块</a></li>
</ul>
</li>
<li><a href="#playbook">Playbook</a><ul>
<li><a href="#%E6%89%A7%E8%A1%8Cshell">执行shell</a></li>
<li><a href="#%E6%89%A7%E8%A1%8Cscript">执行script</a></li>
</ul>
</li>
<li><a href="#become_user%E7%94%A8%E6%B3%95">Become_user用法</a><ul>
<li><a href="#%E7%94%A8%E6%B3%95">用法</a></li>
<li><a href="#%E6%80%BB%E4%BD%93%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B">总体操作流程</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="环境搭建">环境搭建</span></h2><h3><span id="基于alpine-镜像搭建">基于alpine 镜像搭建</span></h3><p>alpine:3.17.3 版本，启动多个docker容器，模拟练习使用ansible</p>
<h4><span id="安装步骤">安装步骤</span></h4><p>1、按如下目录结构创建文件</p>
<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.08.18.png">

<p>&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[defaults]</span><br><span class="line">; 是否启用主机密钥检查: https://docs.ansible.com/ansible/latest/inventory_guide/connection_details.html#managing-host-key-checking</span><br><span class="line">; 初期使用密码连接分发公钥时先设置为False</span><br><span class="line">; 对应的环境变量设置: export ANSIBLE_HOST_KEY_CHECKING=False</span><br><span class="line">host_key_checking = False</span><br><span class="line">interpreter_python = /usr/bin/python</span><br></pre></td></tr></table></figure>

<p>&#x2F;etc&#x2F;ansible&#x2F;hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[myvirtualmachines]</span><br><span class="line">192.0.2.50</span><br><span class="line">192.0.2.51</span><br><span class="line">192.0.2.52</span><br></pre></td></tr></table></figure>

<p>Dockerfile:  构造生成 example-ansible-master 容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM example-ansible-node:latest</span><br><span class="line"></span><br><span class="line">RUN apk add --no-cache ansible openssh sshpass</span><br><span class="line"></span><br><span class="line">RUN mkdir -p /etc/ansible &amp;&amp; \</span><br><span class="line">    ssh-keygen -t rsa -P &quot;&quot; -f ~/.ssh/id_rsa</span><br></pre></td></tr></table></figure>

<p>Dockerfile.node：生成 镜像example-ansible-node</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:3.17.3</span><br><span class="line"></span><br><span class="line">RUN echo &quot;http://mirrors.aliyun.com/alpine/latest-stable/main/&quot; &gt; /etc/apk/repositories &amp;&amp; \</span><br><span class="line">    echo &quot;http://mirrors.aliyun.com/alpine/latest-stable/community/&quot; &gt;&gt; /etc/apk/repositories</span><br><span class="line"></span><br><span class="line">RUN apk update &amp;&amp; \</span><br><span class="line">    apk add --no-cache openssh-server tzdata python3 &amp;&amp; \</span><br><span class="line">    cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \</span><br><span class="line">    sed -i &quot;s/#PermitRootLogin.*/PermitRootLogin yes/g&quot; /etc/ssh/sshd_config &amp;&amp; \</span><br><span class="line">    ssh-keygen -A &amp;&amp; \</span><br><span class="line">    echo &quot;root:123456&quot; | chpasswd</span><br><span class="line"></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line">CMD [&quot;/usr/sbin/sshd&quot;, &quot;-D&quot;]</span><br></pre></td></tr></table></figure>



<p>docker-compose.yml：启动四个容器：</p>
<p>example-ansible-master，example-ansible-node1，example-ansible-node2，example-ansible-node3</p>
<p>master安装ansible，运行ansible命令，控制操作其他三个结点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  net-ansible:</span><br><span class="line">    ipam:</span><br><span class="line">      driver: default</span><br><span class="line">      config:</span><br><span class="line">        - subnet: 192.0.2.0/24</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  master:</span><br><span class="line">    build:</span><br><span class="line">      dockerfile: Dockerfile</span><br><span class="line">      context: .</span><br><span class="line">    container_name: example-ansible-master</span><br><span class="line">    hostname: ansible</span><br><span class="line">    volumes:</span><br><span class="line">      - ./etc/ansible:/etc/ansible</span><br><span class="line">      - ./ansible:/server/scripts/ansible</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.49</span><br><span class="line"></span><br><span class="line">  node1:</span><br><span class="line">    image: example-ansible-node</span><br><span class="line">    hostname: host1</span><br><span class="line">    container_name: example-ansible-node1</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.50</span><br><span class="line">  node2:</span><br><span class="line">    image: example-ansible-node</span><br><span class="line">    hostname: host2</span><br><span class="line">    container_name: example-ansible-node2</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.51</span><br><span class="line">  node3:</span><br><span class="line">    image: example-ansible-node</span><br><span class="line">    hostname: host3</span><br><span class="line">    container_name: example-ansible-node3</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.52</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2、启动容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 构建host1、host2、host3的镜像</span><br><span class="line">docker build -t example-ansible-node -f ./Dockerfile.node .</span><br><span class="line"></span><br><span class="line"># 启动四个容器</span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、登录ansible容器，分发ssh公钥</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#进入ansible容器 (master)</span><br><span class="line">docker exec -it example-ansible-master ash</span><br><span class="line"></span><br><span class="line"># 执行下面命令分发ssh公钥</span><br><span class="line">ansible myvirtualmachines -k -m authorized_key -a &quot;user=root key=&#x27;&#123;&#123; lookup(&#x27;file&#x27;, &#x27;~/.ssh/id_rsa.pub&#x27;) &#125;&#125;&#x27;&quot;</span><br><span class="line">输入密码123456</span><br></pre></td></tr></table></figure>

<p>4、测试连通性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">; 测试连接</span><br><span class="line">ansible all -m ping</span><br><span class="line">ansible myvirtualmachines -m ping</span><br></pre></td></tr></table></figure>

<h4><span id="参考资料">参考资料</span></h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624172594">用Docker搭建Ansible练习环境</a></p>
<p>如何使用容器模拟物理机学习使用ansible?<br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/goloving/p/15032636.html">https://www.cnblogs.com/goloving/p/15032636.html</a><br><a target="_blank" rel="noopener" href="http://mactech.sheridanc.on.ca/team/mark-galaszkiewicz/getting-started-docker-ansible/">http://mactech.sheridanc.on.ca/team/mark-galaszkiewicz/getting-started-docker-ansible/</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shgh_2004/article/details/80599515">https://blog.csdn.net/shgh_2004/article/details/80599515</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1632980">https://cloud.tencent.com/developer/article/1632980</a></p>
<h3><span id="基于centos-7-镜像搭建">基于centos 7 镜像搭建</span></h3><p>Centos 7.9，启动多个docker容器，模拟练习使用ansible</p>
<p>&#x2F;Users&#x2F;****&#x2F;Desktop&#x2F;mydirect&#x2F;github&#x2F;ansible&#x2F;env_build2</p>
<h4><span id="安装步骤">安装步骤</span></h4><p>1、按如下目录结构创建文件</p>
<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.08.18.png">

<p>&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[defaults]</span><br><span class="line">; 是否启用主机密钥检查: https://docs.ansible.com/ansible/latest/inventory_guide/connection_details.html#managing-host-key-checking</span><br><span class="line">; 初期使用密码连接分发公钥时先设置为False</span><br><span class="line">; 对应的环境变量设置: export ANSIBLE_HOST_KEY_CHECKING=False</span><br><span class="line">host_key_checking = False</span><br><span class="line">interpreter_python = /usr/bin/python</span><br></pre></td></tr></table></figure>

<p>&#x2F;etc&#x2F;ansible&#x2F;hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[myvirtualmachines]</span><br><span class="line">192.0.2.50</span><br><span class="line">192.0.2.51</span><br><span class="line">192.0.2.52</span><br></pre></td></tr></table></figure>

<p>Dockerfile:  构造生成 example-ansible-cent-master 容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM example-ansible-cent-node:latest</span><br><span class="line"></span><br><span class="line">RUN yum install  -y ansible openssh sshpass</span><br><span class="line"></span><br><span class="line">RUN mkdir -p /etc/ansible &amp;&amp; \</span><br><span class="line">    ssh-keygen -t rsa -P &quot;&quot; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Dockerfile.node：生成 镜像example-ansible-cent-node</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">FROM centos:centos7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RUN yum install -y openssh-server tzdata python3 &amp;&amp; \</span><br><span class="line">    cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \</span><br><span class="line">    sed -i &quot;s/#PermitRootLogin.*/PermitRootLogin yes/g&quot; /etc/ssh/sshd_config &amp;&amp; \</span><br><span class="line">    ssh-keygen -A &amp;&amp; \</span><br><span class="line">    echo &quot;root:123456&quot; | chpasswd</span><br><span class="line"></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line">CMD [&quot;/usr/sbin/sshd&quot;, &quot;-D&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>docker-compose.yml：启动四个容器：</p>
<p>example-ansible-cent-master，example-ansible-cent-node1，</p>
<p>example-ansible-cent-node2，example-ansible-cent-node3</p>
<p>master安装ansible，运行ansible命令，控制操作其他三个结点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  net-ansible:</span><br><span class="line">    ipam:</span><br><span class="line">      driver: default</span><br><span class="line">      config:</span><br><span class="line">        - subnet: 192.0.2.0/24</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  master:</span><br><span class="line">    build:</span><br><span class="line">      dockerfile: Dockerfile</span><br><span class="line">      context: .</span><br><span class="line">    container_name: example-ansible-cent-master</span><br><span class="line">    hostname: ansible</span><br><span class="line">    volumes:</span><br><span class="line">      - ./etc/ansible:/etc/ansible</span><br><span class="line">      - ./ansible:/server/scripts/ansible</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.49</span><br><span class="line"></span><br><span class="line">  node1:</span><br><span class="line">    image: example-ansible-cent-node</span><br><span class="line">    hostname: host1</span><br><span class="line">    container_name: example-ansible-cent-node1</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.50</span><br><span class="line">  node2:</span><br><span class="line">    image: example-ansible-cent-node</span><br><span class="line">    hostname: host2</span><br><span class="line">    container_name: example-ansible-cent-node2</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.51</span><br><span class="line">  node3:</span><br><span class="line">    image: example-ansible-cent-node</span><br><span class="line">    hostname: host3</span><br><span class="line">    container_name: example-ansible-cent-node3</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.52</span><br></pre></td></tr></table></figure>

<p>2、启动容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 构建host1、host2、host3的镜像</span><br><span class="line">docker build -t example-ansible-cent-node -f ./Dockerfile.node .</span><br><span class="line"></span><br><span class="line"># 启动四个容器</span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、登录ansible容器，修改yum源，安装依赖，分发ssh公钥</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#进入ansible容器 (master)</span><br><span class="line">docker exec -it example-ansible-cent-master bash</span><br><span class="line"></span><br><span class="line">#安装依赖</span><br><span class="line">yum install -y net-tools openssh-clients wget</span><br><span class="line">#修改yum源</span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">wget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line">yum makecache</span><br><span class="line">#安装ansible和ssh</span><br><span class="line">yum install -y ansible openssh sshpass</span><br><span class="line"></span><br><span class="line"># 执行下面命令分发ssh公钥</span><br><span class="line">ansible myvirtualmachines -k -m authorized_key -a &quot;user=root key=&#x27;&#123;&#123; lookup(&#x27;file&#x27;, &#x27;~/.ssh/id_rsa.pub&#x27;) &#125;&#125;&#x27;&quot;</span><br><span class="line">输入密码123456</span><br></pre></td></tr></table></figure>

<p>4、测试连通性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">; 测试连接</span><br><span class="line">ansible all -m ping</span><br><span class="line">ansible myvirtualmachines -m ping</span><br></pre></td></tr></table></figure>

<p>5、所有结点关闭和启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker-compose stop</span><br><span class="line">docker-compose start</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="虚拟机多结点">虚拟机多结点</span></h3><p>使用virturebox和centos7搭建多台虚拟机</p>
<p>网络配置需要配置两块网卡：其一保证可访问外网；其二分配固定ip，虚拟机之间互相访问</p>
<h4><span id="virturebox网卡配置">virturebox网卡配置</span></h4><img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.41.12.png">





<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.41.19.png">



<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.42.09.png">



<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.42.24.png">





<h4><span id="虚拟机网络配置">虚拟机网络配置</span></h4><p>cat  &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-enp0s3 (动态ip:dhcp)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TYPE=static</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=dhcp</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=enp0s3</span><br><span class="line">UUID=55fa0f47-7b70-40c7-a4c1-59e912cf5cc9</span><br><span class="line">DEVICE=enp0s3</span><br><span class="line">ONBOOT=yes</span><br></pre></td></tr></table></figure>

<p>cat  &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-enp0s8 （静态：固定IP）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">TYPE=static</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=enp0s8</span><br><span class="line">#UUID=55fa0f47-7b70-40c7-a4c1-59e912cf5cc9</span><br><span class="line">DEVICE=enp0s8</span><br><span class="line">ONBOOT=yes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">IPADDR=192.168.56.101</span><br><span class="line">#NETMASK=255.255.255.0</span><br><span class="line">#GATEWAY=192.168.56.1</span><br></pre></td></tr></table></figure>

<h4><span id="参考资料">参考资料</span></h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/341328334">VirtualBox虚拟机配置双网卡同时连接内外网</a></p>
<h2><span id="入门使用">入门使用</span></h2><p>ansible是一个配置管理工具，通过ssh(或密码)管理多台主机，不是类似mysql的服务，底层基于python实现</p>
<h3><span id="配置文件">配置文件</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ansible主要文件：</span><br><span class="line"></span><br><span class="line">/etc/ansible/hosts 主机清单文件</span><br><span class="line">/etc/ansible/ansible.cfg 主配置文件</span><br><span class="line"></span><br><span class="line">ansible-doc -l 模块文档查看</span><br><span class="line">ansible-doc ping 模块用法</span><br><span class="line">ansible-doc -s ping  模块用法</span><br></pre></td></tr></table></figure>



<h3><span id="常用模块">常用模块</span></h3><h4><span id="shell-模块">shell 模块</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible myvirtualmachines -m shell -a &#x27;echo $HOSTNAME&#x27;</span><br></pre></td></tr></table></figure>

<h4><span id="script-模块">script 模块</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;&gt; /tmp/test1.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">echo  `date`</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ansible myvirtualmachines -m script -a /tmp/test1.sh</span><br></pre></td></tr></table></figure>



<h3><span id="playbook">Playbook</span></h3><p>cat &#x2F;etc&#x2F;ansible&#x2F;hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[myvirtualmachines]</span><br><span class="line">192.0.2.50</span><br><span class="line">192.0.2.51</span><br><span class="line">192.0.2.52</span><br><span class="line"></span><br><span class="line">[test1]</span><br><span class="line">host1 ansible_ssh_host=192.0.2.50 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22</span><br><span class="line">host2 ansible_ssh_host=192.0.2.51 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22</span><br><span class="line">host3 ansible_ssh_host=192.0.2.52 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22</span><br></pre></td></tr></table></figure>

<h4><span id="执行shell">执行shell</span></h4><p>ansible-playbook hello.yml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat hello.yml</span><br><span class="line"></span><br><span class="line">- hosts: test1</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: tom</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: &#x27;echo 123\|&#x27;</span><br></pre></td></tr></table></figure>



<h4><span id="执行script">执行script</span></h4><p>ansible-playbook test1.yml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat /tmp/test1.sh   </span><br><span class="line">#!/bin/bash</span><br><span class="line">echo 12333</span><br><span class="line"></span><br><span class="line">vi test1.yml</span><br><span class="line">- hosts: test1</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: root</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/test1.sh    </span><br></pre></td></tr></table></figure>



<h3><span id="become_user用法">Become_user用法</span></h3><p>以非root用户执行playbook</p>
<h4><span id="用法">用法</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#命令行用法：适用于全局 (-u remote-user覆盖playbook定义的remote-user)</span><br><span class="line">ansible-playbook  -u tom -b --become-method sudo   hello2.yml </span><br><span class="line"></span><br><span class="line">cat hello2.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: root</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line"></span><br><span class="line">cat test1.sh </span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/hosts</span><br><span class="line">#123</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">#作用与play下的单个task</span><br><span class="line">ansible-playbook     hello4_2.yml </span><br><span class="line"></span><br><span class="line">cat hello4_2.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br><span class="line">      </span><br><span class="line">cat test1.sh </span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/hosts</span><br><span class="line">#123</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#作用与单个play，应用到play下的每个task</span><br><span class="line">ansible-playbook     hello3.yml </span><br><span class="line"></span><br><span class="line">cat hello3.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: tom</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50  </span><br><span class="line"></span><br><span class="line">#作用与单个play，应用到play下的每个task, 比hello3更精简</span><br><span class="line">cat hello4.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50  </span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">#作用与单个play，应用到play下的每个task, 比hello4 多了sudo</span><br><span class="line">cat hello5.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: sudo /usr/sbin/shutdown -h 23:50 </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">总体来说，hello4最精简，不需要sudo, 不需要设置remote_user， hello4_2控制精度最高，become_user对应的是单个task</span><br><span class="line"></span><br><span class="line">become_method 默认值sudo，因此可以继续精简，如下是最简洁的写法 (hello3_3.yml)</span><br><span class="line">cat hello3_3.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">其他用法：</span><br><span class="line">第一种：</span><br><span class="line">ansible-playbook -u tom hello4_3.yml </span><br><span class="line"></span><br><span class="line">cat hello4_3.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  remote_user: root</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br><span class="line"></span><br><span class="line">第二种：</span><br><span class="line">ansible-playbook hello3_2.yml </span><br><span class="line"></span><br><span class="line">cat hello3_2.yml </span><br><span class="line"> </span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: tom</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become_user: root</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50 </span><br><span class="line"></span><br><span class="line">第三种是错误的：如下所示</span><br><span class="line">####如下是错误的写法，要避免</span><br><span class="line">ansible-playbook -u tom hello4_4.yml</span><br><span class="line">ansible-playbook  hello4_4.yml</span><br><span class="line">- hosts: test2</span><br><span class="line">  remote_user: root</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_user: tom</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br></pre></td></tr></table></figure>



<h4><span id="总体操作流程">总体操作流程</span></h4><p>1、首先新建用户（比如tom），加入&#x2F;etc&#x2F;sudoers文件，确保新用户（tom）可以执行sudo su -</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#visudo 添加一行</span><br><span class="line">tom     ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure>

<p>2、编辑 ansible的hosts文件信息，将服务器地址、用户等信息加入hosts文件中,</p>
<p>ansible_become_pass 等价于用户手动执行sudo command要输入的pwd</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[test2]</span><br><span class="line">cent2 ansible_ssh_host=192.168.56.102 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22 ansible_become_pass=&quot;tom&quot;</span><br><span class="line">cent3 ansible_ssh_host=192.168.56.103 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22 ansible_become_pass=&quot;tom&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、修改 playbook yml文件，可以针对单个task， 单个play，或者所有play，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">--单个task</span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">--单个play</span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">--所有task</span><br><span class="line">ansible-playbook  -u tom -b --become-method sudo   hello2.yml </span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4><span id="参考文献">参考文献</span></h4><p><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_15735145/5547297">play-book 普通用户提权</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_privilege_escalation.html">Understanding privilege escalation: become</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/" data-id="cljn4kj610000rx9adxvgh8tg" data-title="ansible使用" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-tools/java/maven/maven远程仓库配置" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/tools/java/maven/maven%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T14:01:21.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/tools-java-maven/">tools/java/maven</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/tools/java/maven/maven%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/">maven远程仓库配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->



<!-- tocstop -->

<p>修改 MAVEN_HOME&#x2F;conf&#x2F;settings.xml文件，</p>
<p>1、本地仓库位置 修改</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;localRepository&gt;$&#123;user.home&#125;/.m2/repository&lt;/localRepository&gt;</span><br><span class="line">默认位置 ~/.m2/repository </span><br></pre></td></tr></table></figure>



<p>2、远程仓库地址 配置</p>
<p>当本地仓库找不到对应依赖包时，从配置的远程仓库中拉取依赖包、下载到本地。</p>
<p>如下示例在profiles  配置了两个远程仓库，id分别为repo、alimaven，id必须是唯一的，不能重复。</p>
<p>activeProfiles 指明两个仓库都是激活状态，都可用于项目构建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&lt;profiles&gt;</span><br><span class="line">    &lt;!--</span><br><span class="line">     | Specifies a set of introductions to the build process, to be activated using one or more of the</span><br><span class="line">     | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/&gt;</span><br><span class="line">     | or the command line, profiles have to have an ID that is unique.</span><br><span class="line">     |</span><br><span class="line">     | An encouraged best practice for profile identification is to use a consistent naming convention</span><br><span class="line">     | for profiles, such as &#x27;env-dev&#x27;, &#x27;env-test&#x27;, &#x27;env-production&#x27;, &#x27;user-jdcasey&#x27;, &#x27;user-brett&#x27;, etc.</span><br><span class="line">     | This will make it more intuitive to understand what the set of introduced profiles is attempting</span><br><span class="line">     | to accomplish, particularly when you only have a list of profile id&#x27;s for debug.</span><br><span class="line">   --&gt;</span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;repo&lt;/id&gt;</span><br><span class="line">      &lt;repositories&gt;</span><br><span class="line">          &lt;repository&gt;</span><br><span class="line">              &lt;id&gt;repo&lt;/id&gt;</span><br><span class="line">              &lt;name&gt;Central Repository&lt;/name&gt;</span><br><span class="line">              &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt;</span><br><span class="line">      &lt;!--         &lt;layout&gt;default&lt;/layout&gt;</span><br><span class="line">              &lt;snapshots&gt;</span><br><span class="line">                  &lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">              &lt;/snapshots&gt; --&gt;</span><br><span class="line">          &lt;/repository&gt;</span><br><span class="line">      &lt;/repositories&gt;</span><br><span class="line">    &lt;/profile&gt;</span><br><span class="line"></span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;alimaven&lt;/id&gt;</span><br><span class="line">      &lt;repositories&gt;</span><br><span class="line">          &lt;repository&gt;</span><br><span class="line">           &lt;id&gt;alimaven&lt;/id&gt;</span><br><span class="line">           &lt;name&gt;alimaven&lt;/name&gt;</span><br><span class="line">           &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span><br><span class="line">      &lt;!--         &lt;layout&gt;default&lt;/layout&gt;</span><br><span class="line">              &lt;snapshots&gt;</span><br><span class="line">                  &lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">              &lt;/snapshots&gt; --&gt;</span><br><span class="line">          &lt;/repository&gt;</span><br><span class="line">      &lt;/repositories&gt;</span><br><span class="line">    &lt;/profile&gt;</span><br><span class="line"></span><br><span class="line">&lt;/profiles&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- activeProfiles</span><br><span class="line">   | List of profiles that are active for all builds.</span><br><span class="line">  &lt;/activeProfiles&gt;</span><br><span class="line">  --&gt;</span><br><span class="line"></span><br><span class="line">&lt;activeProfiles&gt;</span><br><span class="line">    &lt;activeProfile&gt;repo&lt;/activeProfile&gt;</span><br><span class="line">    &lt;activeProfile&gt;alimaven&lt;/activeProfile&gt;</span><br><span class="line">&lt;/activeProfiles&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/tools/java/maven/maven%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/" data-id="cljmy0mmf0000v29a26p20mk6" data-title="maven远程仓库配置" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/command/linux常用命令" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/system/linux/command/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T13:55:00.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-command/">system/linux/command</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/system/linux/command/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">linux常用命令</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E7%8E%AF%E5%A2%83">环境：</a></li>
<li><a href="#%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C">文件操作</a><ul>
<li><a href="#find">Find</a></li>
</ul>
</li>
<li><a href="#%E5%8E%8B%E7%BC%A9%E5%92%8C%E8%A7%A3%E5%8E%8B%E7%BC%A9">压缩和解压缩</a></li>
<li><a href="#vim">VIM</a></li>
<li><a href="#%E7%AE%A1%E7%90%86%E7%94%A8%E6%88%B7%E7%BB%84">管理用户组</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE">网络配置</a></li>
<li><a href="#%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86">软件包管理</a></li>
<li><a href="#%E5%86%85%E6%A0%B8">内核</a><ul>
<li><a href="#grub">GRUB</a></li>
</ul>
</li>
<li><a href="#%E8%BF%9B%E7%A8%8B">进程</a></li>
<li><a href="#%E5%88%86%E5%8C%BA">分区</a></li>
<li><a href="#%E8%B5%84%E6%96%99">资料</a></li>
</ul>
<!-- tocstop -->

<h3><span id="环境">环境：</span></h3><p>linux版本：</p>
<p>内核：<a target="_blank" rel="noopener" href="https://kernel.org/">https://kernel.org/</a></p>
<p>发行版：Redhat\centos\federa\  ubuntu\debian  后两种有界面</p>
<p>init 3  进入登录</p>
<p>Init 0 关机</p>
<p>虚拟机安装：<a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">virtualbox</a></p>
<p>os:  <a target="_blank" rel="noopener" href="http://isoredirect.centos.org/centos/7/isos/x86_64/">centos</a></p>
<h3><span id="文件操作">文件操作</span></h3><p>Mv  :  重命名 或移动 文件（目录）</p>
<p>Ctrl + l  &#x3D;  clear</p>
<p>cd - 返回上一级目录</p>
<h4><span id="find">Find</span></h4><p>find &#x2F;etc  -name init</p>
<p>find &#x2F; -size +204800  # 在根目录查找&gt;100m的文件，100MB &#x3D;&#x3D; 102400KB&#x3D;&#x3D;204800数据快</p>
<h3><span id="压缩和解压缩">压缩和解压缩</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c 打tar包 打包</span><br><span class="line">x解tar包 解包</span><br><span class="line">f 文件</span><br><span class="line">j bzip2解压缩</span><br><span class="line">z gzip压缩</span><br></pre></td></tr></table></figure>

<p>tar jxf  &#x2F;tmp&#x2F;etc-backup.tar.bz2  -C &#x2F;tmp&#x2F;test1</p>
<p>tar jcf  &#x2F;tmp&#x2F;etc-backup.tar.bz2  &#x2F;etc</p>
<p>tar zxf  &#x2F;tmp&#x2F;etc-backup.tar.gz  -C  &#x2F;tmp&#x2F;test2</p>
<p>tar zcf  &#x2F;tmp&#x2F;etc-backup.tar.gz  &#x2F;etc</p>
<p>tar cf   &#x2F;tmp&#x2F;etc.tar  &#x2F;etc     &#x2F;&#x2F;打tar包</p>
<p>gzip  etc.tar         &#x2F;&#x2F;.tar.gz</p>
<p>gzip -d  etc.tar.gz   &#x2F;&#x2F;.tar.gz -&gt; .tar</p>
<p>zip -r etc.zip &#x2F;etc   zip压缩</p>
<p>unzip  etc.zip  -d    &#x2F;tmp&#x2F;test1</p>
<h3><span id="vim">VIM</span></h3><p>配置文件 vi &#x2F;etc&#x2F;virc</p>
<p>正常编辑模式： -i -I -a -A -o -O   hjkl （上下左右） </p>
<p>yy 3yy  y$(光标到行的末尾) 复制</p>
<p>dd 3dd d$ 剪切</p>
<p>u（撤销） ctrl+r 恢复动作</p>
<p>输入set nu 显示行号</p>
<p>x   3x 删除多个字符</p>
<p>r + 新字符 替换</p>
<p>11 + G  移动到指定第11行  g + g移动到首行  G移动到最后一行</p>
<p>shift+^ 行首  shift + $ 行尾</p>
<p>命令模式： </p>
<p>输入set nu 显示行号</p>
<p>输入set nonu 不显示行号</p>
<p>wq 保存 退出</p>
<p>!command 执行linux命令</p>
<p>&#x2F;x  查找字符串 x, n向下查找，shift + n 向上查找</p>
<p>%s&#x2F;old&#x2F;new  所有行第一个替换    s&#x2F;old&#x2F;new 当前行第一个替换</p>
<p>%s&#x2F;old&#x2F;new&#x2F;g  所有行替换 (&#x2F;g全局所有行)</p>
<p>3,5s&#x2F;old&#x2F;new&#x2F;g (第三到第五行替换)</p>
<p>可视模式：</p>
<p>v</p>
<p>shift +v</p>
<p>ctrl + v</p>
<h3><span id="管理用户组">管理用户组</span></h3><p>&#x2F;etc&#x2F;passwd  用户</p>
<p>&#x2F;etc&#x2F;shadow  密码</p>
<p> &#x2F;etc&#x2F;group 用户组</p>
<p>useradd  useradd -g group1 user1</p>
<p>userdel</p>
<p>passwd user1 修改密码</p>
<p>usermod 修改用户属性   usermod -g group1 user1</p>
<p>chage</p>
<p>groupadd</p>
<p>groupdel</p>
<p>su - tom 切换到用户tom</p>
<p>sudo   以普通用户身份登录的状态下，执行某些只有root才能执行的命令，比如sudo shutdown</p>
<p>实现方式，直接执行visudo命令 对文件&#x2F;etc&#x2F;sudoers修改、授权</p>
<p>visudo    为用户添加执行某些root命令的权限，实质是修改 &#x2F;etc&#x2F;sudoers文件 </p>
<p>chown</p>
<p>chgrp</p>
<p>chmod  u g o a  u+x u-x</p>
<p>目录权限：x 进入目录</p>
<p>  rx 进入目录，查看文件</p>
<p> wx 进入目录，删除文件，修改文件名</p>
<h3><span id="网络配置">网络配置</span></h3><p>修改网卡显示名字</p>
<p>vi  &#x2F;etc&#x2F;default&#x2F;grub, 在GRUB_CMDLINE_LINUX 增加 biosdevname&#x3D;0 net.ifnames&#x3D;1</p>
<p>grub2-mkconfig  -o  &#x2F;boot&#x2F;grub2&#x2F;grub.cfg    &#x2F;&#x2F;重新生成grub配置文件</p>
<p>reboot</p>
<p>mii-tool etho  查看网卡物理网络连接情况</p>
<p>route  -n 查看路由 网关</p>
<p>修改ip:  ifconfig  eth0  10.0.2.4  netmask 255.255.255.0</p>
<p>ifup  启动网卡</p>
<p>route add  增加路由</p>
<p>route del   减少一条路由</p>
<p>ping </p>
<p>traceroute -w 1  <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a>   </p>
<p>mtr</p>
<p>nslookup  域名查看  nslookup   <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a></p>
<p>telnet   telnet  ww.baidu.com   80</p>
<p>tcpdump     查看网络数据包  tcpdump  -i any  -n port   80</p>
<p>​                     tcpdump -i any  -n host 10.0.0.1</p>
<p>​                     tcpdump -i any  -n host 10.0.0.1 and port  80</p>
<p>​                     tcpdump -i any  -n host 10.0.0.1 and port  80  -w  &#x2F;tmp&#x2F;file1</p>
<p>netstat  netstat   -tlunp</p>
<p>ss</p>
<p> service network restart  (systemctl   restart  NetworkManager.service)</p>
<p>&#x2F;etc&#x2F;sysconfig&#x2F;netwok-scripts&#x2F;ifcfg-eth0   </p>
<p>​     BOOTPROTO(dhcp|none)  IPADDR  GATEWAY   NETMASK  DNS1  DNS2  DNS3</p>
<p>​	</p>
<p>hostnamectl  set-hostname c7.test11</p>
<p>vi  &#x2F;etc&#x2F;hosts</p>
<p>Selinux  强制访问控制 </p>
<p>​     getenforce</p>
<p>​     setenforce  0</p>
<p>​     &#x2F;etc&#x2F;selinux&#x2F;sysconfig</p>
<p>​    </p>
<h3><span id="软件包管理">软件包管理</span></h3><p>rpm  -qa  查看</p>
<p>rpm  -i   安装</p>
<p>rpm  -e 卸载</p>
<p>yum 简化rpm包的下载、安装、依赖</p>
<p>配置yum 源  &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo,   <a target="_blank" rel="noopener" href="https://developer.aliyun.com/mirror/centos?spm=a2c6h.13651102.0.0.39491b11mLBS4h">国内aliyun yum源</a></p>
<p>yum -y install</p>
<p>yum remove</p>
<p>yum list</p>
<p>yum update  ***   更新软件包</p>
<p>源代码编译安装</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget  https://openresty.org/download/openresty-1.21.4.1.tar.gz</span><br><span class="line">tar -zxvf openresty-1.21.4.1.tar.gz</span><br><span class="line">cd openresty-1.21.4.1</span><br><span class="line">./configure  --prefix=/usr/local/openresty  (prefix 指定安装目录)</span><br><span class="line">make -j2  (-j2 使用2个核进行编译)</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perl: https://www.cpan.org/src/5.0/perl-5.36.1.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>epel    扩展包  yum install  epel-release  -y</p>
<h3><span id="内核">内核</span></h3><p>uname -r 查看内核版本</p>
<p>lscpu  查看cpu</p>
<h4><span id="grub">GRUB</span></h4><p>（GRand Unified Bootloader，大一统启动加载器），是一个<a target="_blank" rel="noopener" href="https://wiki.archlinuxcn.org/wiki/Arch_%E7%9A%84%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B#%E5%BC%95%E5%AF%BC%E5%8A%A0%E8%BD%BD%E7%A8%8B%E5%BA%8F">引导加载程序</a></p>
<p>vi  &#x2F;etc&#x2F;default&#x2F;grub</p>
<p>ls &#x2F;etc&#x2F;grub.d&#x2F;</p>
<p>vi  &#x2F;boot&#x2F;grub2&#x2F;grub.cfg</p>
<p>grub2-mkconfig  -o  &#x2F;boot&#x2F;grub2&#x2F;grub.cfg    &#x2F;&#x2F;重新生成grub配置文件</p>
<h3><span id="进程">进程</span></h3><p>ps</p>
<p>ps -eLf</p>
<p>pstree 进程树</p>
<p>top</p>
<p>top -p pid    只查某一个进程</p>
<p>nice  启动时指定进程优先级</p>
<p>renice  运行中指定进程优先级</p>
<p>.&#x2F;a.sh &amp;    加&amp;表示后端运行，进程放入后端</p>
<p>jobs  查看正在运行的后端进程</p>
<p>fg+ 数字(jobs显示的)  进程调回前端</p>
<p>bg+ 数字(jobs显示的)  进程调入后端</p>
<p>ctrl + z 正在前端运行的进程调入后端，并进行stop状态，通过jobs  fg后续继续执行</p>
<p>kill -l</p>
<p>ctrl+c</p>
<p>kill -9  pid</p>
<p>nohup  + cmd + &amp; </p>
<p>守护进程 daemon</p>
<p>screen   </p>
<p>​         screen 进入screen环境</p>
<p>​         screen  -ls    查看后端有哪些screen环境下的进程</p>
<p>​         ctrl + a  d   进程调入后端</p>
<p>​          screen -r id  进程调回前端</p>
<p>进程日志   </p>
<p>​     &#x2F;var&#x2F;log&#x2F;messages  </p>
<p>​     &#x2F;var&#x2F;log&#x2F;dmesg   内核</p>
<p>​     &#x2F;var&#x2F;log&#x2F;secure</p>
<p>​     &#x2F;var&#x2F;log&#x2F;cron</p>
<p>systemctl   start&#x2F;stop&#x2F;enable&#x2F;disable  </p>
<p>Systemctl    </p>
<p>top</p>
<p>free</p>
<p>fdisk -l  磁盘分区</p>
<p>parted -l</p>
<p>df -h</p>
<p>du -sh 实际占用空间</p>
<p>ls -lh</p>
<p>dd  if&#x3D;&#x2F;dev&#x2F;zero  bs&#x3D;4M  count&#x3D;10 of&#x3D;bfile    复制产生一个文件</p>
<p>文件系统  ext4  xfs ntfs</p>
<p>ls -i  查看i节点</p>
<p>ln   硬链接   i节点相同，不能跨分区</p>
<p>ln -s  软链接  i节点不同，</p>
<p>getfacl   文件访问权限控制</p>
<p>setfacl  -m u:user1:r  afile   赋读权</p>
<h3><span id="分区">分区</span></h3><p>fdisk  分区, 大于2T 要使用parted 进行分区</p>
<p>mkfs.ext4   文件系统格式化</p>
<p>mkdir  &#x2F;mnt&#x2F;sdc1  mount  &#x2F;dev&#x2F;sdc1  &#x2F;mnt&#x2F;sdc1    挂载文件系统</p>
<p>vi  &#x2F;etc&#x2F;fstab   文件里修改，对挂载固化，重启自动挂载</p>
<p>磁盘配额 quota  限制用户对磁盘的使用</p>
<h3><span id="资料">资料</span></h3><p><a target="_blank" rel="noopener" href="https://time.geekbang.org/course/detail/100029601-101469">极客时间 linux实战技能 100讲</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/system/linux/command/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" data-id="cljmxdm1q00001h9a94fi72lf" data-title="linux常用命令" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/host/monitor/linux主机监控" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T10:01:05.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/">linux主机监控</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#cpu%E8%B4%9F%E8%BD%BD">cpu负载</a><ul>
<li><a href="#%E5%B9%B3%E5%9D%87%E8%B4%9F%E8%BD%BD">平均负载</a></li>
<li><a href="#cpu%E4%BD%BF%E7%94%A8%E5%88%86%E6%9E%90">cpu使用分析</a><ul>
<li><a href="#%E8%BF%9B%E7%A8%8Bcpu%E4%BD%BF%E7%94%A8%E7%BB%9F%E8%AE%A1">进程cpu使用统计</a></li>
</ul>
</li>
<li><a href="#%E8%BF%9B%E7%A8%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2">进程上下文切换</a></li>
<li><a href="#cpu%E4%BD%BF%E7%94%A8%E7%8E%87">CPU使用率</a><ul>
<li><a href="#%E6%9F%A5%E6%89%BE%E7%83%AD%E7%82%B9%E5%87%BD%E6%95%B0">查找热点函数</a></li>
</ul>
</li>
<li><a href="#%E7%9F%AD%E6%97%B6%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7">短时进程监控</a><ul>
<li><a href="#execsnoop">execsnoop</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%8F%AF%E4%B8%AD%E6%96%AD%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B">不可中断进程和僵尸进程</a></li>
<li><a href="#%E8%BD%AF%E4%B8%AD%E6%96%AD">软中断</a></li>
<li><a href="#%E7%A1%AC%E4%B8%AD%E6%96%AD">硬中断</a></li>
</ul>
</li>
<li><a href="#%E5%86%85%E5%AD%98">内存</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%89">定义</a></li>
<li><a href="#%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F">内存泄漏</a></li>
<li><a href="#swap-%E4%BA%A4%E6%8D%A2">Swap 交换</a></li>
</ul>
</li>
<li><a href="#io">IO</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87">性能指标</a></li>
</ul>
</li>
<li><a href="#%E7%BD%91%E7%BB%9C">网络</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">性能测试</a></li>
<li><a href="#dns">DNS</a></li>
<li><a href="#%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90">流量分析</a></li>
<li><a href="#nat">NAT</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E7%A1%80">基础</a><ul>
<li><a href="#%E7%B3%BB%E7%BB%9F">系统</a></li>
<li><a href="#cpu">CPU</a></li>
<li><a href="#%E7%A3%81%E7%9B%98">磁盘</a><ul>
<li><a href="#%E7%A3%81%E7%9B%98raid">磁盘raid</a></li>
</ul>
</li>
<li><a href="#%E5%AE%B9%E5%99%A8">容器</a><ul>
<li><a href="#references">References:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E8%B5%84%E6%96%99">资料</a></li>
</ul>
<!-- tocstop -->

<h2><span id="cpu负载">cpu负载</span></h2><h3><span id="平均负载">平均负载</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Load average，系统的平均活跃进程数。它反应了系统的整体负载情况。 uptime显示三个数值，分别指过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载；理想情况下，平均负载等于逻辑 CPU 个数</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>uptime或top查看 CPU负载情况， （watch -d uptime  -d 参数表示高亮显示变化的区域）</p>
<p>定义：平均负载，表示的是活跃进程数，</p>
<p>单位时间内，系统中处于可运行状态和不可中断状态的平均进程数</p>
<p>可运行状态的进程：正在使用cpu或者正在等待cpu的进程，即ps aux命令下STAT处于R状态的进程 </p>
<p>不可中断状态的进程：处于内核态关键流程中的进程，且不可被打断，如等待硬件设备IO响应，ps命令D状态的进程</p>
<p>1、CPU密集型</p>
<p>2、IO 密集型</p>
<p>3、大量进程</p>
<h3><span id="cpu使用分析">cpu使用分析</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">含义：cpu非空闲时间占总 CPU 时间的百分比。cpu状态(us、ni、sys、id、wa、hi、si、st、guest)</span><br><span class="line">工具：top  vmstat  mpstat  sar  /proc/stat</span><br><span class="line">说明：/proc/stat是其他性能工具的数据来源</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>分析工具：安装stress(压测)  sysstat（分析）</p>
<p>stress   -c 1</p>
<p>stress   -c 8        spawn N workers spinning on sqrt()  模拟8个进程，同时在执行任务sqrt</p>
<p>stress   -d 2        spawn N workers spinning on write()&#x2F;unlink() 模拟2个进程，同时在执行磁盘写操作</p>
<h4><span id="进程cpu使用统计">进程cpu使用统计</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具：top pidstat  ps</span><br><span class="line">说明：top和ps按cpu使用率对进程排序，pidstat只显示实际用了cpu的进程</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>cpu监控：mpstat   -P ALL 5  2      打印2次，每5s间隔打印一次，监控所有cpu,</p>
<p>进程级别监控：</p>
<p>​     pidstat  -u 5  2    进程cpu使用统计，打印2次，每5s间隔打印一次</p>
<p>​     pidstat -d 5 2    进程IO统计(磁盘读写速率)，打印2次，每5s间隔打印一次</p>
<p>​     pidstat  -r  5  2    进程内存使用统计，打印2次，每5s间隔打印一次</p>
<h3><span id="进程上下文切换">进程上下文切换</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmstat 3      整个系统上下文切换次数 整体情况</span><br><span class="line">pidstat -wt   5   上下文切换次数  进程级别 + 线程       -w 进程切换  -t具体到线程统计</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>进程的切换只能发生在内核态。所以，</p>
<p>进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，</p>
<p>还包括了内核堆栈、寄存器等内核空间的状态</p>
<p>所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I&#x2F;O、内存等系统资源不足时，就会发生自愿上下文切换。</p>
<p>而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换</p>
<p>压测工具 sysbench：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 以10个线程运行5分钟的基准测试，模拟多线程切换的问题</span><br><span class="line">sysbench --threads=10 --max-time=300 threads run</span><br></pre></td></tr></table></figure>



<p>vmstat 3      整个系统上下文切换次数 整体情况</p>
<p>pidstat -wt   5   上下文切换次数  进程级别 + 线程       -w 进程切换  -t具体到线程统计</p>
<p>cpu中断情况查看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-d 参数表示高亮显示变化的区域</span><br><span class="line">watch -d cat /proc/interrupts</span><br><span class="line">           CPU0       CPU1</span><br><span class="line">RES:    2450431    5279697   Rescheduling interrupts</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>cpu上下文切换排查思路：</p>
<p>首先通过uptime查看系统负载，然后使用mpstat结合pidstat来初步判断到底是cpu计算量大还是进程争抢过大或者是io过多，接着使用vmstat分析切换次数，以及切换类型，来进一步判断到底是io过多导致问题还是进程争抢激烈导致问题。</p>
<h3><span id="cpu使用率">CPU使用率</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间。</span><br><span class="line">nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的    CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低。</span><br><span class="line"></span><br><span class="line">system（通常缩写为 sys），代表内核态 CPU 时间。</span><br><span class="line">idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）。</span><br><span class="line">iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间。</span><br><span class="line">irq（通常缩写为 hi），代表处理硬中断的 CPU 时间。</span><br><span class="line">softirq（通常缩写为 si），代表处理软中断的 CPU 时间。</span><br><span class="line">steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间。</span><br><span class="line">guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间。guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间。</span><br></pre></td></tr></table></figure>

<h4><span id="查找热点函数">查找热点函数</span></h4><p>perf top -g  -p pid    查找热点函数，定位引起cpu使用率高的具体函数</p>
<p>perf record -g  -p pid </p>
<p>perf report </p>
<p>centos install apache benchmark:</p>
<p>yum install -y  httpd-tools</p>
<p> ab -c 10 -n 100 <a target="_blank" rel="noopener" href="http://192.168.56.101:10000/">http://192.168.56.101:10000/</a>    并发10个请求测试Nginx性能，总共测试100个请求</p>
<p>perf  top -g -p 3927   正常查看调用链</p>
<p>perf record -g  -p 3927  导出到文件</p>
<p>perf  report     &#x2F;&#x2F;查看， 如果进程是在容器中，可以如下先挂载文件，再查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#进程在容器中的情况下，可以把容器目录挂载到宿主机</span><br><span class="line">mkdir /tmp/foo</span><br><span class="line">PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; &lt;container-name&gt;)</span><br><span class="line">PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; app)</span><br><span class="line">bindfs /proc/$PID/root /tmp/foo  ( yum -y install bindfs)</span><br><span class="line">perf report --symfs /tmp/foo</span><br><span class="line"></span><br><span class="line"># 使用完成后不要忘记解除绑定</span><br><span class="line">umount /tmp/foo/</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="短时进程监控">短时进程监控</span></h3><p>通过pstree 查看父子进程关系</p>
<p>perf record -g </p>
<p>perf  report     &#x2F;&#x2F;查看， 如果进程是在容器中，可以如上先挂载文件，再查看（如上）</p>
<h4><span id="execsnoop">execsnoop</span></h4><h3><span id="不可中断进程和僵尸进程">不可中断进程和僵尸进程</span></h3><p>僵尸进程,  查看调用链 pstree  -aps  pid</p>
<h3><span id="软中断">软中断</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具：top mpstat   /proc/softirqs </span><br><span class="line">说明：top使用率，其他提供各种软中断在每个cpu运行的累积次数</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>ps aux | grep softirq</p>
<p>cat &#x2F;proc&#x2F;softirqs </p>
<p>Linux 中的中断处理程序分为上半部和下半部：</p>
<p>上半部对应硬件中断，用来快速处理中断。</p>
<p>下半部对应软中断，用来异步处理上半部未完成的工作。</p>
<h3><span id="硬中断">硬中断</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具： vmstat    /proc/interrupts</span><br><span class="line">说明：vmstat总的中断次数，/proc/interrupts提供各种中断在每个cpu运行的累积次数</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="内存">内存</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">系统已用、可用、剩余内存</span><br><span class="line">free -h | vmstat | sar –r | cat /proc/meminfo</span><br><span class="line"></span><br><span class="line">进程虚拟内存、常驻内存、共享</span><br><span class="line">top  | ps aux | </span><br><span class="line">pidstat  -r  5  2  (进程内存使用统计，打印2次，每5s间隔打印一次)</span><br><span class="line"></span><br><span class="line">进程内存分布</span><br><span class="line">pmap</span><br><span class="line"></span><br><span class="line">缓存/缓冲区用量</span><br><span class="line">Free | vmstat | sar | cachestat</span><br><span class="line"></span><br><span class="line">Swap可用空间和剩余空间</span><br><span class="line">Free | sar</span><br><span class="line"></span><br><span class="line">Swap换入和换出</span><br><span class="line">vmstat</span><br><span class="line"></span><br><span class="line">内存泄漏检测</span><br><span class="line">memleak</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="定义">定义</span></h3><p>Buffer </p>
<p>Cache </p>
<p>Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中</p>
<p>cachestat 提供了整个操作系统缓存的读写命中情况</p>
<p>cachetop 提供了每个进程的缓存命中情况</p>
<p>pmap pid  进程的内存分布</p>
<p>available &#x3D; free_pages - total_reserved + pagecache + SReclaimable</p>
<p>(空闲内存减去所有zones的lowmem reserve和high watermark，再加上page cache和slab中可以回收的部分)</p>
<p>Cache &#x3D; page cache + SReclaimable</p>
<pre><code>total  Total installed memory (MemTotal and SwapTotal in /proc/meminfo)

used   Used memory (calculated as total - free - buffers - cache)

free   Unused memory (MemFree and SwapFree in /proc/meminfo)
</code></pre>
<p>​    </p>
<pre><code>buffers
   Memory used by kernel buffers (Buffers in /proc/meminfo)
cache  
   Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)
</code></pre>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93841288">知乎</a>  <a target="_blank" rel="noopener" href="https://lotabout.me/2021/Linux-Available-Memory/">内存去哪里</a></p>
<h3><span id="内存泄漏">内存泄漏</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">应用程序可以访问的用户内存空间，由只读段、数据段、堆、栈以及文件映射段等组成。</span><br><span class="line">其中，堆内存和文件映射段，需要应用程序来动态管理内存段，所以我们必须小心处理。不仅要会用标准库函数 malloc() 来动态分配内存，还要记得在用完内存后，调用库函数 free() 来释放它们。</span><br></pre></td></tr></table></figure>



<p>pmap pid  进程的内存分布</p>
<p>&#x2F;usr&#x2F;share&#x2F;bcc&#x2F;tools&#x2F;memleak -a -p  pid</p>
<p>memleak 可以跟踪系统或指定进程的内存分配、释放请求，然后定期输出一个未释放内存和相应调用栈的汇总情况（默认 5 秒）</p>
<h3><span id="swap-交换">Swap 交换</span></h3><p>Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。</p>
<p>Swap 说白了就是把一块磁盘空间或者一个本地文件</p>
<p>所谓换出，就是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存。</p>
<p>而换入，则是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来。</p>
<p>在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，来释放文件页和匿名页，以便把内存分配给更需要的进程使用。</p>
<p>文件页的回收比较容易理解，直接清空缓存，或者把脏数据写回磁盘后，再释放缓存就可以了。</p>
<p>而对不常访问的匿名页，则需要通过 Swap 换出到磁盘中，这样在下次访问的时候，再次从磁盘换入到内存中就可以了。</p>
<h2><span id="io">IO</span></h2><h3><span id="性能指标">性能指标</span></h3><p>五个常见指标: 使用率、饱和度、IOPS、吞吐量以及响应时间。这五个指标，是衡量磁盘性能的基本指标。<br>• 使用率，是指磁盘处理 I&#x2F;O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I&#x2F;O 存在性能瓶颈。<br>• 饱和度，是指磁盘处理 I&#x2F;O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I&#x2F;O 请求。<br>• IOPS（Input&#x2F;Output Per Second），是指每秒的 I&#x2F;O 请求数。<br>• 吞吐量，是指每秒的 I&#x2F;O 请求大小。<br>• 响应时间，是指 I&#x2F;O 请求从发出到收到响应的间隔时间。</p>
<p>磁盘IO性能</p>
<p>iostat -d -x 1</p>
<p>iotop</p>
<p>pidstat -d 1</p>
<p>sar -d</p>
<p>dstat</p>
<p>其他：</p>
<p>文件系统空间用量  df -h</p>
<p>索引节点使用 df -ih</p>
<p>磁盘大小 lsblk</p>
<h2><span id="网络">网络</span></h2><table>
<thead>
<tr>
<th align="left"><strong>指标</strong></th>
<th><strong>工具</strong></th>
<th align="right"><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">吞吐量</td>
<td>sar -n  DEV | nethogs</td>
<td align="right">网络接口、进程的网络吞吐量</td>
</tr>
<tr>
<td align="left">PPS</td>
<td>sar | proc&#x2F;net&#x2F;dev</td>
<td align="right">网络接口PPS</td>
</tr>
<tr>
<td align="left">连接数</td>
<td>netstat -ltunp | ss</td>
<td align="right">查看端口、网络连接数</td>
</tr>
<tr>
<td align="left">端口占用</td>
<td>lsof -i:22 | netstat -ltunp</td>
<td align="right">查看端口占用</td>
</tr>
<tr>
<td align="left">端口连通性</td>
<td>telnet 192.68.56.1 22</td>
<td align="right">查看远程服务端口是否是通的</td>
</tr>
<tr>
<td align="left">延迟</td>
<td>ping</td>
<td align="right">测试网络延迟</td>
</tr>
<tr>
<td align="left">路由</td>
<td>route | traceroute</td>
<td align="right">查看路由并测试链路信息</td>
</tr>
<tr>
<td align="left">DNS</td>
<td>dig | nslookup</td>
<td align="right">排查DNS解析问题</td>
</tr>
<tr>
<td align="left">防火墙</td>
<td>iptables</td>
<td align="right">配置和管理防火墙</td>
</tr>
<tr>
<td align="left">ip地址</td>
<td>ifconfig</td>
<td align="right"></td>
</tr>
<tr>
<td align="left">网络抓包</td>
<td>tcpdump | wireshark</td>
<td align="right">抓包分析网络流量</td>
</tr>
</tbody></table>
<p>sar -n DEV 1</p>
<p>nethogs  -d 2   按进程实时统计网络带宽利用率（yum install -y nethogs）</p>
<p>dstat  性能查询工具，包括网络收发情况</p>
<h3><span id="性能测试">性能测试</span></h3><p>tcp&#x2F;udp  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># -s表示启动服务端，-i表示汇报间隔，-p表示监听端口</span><br><span class="line">$ iperf3 -s -i 1 -p 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># -c表示启动客户端，192.168.56.100为目标服务器的IP</span><br><span class="line"># -b表示目标带宽(单位是bits/s)</span><br><span class="line"># -t表示测试时间</span><br><span class="line"># -P表示并发数，-p表示目标服务器监听端口</span><br><span class="line">$ iperf3 -c 192.168.56.101 -b 1G -t 15 -P 2 -p 10000</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>http性能：</p>
<p>ab -c 1000 -n 10000 <a target="_blank" rel="noopener" href="http://192.168.0.30/">http://192.168.0.30/</a></p>
<p>应用负载性能：</p>
<p>wrk\Jmeter\LoadRunner</p>
<h3><span id="dns">DNS</span></h3><p>cat &#x2F;etc&#x2F;resolv.conf   dns 服务器配置</p>
<p>ping  163.com</p>
<p>nslookup  163.com   域名解析</p>
<p>time nslookup  163.com   域名解析消耗时间</p>
<p>yum -y install dnsmasq; systemctl start  dnsmasq     dnsmasq 增加DNS缓存</p>
<p>nslookup -type&#x3D;PTR 35.190.27.188 8.8.8.8  根据ip解析域名(反解析)</p>
<h3><span id="流量分析">流量分析</span></h3><p>tcpdump &#x2F; Wireshark</p>
<p>tcpdump使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-nn ，表示不解析抓包中的域名（即不反向解析）、协议以及端口号。</span><br><span class="line">udp port 53 ，表示只显示 UDP 协议的端口号（包括源端口和目的端口）为 53 的包。</span><br><span class="line">host 35.190.27.188 ，表示只显示 IP 地址（包括源地址和目的地址）为 35.190.27.188 的包。</span><br><span class="line">结果保存到ping.pcap文件</span><br><span class="line"></span><br><span class="line">tcpdump -nn udp port 53 or host 35.190.27.188 -w ping.pcap</span><br><span class="line"></span><br><span class="line">--tcpdump 的输出格式</span><br><span class="line">时间戳 协议 源地址.源端口 &gt; 目的地址.目的端口 网络包详细信息</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Wireshark</p>
<p>wireshark的使用推荐阅读林沛满的《Wireshark网络分析就这么简单》和《Wireshark网络分析的艺术》</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -nn host example.com -w web.pcap</span><br><span class="line">curl http://example.com</span><br><span class="line"></span><br><span class="line">使用 Wireshark 打开 web.pcap</span><br></pre></td></tr></table></figure>



<h3><span id="nat">NAT</span></h3><p>Network Address Translation</p>
<p>NAT 技术可以重写 IP 数据包的源 IP 或者目的 IP，被普遍地用来解决公网 IP 地址短缺的问题。它的主要原理就是，网络中的多台主机，通过共享同一个公网 IP 地址，来访问外网资源。</p>
<p>NAT 的主要目的，是实现地址转换。根据实现方式的不同，</p>
<p>NAT 可以分为三类：</p>
<p>静态 NAT，即内网 IP 与公网 IP 是一对一的永久映射关系；</p>
<p>动态 NAT，即内网 IP 从公网 IP 池中，动态选择一个进行映射；</p>
<p>网络地址端口转换 NAPT（Network Address and Port Translation），即把内网 IP 映射到公网 IP 的不同端口上，让多个内网 IP 可以共享同一个公网 IP 地址。</p>
<p>网络性能</p>
<p>sar -n DEV</p>
<h2><span id="基础">基础</span></h2><h3><span id="系统">系统</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">版本：cat /etc/redhat-release</span><br></pre></td></tr></table></figure>



<h3><span id="cpu">CPU</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">架构：arch | uname -a</span><br><span class="line">cpu核数和型号： lscpu</span><br></pre></td></tr></table></figure>

<p>64位和32位处理器：64表示cpu可以处理的最大位数</p>
<p>cpu架构：x86（pc、服务器）  arm(移动端，如高通骁龙)  </p>
<p>cpu核数： 一个CPU可以包含若干个物理核，通过超线程HT（Hyper-Threading）技术可以将一个物理核变成两个逻辑处理核。vCPU（virtual CPU）是ECS实例的虚拟处理核</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查看物理CPU个数</span><br><span class="line">cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l</span><br><span class="line"></span><br><span class="line"># 查看每个物理CPU中core的个数(即核数)</span><br><span class="line">cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br><span class="line"></span><br><span class="line"># 查看逻辑CPU的个数</span><br><span class="line">cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l</span><br></pre></td></tr></table></figure>



<h3><span id="磁盘">磁盘</span></h3><p>磁盘大小:  lsblk</p>
<p>磁盘使用情况：df -h</p>
<h4><span id="磁盘raid">磁盘raid</span></h4><p>在Linux系统中，常见的RAID（冗余磁盘阵列）级别有以下几种类型：</p>
<ol>
<li><p>RAID 0（条带化）：<br>RAID 0将多个物理磁盘组合成一个逻辑卷，实现数据的条带化分布。它提供了较高的读写性能，但没有冗余功能，即一个磁盘损坏将导致数据完全丢失。</p>
</li>
<li><p>RAID 1（镜像）：<br>RAID 1通过将数据同时写入两个或多个磁盘来实现数据的镜像备份。即使一个磁盘故障，数据仍然可以从其他镜像磁盘恢复。RAID 1提供了很好的数据冗余和容错性能，但对存储空间有一定的浪费。</p>
</li>
<li><p>RAID 5：<br>RAID 5通过将数据和奇偶校验分布在多个磁盘上来提供冗余和容错性能。数据条带化并使用奇偶校验进行分布，允许从单个磁盘故障中恢复数据。RAID 5需要至少3个磁盘，并且具有相对较高的读取性能和适度的写入性能。</p>
</li>
<li><p>RAID 6：<br>RAID 6类似于RAID 5，但它使用两个奇偶校验进行冗余，提供更高的容错能力。RAID 6需要至少4个磁盘，并且可以从两个磁盘故障中恢复数据。它比RAID 5更安全，但对写入性能有一定的影响。</p>
</li>
<li><p>RAID 10（1+0）：<br>RAID 10是将RAID 1和RAID 0组合在一起的级别。它通过将数据镜像和条带化结合，提供了较高的读写性能和冗余能力。RAID 10需要至少4个磁盘，但对存储空间有一定的浪费。</p>
</li>
<li><p>RAID 50：<br>RAID 50是将RAID 5和RAID 0结合在一起的级别。它通过条带化和奇偶校验的组合提供了较高的性能和容错性能。RAID 50需要至少6个磁盘。</p>
</li>
</ol>
<p>这些是常见的Linux RAID级别，每个级别都有自己的特点、优势和限制。选择适当的RAID级别取决于数据安全性、性能需求和可用的硬件资源。注意，某些高级RAID级别可能需要专用的RAID控制器支持。</p>
<h3><span id="容器">容器</span></h3><p>OCI（Open Container Initiative）即开放的容器运行时<code>规范</code>，目的在于定义一个容器运行时及镜像的相关标准和规范，其中包括</p>
<ul>
<li>runtime-spec：容器的生命周期管理</li>
<li>image-spec：镜像的生命周期管理</li>
</ul>
<p>runc(run container)&#96;是一个基于OCI标准实现的一个轻量级容器运行工具，用来创建和运行容器。而Containerd是用来维持通过runc创建的容器的运行状态。即runc用来创建和运行容器，containerd作为常驻进程用来管理容器。</p>
<p><code>containerd（container daemon）</code>是一个daemon进程用来管理和运行容器，可以用来拉取&#x2F;推送镜像和管理容器的存储和网络。其中可以调用runc来创建和运行 容器。</p>
<p>docker 和runc、containerd的 关系：</p>
<img src="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/1.png">







<h4><span id="references">References:</span></h4><p><a target="_blank" rel="noopener" href="https://www.huweihuang.com/kubernetes-notes/runtime/runtime.html">分析OCI，CRI，runc，containerd，cri-containerd，dockershim等组件说明及调用关系</a></p>
<p><a target="_blank" rel="noopener" href="https://chinalhr.github.io/post/docker-runc/">Docker容器运行时引擎-runC分析</a></p>
<h2><span id="资料">资料</span></h2><p>极客时间：<a target="_blank" rel="noopener" href="https://time.geekbang.org/column/intro/100020901?tab=catalog">Linux性能优化实战</a>，多操作，多实践，反复阅读并实践</p>
<p>网络分析：Wireshark网络分析就这么简单 (林沛满）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/" data-id="cljm9suky00002a9a4i41bbbm" data-title="linux主机监控" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdate-spark-spark-env/">bigdate/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/">kyuubi_simple_usage</a>
          </li>
        
          <li>
            <a href="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/">spark3学习笔记20230704</a>
          </li>
        
          <li>
            <a href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">hadoop本地环境搭建</a>
          </li>
        
          <li>
            <a href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">spark环境搭建</a>
          </li>
        
          <li>
            <a href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/">processon</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>