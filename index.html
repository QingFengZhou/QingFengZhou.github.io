<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://qingfengzhou.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-system/linux/host/monitor/linux主机监控" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T10:01:05.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/">linux主机监控</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#cpu%E8%B4%9F%E8%BD%BD">cpu负载</a><ul>
<li><a href="#%E5%B9%B3%E5%9D%87%E8%B4%9F%E8%BD%BD">平均负载</a></li>
<li><a href="#cpu%E4%BD%BF%E7%94%A8%E5%88%86%E6%9E%90">cpu使用分析</a><ul>
<li><a href="#%E8%BF%9B%E7%A8%8Bcpu%E4%BD%BF%E7%94%A8%E7%BB%9F%E8%AE%A1">进程cpu使用统计</a></li>
</ul>
</li>
<li><a href="#%E8%BF%9B%E7%A8%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2">进程上下文切换</a></li>
<li><a href="#cpu%E4%BD%BF%E7%94%A8%E7%8E%87">CPU使用率</a><ul>
<li><a href="#%E6%9F%A5%E6%89%BE%E7%83%AD%E7%82%B9%E5%87%BD%E6%95%B0">查找热点函数</a></li>
</ul>
</li>
<li><a href="#%E7%9F%AD%E6%97%B6%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7">短时进程监控</a><ul>
<li><a href="#execsnoop">execsnoop</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%8F%AF%E4%B8%AD%E6%96%AD%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B">不可中断进程和僵尸进程</a></li>
<li><a href="#%E8%BD%AF%E4%B8%AD%E6%96%AD">软中断</a></li>
<li><a href="#%E7%A1%AC%E4%B8%AD%E6%96%AD">硬中断</a></li>
</ul>
</li>
<li><a href="#%E5%86%85%E5%AD%98">内存</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%89">定义</a></li>
<li><a href="#%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F">内存泄漏</a></li>
<li><a href="#swap-%E4%BA%A4%E6%8D%A2">Swap 交换</a></li>
</ul>
</li>
<li><a href="#io">IO</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87">性能指标</a></li>
</ul>
</li>
<li><a href="#%E7%BD%91%E7%BB%9C">网络</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">性能测试</a></li>
<li><a href="#dns">DNS</a></li>
<li><a href="#%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90">流量分析</a></li>
<li><a href="#nat">NAT</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E7%A1%80">基础</a><ul>
<li><a href="#%E7%B3%BB%E7%BB%9F">系统</a></li>
<li><a href="#cpu">CPU</a></li>
<li><a href="#%E7%A3%81%E7%9B%98">磁盘</a><ul>
<li><a href="#%E7%A3%81%E7%9B%98raid">磁盘raid</a></li>
</ul>
</li>
<li><a href="#%E5%AE%B9%E5%99%A8">容器</a><ul>
<li><a href="#references">References:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E8%B5%84%E6%96%99">资料</a></li>
</ul>
<!-- tocstop -->

<h2><span id="cpu负载">cpu负载</span></h2><h3><span id="平均负载">平均负载</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Load average，系统的平均活跃进程数。它反应了系统的整体负载情况。 uptime显示三个数值，分别指过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载；理想情况下，平均负载等于逻辑 CPU 个数</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>uptime或top查看 CPU负载情况， （watch -d uptime  -d 参数表示高亮显示变化的区域）</p>
<p>定义：平均负载，表示的是活跃进程数，</p>
<p>单位时间内，系统中处于可运行状态和不可中断状态的平均进程数</p>
<p>可运行状态的进程：正在使用cpu或者正在等待cpu的进程，即ps aux命令下STAT处于R状态的进程 </p>
<p>不可中断状态的进程：处于内核态关键流程中的进程，且不可被打断，如等待硬件设备IO响应，ps命令D状态的进程</p>
<p>1、CPU密集型</p>
<p>2、IO 密集型</p>
<p>3、大量进程</p>
<h3><span id="cpu使用分析">cpu使用分析</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">含义：cpu非空闲时间占总 CPU 时间的百分比。cpu状态(us、ni、sys、id、wa、hi、si、st、guest)</span><br><span class="line">工具：top  vmstat  mpstat  sar  /proc/stat</span><br><span class="line">说明：/proc/stat是其他性能工具的数据来源</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>分析工具：安装stress(压测)  sysstat（分析）</p>
<p>stress   -c 1</p>
<p>stress   -c 8        spawn N workers spinning on sqrt()  模拟8个进程，同时在执行任务sqrt</p>
<p>stress   -d 2        spawn N workers spinning on write()&#x2F;unlink() 模拟2个进程，同时在执行磁盘写操作</p>
<h4><span id="进程cpu使用统计">进程cpu使用统计</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具：top pidstat  ps</span><br><span class="line">说明：top和ps按cpu使用率对进程排序，pidstat只显示实际用了cpu的进程</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>cpu监控：mpstat   -P ALL 5  2      打印2次，每5s间隔打印一次，监控所有cpu,</p>
<p>进程级别监控：</p>
<p>​     pidstat  -u 5  2    进程cpu使用统计，打印2次，每5s间隔打印一次</p>
<p>​     pidstat -d 5 2    进程IO统计(磁盘读写速率)，打印2次，每5s间隔打印一次</p>
<p>​     pidstat  -r  5  2    进程内存使用统计，打印2次，每5s间隔打印一次</p>
<h3><span id="进程上下文切换">进程上下文切换</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmstat 3      整个系统上下文切换次数 整体情况</span><br><span class="line">pidstat -wt   5   上下文切换次数  进程级别 + 线程       -w 进程切换  -t具体到线程统计</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>进程的切换只能发生在内核态。所以，</p>
<p>进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，</p>
<p>还包括了内核堆栈、寄存器等内核空间的状态</p>
<p>所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I&#x2F;O、内存等系统资源不足时，就会发生自愿上下文切换。</p>
<p>而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换</p>
<p>压测工具 sysbench：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 以10个线程运行5分钟的基准测试，模拟多线程切换的问题</span><br><span class="line">sysbench --threads=10 --max-time=300 threads run</span><br></pre></td></tr></table></figure>



<p>vmstat 3      整个系统上下文切换次数 整体情况</p>
<p>pidstat -wt   5   上下文切换次数  进程级别 + 线程       -w 进程切换  -t具体到线程统计</p>
<p>cpu中断情况查看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-d 参数表示高亮显示变化的区域</span><br><span class="line">watch -d cat /proc/interrupts</span><br><span class="line">           CPU0       CPU1</span><br><span class="line">RES:    2450431    5279697   Rescheduling interrupts</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>cpu上下文切换排查思路：</p>
<p>首先通过uptime查看系统负载，然后使用mpstat结合pidstat来初步判断到底是cpu计算量大还是进程争抢过大或者是io过多，接着使用vmstat分析切换次数，以及切换类型，来进一步判断到底是io过多导致问题还是进程争抢激烈导致问题。</p>
<h3><span id="cpu使用率">CPU使用率</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间。</span><br><span class="line">nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的    CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低。</span><br><span class="line"></span><br><span class="line">system（通常缩写为 sys），代表内核态 CPU 时间。</span><br><span class="line">idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）。</span><br><span class="line">iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间。</span><br><span class="line">irq（通常缩写为 hi），代表处理硬中断的 CPU 时间。</span><br><span class="line">softirq（通常缩写为 si），代表处理软中断的 CPU 时间。</span><br><span class="line">steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间。</span><br><span class="line">guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间。guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间。</span><br></pre></td></tr></table></figure>

<h4><span id="查找热点函数">查找热点函数</span></h4><p>perf top -g  -p pid    查找热点函数，定位引起cpu使用率高的具体函数</p>
<p>perf record -g  -p pid </p>
<p>perf report </p>
<p>centos install apache benchmark:</p>
<p>yum install -y  httpd-tools</p>
<p> ab -c 10 -n 100 <a target="_blank" rel="noopener" href="http://192.168.56.101:10000/">http://192.168.56.101:10000/</a>    并发10个请求测试Nginx性能，总共测试100个请求</p>
<p>perf  top -g -p 3927   正常查看调用链</p>
<p>perf record -g  -p 3927  导出到文件</p>
<p>perf  report     &#x2F;&#x2F;查看， 如果进程是在容器中，可以如下先挂载文件，再查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#进程在容器中的情况下，可以把容器目录挂载到宿主机</span><br><span class="line">mkdir /tmp/foo</span><br><span class="line">PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; &lt;container-name&gt;)</span><br><span class="line">PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; app)</span><br><span class="line">bindfs /proc/$PID/root /tmp/foo  ( yum -y install bindfs)</span><br><span class="line">perf report --symfs /tmp/foo</span><br><span class="line"></span><br><span class="line"># 使用完成后不要忘记解除绑定</span><br><span class="line">umount /tmp/foo/</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="短时进程监控">短时进程监控</span></h3><p>通过pstree 查看父子进程关系</p>
<p>perf record -g </p>
<p>perf  report     &#x2F;&#x2F;查看， 如果进程是在容器中，可以如上先挂载文件，再查看（如上）</p>
<h4><span id="execsnoop">execsnoop</span></h4><h3><span id="不可中断进程和僵尸进程">不可中断进程和僵尸进程</span></h3><p>僵尸进程,  查看调用链 pstree  -aps  pid</p>
<h3><span id="软中断">软中断</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具：top mpstat   /proc/softirqs </span><br><span class="line">说明：top使用率，其他提供各种软中断在每个cpu运行的累积次数</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>ps aux | grep softirq</p>
<p>cat &#x2F;proc&#x2F;softirqs </p>
<p>Linux 中的中断处理程序分为上半部和下半部：</p>
<p>上半部对应硬件中断，用来快速处理中断。</p>
<p>下半部对应软中断，用来异步处理上半部未完成的工作。</p>
<h3><span id="硬中断">硬中断</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具： vmstat    /proc/interrupts</span><br><span class="line">说明：vmstat总的中断次数，/proc/interrupts提供各种中断在每个cpu运行的累积次数</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="内存">内存</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">系统已用、可用、剩余内存</span><br><span class="line">free -h | vmstat | sar –r | cat /proc/meminfo</span><br><span class="line"></span><br><span class="line">进程虚拟内存、常驻内存、共享</span><br><span class="line">top  | ps aux | </span><br><span class="line">pidstat  -r  5  2  (进程内存使用统计，打印2次，每5s间隔打印一次)</span><br><span class="line"></span><br><span class="line">进程内存分布</span><br><span class="line">pmap</span><br><span class="line"></span><br><span class="line">缓存/缓冲区用量</span><br><span class="line">Free | vmstat | sar | cachestat</span><br><span class="line"></span><br><span class="line">Swap可用空间和剩余空间</span><br><span class="line">Free | sar</span><br><span class="line"></span><br><span class="line">Swap换入和换出</span><br><span class="line">vmstat</span><br><span class="line"></span><br><span class="line">内存泄漏检测</span><br><span class="line">memleak</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="定义">定义</span></h3><p>Buffer </p>
<p>Cache </p>
<p>Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中</p>
<p>cachestat 提供了整个操作系统缓存的读写命中情况</p>
<p>cachetop 提供了每个进程的缓存命中情况</p>
<p>pmap pid  进程的内存分布</p>
<p>available &#x3D; free_pages - total_reserved + pagecache + SReclaimable</p>
<p>(空闲内存减去所有zones的lowmem reserve和high watermark，再加上page cache和slab中可以回收的部分)</p>
<p>Cache &#x3D; page cache + SReclaimable</p>
<pre><code>total  Total installed memory (MemTotal and SwapTotal in /proc/meminfo)

used   Used memory (calculated as total - free - buffers - cache)

free   Unused memory (MemFree and SwapFree in /proc/meminfo)
</code></pre>
<p>​    </p>
<pre><code>buffers
   Memory used by kernel buffers (Buffers in /proc/meminfo)
cache  
   Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)
</code></pre>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93841288">知乎</a>  <a target="_blank" rel="noopener" href="https://lotabout.me/2021/Linux-Available-Memory/">内存去哪里</a></p>
<h3><span id="内存泄漏">内存泄漏</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">应用程序可以访问的用户内存空间，由只读段、数据段、堆、栈以及文件映射段等组成。</span><br><span class="line">其中，堆内存和文件映射段，需要应用程序来动态管理内存段，所以我们必须小心处理。不仅要会用标准库函数 malloc() 来动态分配内存，还要记得在用完内存后，调用库函数 free() 来释放它们。</span><br></pre></td></tr></table></figure>



<p>pmap pid  进程的内存分布</p>
<p>&#x2F;usr&#x2F;share&#x2F;bcc&#x2F;tools&#x2F;memleak -a -p  pid</p>
<p>memleak 可以跟踪系统或指定进程的内存分配、释放请求，然后定期输出一个未释放内存和相应调用栈的汇总情况（默认 5 秒）</p>
<h3><span id="swap-交换">Swap 交换</span></h3><p>Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。</p>
<p>Swap 说白了就是把一块磁盘空间或者一个本地文件</p>
<p>所谓换出，就是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存。</p>
<p>而换入，则是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来。</p>
<p>在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，来释放文件页和匿名页，以便把内存分配给更需要的进程使用。</p>
<p>文件页的回收比较容易理解，直接清空缓存，或者把脏数据写回磁盘后，再释放缓存就可以了。</p>
<p>而对不常访问的匿名页，则需要通过 Swap 换出到磁盘中，这样在下次访问的时候，再次从磁盘换入到内存中就可以了。</p>
<h2><span id="io">IO</span></h2><h3><span id="性能指标">性能指标</span></h3><p>五个常见指标: 使用率、饱和度、IOPS、吞吐量以及响应时间。这五个指标，是衡量磁盘性能的基本指标。<br>• 使用率，是指磁盘处理 I&#x2F;O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I&#x2F;O 存在性能瓶颈。<br>• 饱和度，是指磁盘处理 I&#x2F;O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I&#x2F;O 请求。<br>• IOPS（Input&#x2F;Output Per Second），是指每秒的 I&#x2F;O 请求数。<br>• 吞吐量，是指每秒的 I&#x2F;O 请求大小。<br>• 响应时间，是指 I&#x2F;O 请求从发出到收到响应的间隔时间。</p>
<p>磁盘IO性能</p>
<p>iostat -d -x 1</p>
<p>iotop</p>
<p>pidstat -d 1</p>
<p>sar -d</p>
<p>dstat</p>
<p>其他：</p>
<p>文件系统空间用量  df -h</p>
<p>索引节点使用 df -ih</p>
<p>磁盘大小 lsblk</p>
<h2><span id="网络">网络</span></h2><table>
<thead>
<tr>
<th align="left"><strong>指标</strong></th>
<th><strong>工具</strong></th>
<th align="right"><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">吞吐量</td>
<td>sar -n  DEV | nethogs</td>
<td align="right">网络接口、进程的网络吞吐量</td>
</tr>
<tr>
<td align="left">PPS</td>
<td>sar | proc&#x2F;net&#x2F;dev</td>
<td align="right">网络接口PPS</td>
</tr>
<tr>
<td align="left">连接数</td>
<td>netstat -ltunp | ss</td>
<td align="right">查看端口、网络连接数</td>
</tr>
<tr>
<td align="left">端口占用</td>
<td>lsof -i:22 | netstat -ltunp</td>
<td align="right">查看端口占用</td>
</tr>
<tr>
<td align="left">端口连通性</td>
<td>telnet 192.68.56.1 22</td>
<td align="right">查看远程服务端口是否是通的</td>
</tr>
<tr>
<td align="left">延迟</td>
<td>ping</td>
<td align="right">测试网络延迟</td>
</tr>
<tr>
<td align="left">路由</td>
<td>route | traceroute</td>
<td align="right">查看路由并测试链路信息</td>
</tr>
<tr>
<td align="left">DNS</td>
<td>dig | nslookup</td>
<td align="right">排查DNS解析问题</td>
</tr>
<tr>
<td align="left">防火墙</td>
<td>iptables</td>
<td align="right">配置和管理防火墙</td>
</tr>
<tr>
<td align="left">ip地址</td>
<td>ifconfig</td>
<td align="right"></td>
</tr>
<tr>
<td align="left">网络抓包</td>
<td>tcpdump | wireshark</td>
<td align="right">抓包分析网络流量</td>
</tr>
</tbody></table>
<p>sar -n DEV 1</p>
<p>nethogs  -d 2   按进程实时统计网络带宽利用率（yum install -y nethogs）</p>
<p>dstat  性能查询工具，包括网络收发情况</p>
<h3><span id="性能测试">性能测试</span></h3><p>tcp&#x2F;udp  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># -s表示启动服务端，-i表示汇报间隔，-p表示监听端口</span><br><span class="line">$ iperf3 -s -i 1 -p 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># -c表示启动客户端，192.168.56.100为目标服务器的IP</span><br><span class="line"># -b表示目标带宽(单位是bits/s)</span><br><span class="line"># -t表示测试时间</span><br><span class="line"># -P表示并发数，-p表示目标服务器监听端口</span><br><span class="line">$ iperf3 -c 192.168.56.101 -b 1G -t 15 -P 2 -p 10000</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>http性能：</p>
<p>ab -c 1000 -n 10000 <a target="_blank" rel="noopener" href="http://192.168.0.30/">http://192.168.0.30/</a></p>
<p>应用负载性能：</p>
<p>wrk\Jmeter\LoadRunner</p>
<h3><span id="dns">DNS</span></h3><p>cat &#x2F;etc&#x2F;resolv.conf   dns 服务器配置</p>
<p>ping  163.com</p>
<p>nslookup  163.com   域名解析</p>
<p>time nslookup  163.com   域名解析消耗时间</p>
<p>yum -y install dnsmasq; systemctl start  dnsmasq     dnsmasq 增加DNS缓存</p>
<p>nslookup -type&#x3D;PTR 35.190.27.188 8.8.8.8  根据ip解析域名(反解析)</p>
<h3><span id="流量分析">流量分析</span></h3><p>tcpdump &#x2F; Wireshark</p>
<p>tcpdump使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-nn ，表示不解析抓包中的域名（即不反向解析）、协议以及端口号。</span><br><span class="line">udp port 53 ，表示只显示 UDP 协议的端口号（包括源端口和目的端口）为 53 的包。</span><br><span class="line">host 35.190.27.188 ，表示只显示 IP 地址（包括源地址和目的地址）为 35.190.27.188 的包。</span><br><span class="line">结果保存到ping.pcap文件</span><br><span class="line"></span><br><span class="line">tcpdump -nn udp port 53 or host 35.190.27.188 -w ping.pcap</span><br><span class="line"></span><br><span class="line">--tcpdump 的输出格式</span><br><span class="line">时间戳 协议 源地址.源端口 &gt; 目的地址.目的端口 网络包详细信息</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Wireshark</p>
<p>wireshark的使用推荐阅读林沛满的《Wireshark网络分析就这么简单》和《Wireshark网络分析的艺术》</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -nn host example.com -w web.pcap</span><br><span class="line">curl http://example.com</span><br><span class="line"></span><br><span class="line">使用 Wireshark 打开 web.pcap</span><br></pre></td></tr></table></figure>



<h3><span id="nat">NAT</span></h3><p>Network Address Translation</p>
<p>NAT 技术可以重写 IP 数据包的源 IP 或者目的 IP，被普遍地用来解决公网 IP 地址短缺的问题。它的主要原理就是，网络中的多台主机，通过共享同一个公网 IP 地址，来访问外网资源。</p>
<p>NAT 的主要目的，是实现地址转换。根据实现方式的不同，</p>
<p>NAT 可以分为三类：</p>
<p>静态 NAT，即内网 IP 与公网 IP 是一对一的永久映射关系；</p>
<p>动态 NAT，即内网 IP 从公网 IP 池中，动态选择一个进行映射；</p>
<p>网络地址端口转换 NAPT（Network Address and Port Translation），即把内网 IP 映射到公网 IP 的不同端口上，让多个内网 IP 可以共享同一个公网 IP 地址。</p>
<p>网络性能</p>
<p>sar -n DEV</p>
<h2><span id="基础">基础</span></h2><h3><span id="系统">系统</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">版本：cat /etc/redhat-release</span><br></pre></td></tr></table></figure>



<h3><span id="cpu">CPU</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">架构：arch | uname -a</span><br><span class="line">cpu核数和型号： lscpu</span><br></pre></td></tr></table></figure>

<p>64位和32位处理器：64表示cpu可以处理的最大位数</p>
<p>cpu架构：x86（pc、服务器）  arm(移动端，如高通骁龙)  </p>
<p>cpu核数： 一个CPU可以包含若干个物理核，通过超线程HT（Hyper-Threading）技术可以将一个物理核变成两个逻辑处理核。vCPU（virtual CPU）是ECS实例的虚拟处理核</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查看物理CPU个数</span><br><span class="line">cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l</span><br><span class="line"></span><br><span class="line"># 查看每个物理CPU中core的个数(即核数)</span><br><span class="line">cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br><span class="line"></span><br><span class="line"># 查看逻辑CPU的个数</span><br><span class="line">cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l</span><br></pre></td></tr></table></figure>



<h3><span id="磁盘">磁盘</span></h3><p>磁盘大小:  lsblk</p>
<p>磁盘使用情况：df -h</p>
<h4><span id="磁盘raid">磁盘raid</span></h4><p>在Linux系统中，常见的RAID（冗余磁盘阵列）级别有以下几种类型：</p>
<ol>
<li><p>RAID 0（条带化）：<br>RAID 0将多个物理磁盘组合成一个逻辑卷，实现数据的条带化分布。它提供了较高的读写性能，但没有冗余功能，即一个磁盘损坏将导致数据完全丢失。</p>
</li>
<li><p>RAID 1（镜像）：<br>RAID 1通过将数据同时写入两个或多个磁盘来实现数据的镜像备份。即使一个磁盘故障，数据仍然可以从其他镜像磁盘恢复。RAID 1提供了很好的数据冗余和容错性能，但对存储空间有一定的浪费。</p>
</li>
<li><p>RAID 5：<br>RAID 5通过将数据和奇偶校验分布在多个磁盘上来提供冗余和容错性能。数据条带化并使用奇偶校验进行分布，允许从单个磁盘故障中恢复数据。RAID 5需要至少3个磁盘，并且具有相对较高的读取性能和适度的写入性能。</p>
</li>
<li><p>RAID 6：<br>RAID 6类似于RAID 5，但它使用两个奇偶校验进行冗余，提供更高的容错能力。RAID 6需要至少4个磁盘，并且可以从两个磁盘故障中恢复数据。它比RAID 5更安全，但对写入性能有一定的影响。</p>
</li>
<li><p>RAID 10（1+0）：<br>RAID 10是将RAID 1和RAID 0组合在一起的级别。它通过将数据镜像和条带化结合，提供了较高的读写性能和冗余能力。RAID 10需要至少4个磁盘，但对存储空间有一定的浪费。</p>
</li>
<li><p>RAID 50：<br>RAID 50是将RAID 5和RAID 0结合在一起的级别。它通过条带化和奇偶校验的组合提供了较高的性能和容错性能。RAID 50需要至少6个磁盘。</p>
</li>
</ol>
<p>这些是常见的Linux RAID级别，每个级别都有自己的特点、优势和限制。选择适当的RAID级别取决于数据安全性、性能需求和可用的硬件资源。注意，某些高级RAID级别可能需要专用的RAID控制器支持。</p>
<h3><span id="容器">容器</span></h3><p>OCI（Open Container Initiative）即开放的容器运行时<code>规范</code>，目的在于定义一个容器运行时及镜像的相关标准和规范，其中包括</p>
<ul>
<li>runtime-spec：容器的生命周期管理</li>
<li>image-spec：镜像的生命周期管理</li>
</ul>
<p>runc(run container)&#96;是一个基于OCI标准实现的一个轻量级容器运行工具，用来创建和运行容器。而Containerd是用来维持通过runc创建的容器的运行状态。即runc用来创建和运行容器，containerd作为常驻进程用来管理容器。</p>
<p><code>containerd（container daemon）</code>是一个daemon进程用来管理和运行容器，可以用来拉取&#x2F;推送镜像和管理容器的存储和网络。其中可以调用runc来创建和运行 容器。</p>
<p>docker 和runc、containerd的 关系：</p>
<img src="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/1.png">







<h4><span id="references">References:</span></h4><p><a target="_blank" rel="noopener" href="https://www.huweihuang.com/kubernetes-notes/runtime/runtime.html">分析OCI，CRI，runc，containerd，cri-containerd，dockershim等组件说明及调用关系</a></p>
<p><a target="_blank" rel="noopener" href="https://chinalhr.github.io/post/docker-runc/">Docker容器运行时引擎-runC分析</a></p>
<h2><span id="资料">资料</span></h2><p>极客时间：<a target="_blank" rel="noopener" href="https://time.geekbang.org/column/intro/100020901?tab=catalog">Linux性能优化实战</a>，多操作，多实践，反复阅读并实践</p>
<p>网络分析：Wireshark网络分析就这么简单 (林沛满）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/" data-id="cljm9suky00002a9a4i41bbbm" data-title="linux主机监控" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-tools/github/hexo/simple_usage" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/tools/github/hexo/simple_usage/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T09:06:37.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/tools-github-hexo/">tools/github/hexo</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/tools/github/hexo/simple_usage/">simple_usage</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#install-and-usage">Install And Usage</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98">其他问题</a><ul>
<li><a href="#%E7%9B%AE%E5%BD%95%E4%B8%8D%E6%98%BE%E7%A4%BA">目录不显示</a></li>
<li><a href="#%E5%9B%BE%E7%89%87%E4%B8%8D%E6%98%BE%E7%A4%BA">图片不显示</a></li>
<li><a href="#github-token-%E8%AE%BE%E7%BD%AE">Github token 设置</a></li>
<li><a href="#think">Think</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="install-and-usage">Install And Usage</span></h2><p>环境： macos 12.4  | Intel Core i7 | hexo: 6.3.0  | hexo-cli: 4.3.1  | node: 16.14.2</p>
<p>1、github 新建仓库 username.github.io</p>
<p>2、install node.js:  brew install node  &#x2F; <a target="_blank" rel="noopener" href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a></p>
<p>3、install hexo<br>npm install -g hexo-cli</p>
<p>4、指定目录，初始化<br>mkdir &#x2F;Users&#x2F;username&#x2F;Desktop&#x2F;mydirect&#x2F;gitblog&#x2F;cont<br>hexo init</p>
<p>(出现问题，找不到或无法加载主类 install： npm install，hexo s )<br>5、本地运行<br>hexo s</p>
<p>6、部署到github<br>a、新建仓库  username.github.io（username 为github用户名）<br>b、修改根目录配置文件 _congif.yml</p>
<p>deploy:<br>  type: git<br>  repo: <a target="_blank" rel="noopener" href="https://github.com/username/username.github.io.git">https://github.com/username/username.github.io.git</a><br>  branch: master</p>
<p>c、<br>npm install hexo-deployer-git –save<br>hexo clean<br>hexo deploy</p>
<p>d、查看效果<br><a target="_blank" rel="noopener" href="https://username.github.io/">https://username.github.io/</a></p>
<p>7、发布一篇博客</p>
<p>a.</p>
<p>hexo new  ‘HBase概览’  -p  nosql&#x2F;hbase&#x2F;HBase概览</p>
<p>hexo new  ‘realtime_doc’  -p  bigdata&#x2F;warehouse&#x2F;realtime_doc</p>
<p>b.<br>source&#x2F;_posts&#x2F;nosql&#x2F;hbase&#x2F;目录下找到刚才建立的 HBase概览.md文件, 进行编辑</p>
<p>title: Hello post #文章标题<br>date: 2014-08-05 11:15:00 #发表时间<br>categories: #分类<br>toc: true</p>
<p>tags: #标签，多个标签时可以用[标签1,标签2]的方式，或者”- 标签“的方式每行一个。</p>
<p>#这里是正文，用markdown语法写。</p>
<p>c. 编辑完文章后依次执行hexo g和hexo s,然后访问localhost:4000来预览效果<br>d. 没有问题了以后，执行hexo d来同步到GitHub</p>
<h2><span id="其他问题">其他问题</span></h2><p>其他：<br>ssh免密码登录配置<br>更换主题：<a target="_blank" rel="noopener" href="https://hexo.io/themes/">https://hexo.io/themes/</a> 下载，安装，<br>下载项目至博客项目下的themes目录中，文件夹命名为material，并在博客配置文件_config.yml中指定使用该主题<br>hexo 使用：<a target="_blank" rel="noopener" href="http://ijiaober.github.io/2014/08/04/hexo/hexo-03/">http://ijiaober.github.io/2014/08/04/hexo/hexo-03/</a></p>
<p>–如何上传source .md 到github ?<br>–图片显示  <a target="_blank" rel="noopener" href="https://chiselee.cn/2020/02/04/HexoNotShowpic/">https://chiselee.cn/2020/02/04/HexoNotShowpic/</a><br>–目录显示  <a target="_blank" rel="noopener" href="http://kuangqi.me/tricks/enable-table-of-contents-on-hexo/">http://kuangqi.me/tricks/enable-table-of-contents-on-hexo/</a></p>
<p>npm install <a target="_blank" rel="noopener" href="https://github.com/CodeFalling/hexo-asset-image">https://github.com/CodeFalling/hexo-asset-image</a> –save</p>
<h3><span id="目录不显示">目录不显示</span></h3><p>1、安装hex-toc 插件：npm install  hexo-toc –save</p>
<p>2、修改_config.yml:  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">toc:</span><br><span class="line">  maxdepth: 6</span><br></pre></td></tr></table></figure>

<p>3、在需要显示目录的地方 添加一行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- toc --&gt;</span><br></pre></td></tr></table></figure>



<h3><span id="图片不显示">图片不显示</span></h3><p>1、安装一个图片路径转换的插件，这个插件名字是hexo-asset-image</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure>

<p>2、修改node_modules&#x2F;hexo-asset-image&#x2F;index.js，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">&#x27;use strict&#x27;;</span><br><span class="line">var cheerio = require(&#x27;cheerio&#x27;);</span><br><span class="line"></span><br><span class="line">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span><br><span class="line">function getPosition(str, m, i) &#123;</span><br><span class="line">  return str.split(m, i).join(m).length;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">var version = String(hexo.version).split(&#x27;.&#x27;);</span><br><span class="line">hexo.extend.filter.register(&#x27;after_post_render&#x27;, function(data)&#123;</span><br><span class="line">  var config = hexo.config;</span><br><span class="line">  if(config.post_asset_folder)&#123;</span><br><span class="line">    	var link = data.permalink;</span><br><span class="line">	if(version.length &gt; 0 &amp;&amp; Number(version[0]) == 3)</span><br><span class="line">	   var beginPos = getPosition(link, &#x27;/&#x27;, 1) + 1;</span><br><span class="line">	else</span><br><span class="line">	   var beginPos = getPosition(link, &#x27;/&#x27;, 3) + 1;</span><br><span class="line">	// In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.</span><br><span class="line">	var endPos = link.lastIndexOf(&#x27;/&#x27;) + 1;</span><br><span class="line">    link = link.substring(beginPos, endPos);</span><br><span class="line"></span><br><span class="line">    var toprocess = [&#x27;excerpt&#x27;, &#x27;more&#x27;, &#x27;content&#x27;];</span><br><span class="line">    for(var i = 0; i &lt; toprocess.length; i++)&#123;</span><br><span class="line">      var key = toprocess[i];</span><br><span class="line"> </span><br><span class="line">      var $ = cheerio.load(data[key], &#123;</span><br><span class="line">        ignoreWhitespace: false,</span><br><span class="line">        xmlMode: false,</span><br><span class="line">        lowerCaseTags: false,</span><br><span class="line">        decodeEntities: false</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      $(&#x27;img&#x27;).each(function()&#123;</span><br><span class="line">		if ($(this).attr(&#x27;src&#x27;))&#123;</span><br><span class="line">			// For windows style path, we replace &#x27;\&#x27; to &#x27;/&#x27;.</span><br><span class="line">			var src = $(this).attr(&#x27;src&#x27;).replace(&#x27;\\&#x27;, &#x27;/&#x27;);</span><br><span class="line">			if(!/http[s]*.*|\/\/.*/.test(src) &amp;&amp;</span><br><span class="line">			   !/^\s*\//.test(src)) &#123;</span><br><span class="line">			  // For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed.</span><br><span class="line">			  // In addition, to support multi-level local directory.</span><br><span class="line">			  var linkArray = link.split(&#x27;/&#x27;).filter(function(elem)&#123;</span><br><span class="line">				return elem != &#x27;&#x27;;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  var srcArray = src.split(&#x27;/&#x27;).filter(function(elem)&#123;</span><br><span class="line">				return elem != &#x27;&#x27; &amp;&amp; elem != &#x27;.&#x27;;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  if(srcArray.length &gt; 1)</span><br><span class="line">				srcArray.shift();</span><br><span class="line">			  src = srcArray.join(&#x27;/&#x27;);</span><br><span class="line">			  $(this).attr(&#x27;src&#x27;, config.root + link + src);</span><br><span class="line">			  console.info&amp;&amp;console.info(&quot;update link as:--&gt;&quot;+config.root + link + src);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;else&#123;</span><br><span class="line">			console.info&amp;&amp;console.info(&quot;no src attr, skipped...&quot;);</span><br><span class="line">			console.info&amp;&amp;console.info($(this));</span><br><span class="line">		&#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      data[key] = $.html();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、修改_config.yml文件，新建文章时自动创建一个与文章名相同的文件夹用来存放图片文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>

<p>比如新建文章simple_usage时， 执行 hexo new  ‘simple_usage’  -p  tools&#x2F;github&#x2F;hexo&#x2F;simple_usage，</p>
<p>自动创建目录simple_usage，该目录存放图片，</p>
<p><strong>注意 注意</strong></p>
<p><strong>特别需要注意的是</strong>：markdown中插入图片要使用img的用法，并且要使用相对路径，否则不显示，示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img  src=&quot;simple_usage/截屏2023-07-03 16.34.09.png&quot;&gt;</span><br></pre></td></tr></table></figure>

<img src="/2023/07/03/tools/github/hexo/simple_usage/截屏2023-07-03 16.34.09.png">



<p>参考资料:  </p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/542101567">hexo图片不显示</a></p>
<p><a target="_blank" rel="noopener" href="https://juejin.cn/post/7006594302604214280">解决hexo引用本地图片无法显示的问题</a></p>
<h3><span id="github-token-设置">Github token 设置</span></h3><p><a target="_blank" rel="noopener" href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens">github access token设置</a></p>
<h3><span id="think">Think</span></h3><p>所有资料仅为参考，要根据实际问题具体分析</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/tools/github/hexo/simple_usage/" data-id="cljmm9dpf00008t9abge5cb7s" data-title="simple_usage" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/nosql/hbase/HBase概览" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T07:44:53.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/">HBase概览</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#hbase">HBase</a><ul>
<li><a href="#one">One</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="hbase">HBase</span></h2><h3><span id="one">One</span></h3><img src="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/截屏2023-07-03 15.49.39.png">


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/" data-id="cljmkav790000qj9ack1hbz8f" data-title="HBase概览" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_file_read_partition_num" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_file_read_partition_num/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:35:32.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_file_read_partition_num/">spark_file_read_partition_num</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h2><p>文件格式: text、csv、json、parquet、orc、avro</p>
<p>压缩格式：snappy、gzip、bzip2、lzo、lz4 (bzip2和lzo支持切分)</p>
<p>spark 读取数据分区个数计算：</p>
<p>影响因子：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">block.size(默认128m)、openCostInBytes(文件打开开销，比如打开一个parquet文件需要4m)、</span><br><span class="line">minPartitionNum(分区个数，最少为2)、totalBytes(总内存开销:文件大小+openCostInBytes)、</span><br><span class="line">bytesPerCore(每个core的内存开心 totalBytes/core num)</span><br><span class="line">举例：10个核，block.size 128m，parquet文件总大小 = 113.918m，8个文件，</span><br><span class="line">计算最大分片大小maxSplitBytes 过程如下, 会读snappy.parquet进行切分：</span><br><span class="line"></span><br><span class="line">defaultMaxSplitBytes = 128MB</span><br><span class="line">openCostInBytes = 4MB</span><br><span class="line">minPartitionNum = max(10, 2) = 10</span><br><span class="line">totalBytes = 113.918 + 8 * 4MB = 145.918MB</span><br><span class="line">bytesPerCore = 145.918MB / 10 = 14.5918MB</span><br><span class="line">maxSplitBytes = 14.5918MB = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</span><br></pre></td></tr></table></figure>

<p>测试：spark读取parquet 会对自动对小文件合并，大文件拆分，</p>
<p>3个executor(3核)读取4.46m文件对自动进行切分为2个分片，读取54kb的parquet文件不会对文件切分</p>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/544905929">parquet 源数据分区个数计算剖析</a>   <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/490729788">文件压缩格式</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_file_read_partition_num/" data-id="cljllibc90003029a6lrz68g3" data-title="spark_file_read_partition_num" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_file_combine" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_file_combine/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:30:09.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_file_combine/">spark_file_combine</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists emp_ext(</span><br><span class="line">    	empno int,</span><br><span class="line">	ename string,</span><br><span class="line">	job string,</span><br><span class="line">	mgr int,</span><br><span class="line">	hiredate string,</span><br><span class="line">	sal double,</span><br><span class="line">	comm double,</span><br><span class="line">	deptno int</span><br><span class="line">) row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">LOCATION &#x27;/user/hive/warehouse/emp&#x27;;</span><br><span class="line"></span><br><span class="line">CREATE EXTERNAL TABLE parquet_test1 (value string) </span><br><span class="line">STORED AS PARQUET </span><br><span class="line">LOCATION &#x27;/tmp/test_out/parquet_big/&#x27;</span><br><span class="line">TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;SNAPPY&quot;)</span><br><span class="line">;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>(1)   使用 </p>
<p>Spark sql:</p>
<p>思路：创建临时表，通过查看文件目录大小判断分区个数，通过distribute by num_partitions重分区 写入到临时表，判断临时表和源表数据内容是否一致，如果一致，删除源表，重命名临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># insert overwrite源表 报错</span><br><span class="line">insert overwrite table parquet_test1 select * from parquet_test1;</span><br><span class="line"></span><br><span class="line">create table parquet_test2 like  parquet_test1 LOCATION &#x27;/tmp/test_out/parquet_big3/&#x27;</span><br><span class="line"></span><br><span class="line">insert overwrite table parquet_test2 select * from parquet_test1 distribute by floor(rand()*2);</span><br><span class="line"></span><br><span class="line"># 判断源表parquet_test1 和 parquet_test2是否数据量一致</span><br><span class="line"># 删除源表parquet_test1</span><br><span class="line">drop table parquet_test1;</span><br><span class="line"></span><br><span class="line">ALTER TABLE parquet_test2 RENAME TO parquet_test1</span><br></pre></td></tr></table></figure>

<p>Spark code:</p>
<p>思路：数据写入临时文件目录，判断临时文件目录大小设定重分区个数，数据写入正式文件目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.fs.&#123;FileSystem, Path&#125;</span><br><span class="line"></span><br><span class="line">    nowDF.persist()</span><br><span class="line">    val tempPath = &quot;/nativeinfopath/&quot;</span><br><span class="line">    nowDF.write.mode(&quot;overwrite&quot;).parquet(tempPath)</span><br><span class="line">    </span><br><span class="line">    val fs = FileSystem.get(sc.hadoopConfiguration)</span><br><span class="line">    val dirSize = fs.getContentSummary(new Path(tempPath)).getLength</span><br><span class="line">    val fileNum = dirSize / (128 * 1024 * 1024) </span><br><span class="line">    </span><br><span class="line">    val regularPath = &quot;&quot;</span><br><span class="line">    nowDF.coalesce(fileNum.toInt).write.mode(&quot;overwrite&quot;).parquet(regularPath)</span><br><span class="line">    nowDF.unpersist()</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<p>（2）使用hive  insert  overwrite  (自动小文件合并，不需要创建临时表，分区表分区也适用)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE parquet_test1 (value string) </span><br><span class="line">STORED AS PARQUET </span><br><span class="line">LOCATION &#x27;/tmp/test_out/parquet_big/&#x27;</span><br><span class="line">TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;SNAPPY&quot;)</span><br><span class="line">;</span><br><span class="line">set hive.execution.engine = mr;</span><br><span class="line">insert overwrite table parquet_test1 select * from parquet_test1;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--分区表</span><br><span class="line">day=`date  +&quot;%Y%m%d&quot; -d  &quot;-1 days&quot;`</span><br><span class="line">hive  -d  day=$&#123;day&#125;  -e  &quot;use gps; </span><br><span class="line">                           alter table gpsinfo  add partition (day=$&#123;day&#125;);</span><br><span class="line">                           insert overwrite table gpsinfo partition(day)  select * from gpsinfo where day=$&#123;day&#125;;&quot; </span><br></pre></td></tr></table></figure>

<p>(3)  hive 分片大小和自动小文件合并参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//每个Map最大输入大小(这个值决定了合并后文件的数量)</span><br><span class="line">set mapred.max.split.size=256000000;  </span><br><span class="line">//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span><br><span class="line">set mapred.min.split.size.per.node=100000000;</span><br><span class="line">//一个rack下split的至少的大小(这个值决定了rack上的文件是否需要合并)  </span><br><span class="line">set mapred.min.split.size.per.rack=100000000;</span><br><span class="line">//执行Map前进行小文件合并</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; </span><br><span class="line">//设置map端输出进行合并，默认为true</span><br><span class="line">set hive.merge.mapfiles = true</span><br><span class="line">//设置reduce端输出进行合并，默认为false</span><br><span class="line">set hive.merge.mapredfiles = true</span><br><span class="line">//设置合并文件的大小</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000</span><br><span class="line">//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</span><br><span class="line">set hive.merge.smallfiles.avgsize=16000000</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_file_combine/" data-id="cljllibc40001029a724i9b3s" data-title="spark_file_combine" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_minio" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_minio/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:29:29.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_minio/">spark_minio</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="Mino"><a href="#Mino" class="headerlink" title="Mino"></a>Mino</h3><p><a target="_blank" rel="noopener" href="https://min.io/docs/minio/macos/operations/install-deploy-manage/deploy-minio-single-node-single-drive.html#connect-to-the-minio-deployment">官方文档</a></p>
<p>本地安装Web控制台: <a target="_blank" rel="noopener" href="http://192.168.43.65:9011/">http://192.168.43.65:9011/</a></p>
<p>最小化安装</p>
<p>server start:  minio server –address “:9001”  –console-address :9011</p>
<p>client install and start: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 为搭建的s3 服务设置别名 local</span><br><span class="line">api: http://192.168.43.65:9001  </span><br><span class="line">access key: 4PtooCkozQcSIT5QLJFR  (创建的某个用户对应的key)</span><br><span class="line">secret key: 4PtooCkozQcSIT5QLJFR</span><br><span class="line"></span><br><span class="line">mc alias set local  http://192.168.43.65:9001 4PtooCkozQcSIT5QLJFR  4PtooCkozQcSIT5QLJFR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#查看文件列表</span><br><span class="line">mc ls   local/bucktest111</span><br><span class="line"></span><br><span class="line">#查看某个文件</span><br><span class="line">mc cat   local/bucktest111/tmp/pingan_art0629</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="Spark-读写minio"><a href="#Spark-读写minio" class="headerlink" title="Spark 读写minio"></a>Spark 读写minio</h4><p>Spark 3.4.1  </p>
<p>minio  version RELEASE.2023-06-23T20-26-00Z, 依赖版本一定要匹配，不然会读取出错 </p>
<p>（1） 使用minio jar包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;!-- https://mvnrepository.com/artifact/io.minio/spark-select --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;io.minio&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;spark-select_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-aws&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.3.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;com.google.guava&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;guava&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;23.6-jre&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>Test Code:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import com.amazonaws.SDKGlobalConfiguration</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object MinioExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">//    System.setProperty(SDKGlobalConfiguration.DISABLE_CERT_CHECKING_SYSTEM_PROPERTY, &quot;true&quot;)</span><br><span class="line"></span><br><span class="line">    lazy val spark = SparkSession.builder().appName(&quot;MinIOTest&quot;).master(&quot;local[*]&quot;).getOrCreate()</span><br><span class="line"></span><br><span class="line">    val s3accessKeyAws = &quot;4PtooCkozQcSIT5QLJFR&quot;</span><br><span class="line">    val s3secretKeyAws = &quot;4PtooCkozQcSIT5QLJFR&quot;</span><br><span class="line">    val connectionTimeOut = &quot;600000&quot;</span><br><span class="line">    val s3endPointLoc: String = &quot;http://192.168.43.65:9001&quot;</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.endpoint&quot;, s3endPointLoc)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.access.key&quot;, s3accessKeyAws)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.secret.key&quot;, s3secretKeyAws)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.connection.timeout&quot;, connectionTimeOut)</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;spark.sql.debug.maxToStringFields&quot;, &quot;100&quot;)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.path.style.access&quot;, &quot;true&quot;)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.impl&quot;, &quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot;)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.connection.ssl.enabled&quot;, &quot;false&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sourceBucket: String = &quot;bucktest111&quot;</span><br><span class="line">    val inputPath: String = s&quot;s3a://$sourceBucket/tmp/credentials2.json&quot;</span><br><span class="line">    val outputPath = s&quot;s3a://$sourceBucket/tmp/credentials4&quot;</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .read</span><br><span class="line">      //.format(&quot;minioSelectJSON&quot;) // minioSelectCSV  for csv or &quot;minioSelectJSON&quot; for JSON or &quot;minioSelectParquet&quot; for Parquet</span><br><span class="line">      .json(inputPath)</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">    df.write.mode(&quot;overwrite&quot;).json(outputPath)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（2）使用<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/cloud-integration.html">spark官方</a>提供的包，简单、高效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-hadoop-cloud_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_minio/" data-id="cljllibca0004029aamiueeex" data-title="spark_minio" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_connect" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_connect/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:28:37.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_connect/">spark_connect</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Spark-Connect"><a href="#Spark-Connect" class="headerlink" title="Spark Connect"></a>Spark Connect</h2><p>Env: spark3.4.1 hadoop2.7.7</p>
<h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><p>Server: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.4.1 --master yarn  --conf spark.sql.catalogImplementation=hive</span><br><span class="line"></span><br><span class="line">./sbin/stop-connect-server.sh </span><br></pre></td></tr></table></figure>

<p>Client：</p>
<p>1、Pyspark:</p>
<p>python最低版本3.9, 需要安装多个依赖包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/pyspark --remote <span class="string">&quot;sc://localhost&quot;</span></span><br></pre></td></tr></table></figure>

<p>2、maven Scala： 注：目前scala client不太稳定，还没正式对外release，使用方式来自stackoverflow</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-connect-client-jvm --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-connect-client-jvm_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-catalyst_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>Test code:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object SparkConnectTest &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val ip = &quot;192.168.43.65&quot;</span><br><span class="line">    val spark = SparkSession.builder().remote(s&quot;sc://$ip&quot;).build()</span><br><span class="line">//    spark.sql(&quot;show databases&quot;).show()</span><br><span class="line">//    spark.sql(&quot;use test_zhou&quot;)</span><br><span class="line">//    spark.sql(&quot;select addr, count(1) cnt from users group by addr&quot;).show()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val ds1 = spark.read.text(&quot;/tmp/output.txt&quot;)</span><br><span class="line">    val cnt = ds1.count()</span><br><span class="line"></span><br><span class="line">    println(cnt)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id=""><a href="#" class="headerlink" title=""></a></h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_connect/" data-id="cljllibbx0000029aavti3wcr" data-title="spark_connect" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-life/thought/20230629" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/30/life/thought/20230629/" class="article-date">
  <time class="dt-published" datetime="2023-06-30T11:58:06.000Z" itemprop="datePublished">2023-06-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/life-thought/">life/thought</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/30/life/thought/20230629/">20230629</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="One"><a href="#One" class="headerlink" title="One"></a>One</h1><p>What is important ?</p>
<h1 id="Two"><a href="#Two" class="headerlink" title="Two"></a>Two</h1><p>Time flies</p>
<h1 id="Three"><a href="#Three" class="headerlink" title="Three"></a>Three</h1><p>Do the right thing</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/06/30/life/thought/20230629/" data-id="cljiiyeep00029o9ah0lc6tm1" data-title="20230629" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-life/daily/20230630" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/30/life/daily/20230630/" class="article-date">
  <time class="dt-published" datetime="2023-06-30T09:39:44.000Z" itemprop="datePublished">2023-06-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/life-daily/">life/daily</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/30/life/daily/20230630/">20230630</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Morning"><a href="#Morning" class="headerlink" title="Morning"></a>Morning</h2><p>8:10 - 8:35</p>
<p>8:40 - 11:30</p>
<h2 id="loon"><a href="#loon" class="headerlink" title="loon"></a>loon</h2><p>11:30 - 13:30</p>
<h2 id="Afternoon"><a href="#Afternoon" class="headerlink" title="Afternoon"></a>Afternoon</h2><p>13:30 - 18:00<br>18:00 - 18:40</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/06/30/life/daily/20230630/" data-id="cljieq5zk0001bt9a4yle9w6t" data-title="20230630" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/">linux主机监控</a>
          </li>
        
          <li>
            <a href="/2023/07/03/tools/github/hexo/simple_usage/">simple_usage</a>
          </li>
        
          <li>
            <a href="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/">HBase概览</a>
          </li>
        
          <li>
            <a href="/2023/07/02/bigdata/spark/spark_file_read_partition_num/">spark_file_read_partition_num</a>
          </li>
        
          <li>
            <a href="/2023/07/02/bigdata/spark/spark_file_combine/">spark_file_combine</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>