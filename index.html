<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://qingfengzhou.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/hadoop/hadoop_env/hadoop本地环境搭建" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T02:13:53.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">hadoop本地环境搭建</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2><span id="env">Env</span></h2><p>macOS 12.4</p>
<p>java version “1.8.0_102”</p>
<p>Spark3.4.1  hadoop2.7.7</p>
<h2><span id="install-hadoop-277">Install hadoop 2.7.7</span></h2><p>&#x2F;Users&#x2F;zhouqingfeng&#x2F;Desktop&#x2F;software&#x2F;hadoop-2.7.7 </p>
<p>Site:<a target="_blank" rel="noopener" href="http://apache.communilink.net/hadoop/common/hadoop-2.7.7/">http://apache.communilink.net/hadoop/common/hadoop-2.7.7/</a></p>
<p>ref1:<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/de7eb61c983a">https://www.jianshu.com/p/de7eb61c983a</a></p>
<p>ref2:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/landed/p/6831758.html">https://www.cnblogs.com/landed/p/6831758.html</a></p>
<p>.&#x2F;sbin&#x2F;start-all.sh</p>
<p>Localhost:50070</p>
<p>Localhost:8088</p>
<h2><span id="install-mysql">Install mysql</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql-8.0.12-macos10.13-x86_64.dmg</span><br></pre></td></tr></table></figure>

<p>root: Zoom@123</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MYSQL_HOME=/usr/local/mysql</span><br><span class="line">export MYSQL_HOME</span><br><span class="line"></span><br><span class="line">PATH=.:$MYSQL_HOME/bin:$PATH</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure>

<p>mysql –version</p>
<p>mysql -uroot -p</p>
<h2><span id="install-hive">Install hive</span></h2><p>cd &#x2F;Users&#x2F;zhouqingfeng&#x2F;Desktop&#x2F;software&#x2F;apache-hive-2.3.3-bin</p>
<p>download mirror: <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/">https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/</a></p>
<p>#使用MySQL中的hive数据库，如果没有就创建一个<br>    <property><br>      <name>javax.jdo.option.ConnectionURL</name><br>      <value>jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true</value><br>      <description>JDBC connect string for a JDBC metastore</description><br>    </property></p>
<p>#连接myslq驱动<br>    <property><br>      <name>javax.jdo.option.ConnectionDriverName</name><br>      <value>com.mysql.jdbc.Driver</value><br>      <description>Driver class name for a JDBC metastore</description><br>    </property></p>
<p>#用户名<br>    <property><br>      <name>javax.jdo.option.ConnectionUserName</name><br>      <value>hive</value><br>      <description>username to use against metastore database</description><br>    </property></p>
<p>#密码:<br>    <property><br>      <name>javax.jdo.option.ConnectionPassword</name><br>      <value>hive</value><br>      <description>password to use against metastore database</description><br>    </property></p>
<p>#日志目录<br>    <property><br>      <name>hive.querylog.location</name><br>      <value>&#x2F;Users&#x2F;zhouqingfeng&#x2F;Desktop&#x2F;software&#x2F;tmp&#x2F;hive&#x2F;querylog</value><br>      <description>Location of Hive run time structured log file</description><br>    </property></p>
<pre><code>&lt;property&gt;
  &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
  &lt;value&gt;/Users/zhouqingfeng/Desktop/software/tmp/hive/scratchdir&lt;/value&gt;
  &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
  &lt;value&gt;/Users/zhouqingfeng/Desktop/software/tmp/hive/resoucesDir&lt;/value&gt;
  &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
  &lt;value&gt;/Users/zhouqingfeng/Desktop/software/tmp/hive/operationlog&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<hr>
<h3><span id="start">Start</span></h3><p>启动hive:</p>
<p>首先启动.&#x2F;hive –service metastore</p>
<p>hive client:</p>
<p>.&#x2F;hive</p>
<p>使用hiveserver2，然后使用beeline链接hive</p>
<p>.&#x2F;hive –service hiveserver2  |  hiveserver2   | lsof -i tcp:10000</p>
<p>beeline -u “jdbc:hive2:&#x2F;&#x2F;localhost:10000” </p>
<p>beeline -u “jdbc:hive2:&#x2F;&#x2F;192.168.43.65:10000”</p>
<p>hiveserver2 web url: <a target="_blank" rel="noopener" href="http://localhost:10002/">http://localhost:10002/</a></p>
<p>set hive.execution.engine &#x3D; mr;</p>
<p>beeline远程客户端配置：</p>
<p>修改conf&#x2F;hive-env.sh: HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.7.7</p>
<p>修改hadoop_home配置文件：&#x2F;opt&#x2F;hadoop-2.7.7&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh</p>
<h3><span id="references">References</span></h3><p><a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2016/01/600.htm">hiveserver2用户名和密码设置</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qiaojialin/article/details/55506439">https://blog.csdn.net/qiaojialin/article/details/55506439</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/micrari/p/7067968.html">https://www.cnblogs.com/micrari/p/7067968.html</a></p>
<h2><span id="install-hbase">Install  HBase</span></h2><h3><span id="pseudo-distributed-local-install">Pseudo-Distributed Local Install</span></h3><p>Hbase-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export  JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home </span><br></pre></td></tr></table></figure>

<p>Hbase-site.xml</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;hbase.rootdir&lt;/name&gt;
 &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>bin&#x2F;start-hbase.sh</p>
<p>bin&#x2F;stop-hbase.sh</p>
<p>Ref:<a target="_blank" rel="noopener" href="https://hbase.apache.org/book.html">https://hbase.apache.org/book.html</a></p>
<h3><span id="simple-usage">Simple Usage</span></h3><p>create  ‘test’, ‘cf1’</p>
<p>list  ‘test’  &#x2F;&#x2F;Use the <code>list</code> command to confirm your table exists</p>
<p>describe ‘test’</p>
<p>&#x2F;&#x2F;insert</p>
<p>put ‘test’,  ‘row1’,  ‘cf1:c1’,  ‘c1’</p>
<p>put ‘test’, ‘row1’, ‘cf1:a’, ‘value1’ </p>
<p>put ‘test’,  ‘row1’,   ‘cf1:c2’,  ‘c2’</p>
<p>put ‘test’,  ‘row2’,   ‘cf1:c3’,  ‘value1’</p>
<p>&#x2F;&#x2F;select</p>
<p>scan ‘test’</p>
<p>get ‘test’,  ‘row1’</p>
<p>get ‘test’,  ‘row1’,  ‘cf1’</p>
<p>get ‘test’,  ‘row1’,  ‘cf1:c1’</p>
<h3><span id="phoenix">phoenix</span></h3><h2><span id="install-zookeeper">Install Zookeeper</span></h2><p><a target="_blank" rel="noopener" href="http://apache.website-solution.net/zookeeper/">http://apache.website-solution.net/zookeeper/</a></p>
<h2><span id="install-kafka">Install Kafka</span></h2><h2><span id="install-spark">Install Spark</span></h2><p>Spark  local:</p>
<h3><span id="spark-yarn-deploy">Spark  yarn deploy</span></h3><h3><span id="spark-to-hbase">spark  to  hbase</span></h3><p>ETL</p>
<p>mysql  -&gt;  hive -&gt; spark sql  ?</p>
<p>machine learning  ?</p>
<p>spark + machine learning  ?</p>
<p>spark + AI ?</p>
<h2><span id="hive-to-or-from-hbase">Hive to (or from) Hbase</span></h2><h3><span id="hive-to-hbase">Hive to hbase</span></h3><p>create hive:</p>
<p>CREATE  TABLE tb2 (<br>name  string,<br>age int,<br>addr string<br>)<br>COMMENT ‘test table’</p>
<p>ROW FORMAT DELIMITED</p>
<p>FIELDS TERMINATED BY ‘,’</p>
<p>STORED AS textfile;</p>
<p>Load data to hive:</p>
<p>hive  -e  “LOAD DATA LOCAL INPATH ‘&#x2F;Users&#x2F;zhouqingfeng&#x2F;Desktop&#x2F;software&#x2F;tmp&#x2F;hive&#x2F;definedata&#x2F;tb2&#x2F;tb2.txt’ OVERWRITE INTO TABLE default.tb2” </p>
<p>create another hive table:</p>
<p>hbase: create  ‘test_tb2’,  ‘cf’</p>
<p>create external table tb2_cp (<br>name  string,<br>age int,<br>addr string<br>) </p>
<p>STORED BY ‘org.apache.hadoop.hive.hbase.HBaseStorageHandler’</p>
<p>WITH SERDEPROPERTIES (“hbase.columns.mapping” &#x3D; “:key,  cf:age,  cf:addr”)</p>
<p>TBLPROPERTIES (“hbase.table.name” &#x3D; “test_tb2”);</p>
<p>insert into table:  this is slow because of mapreduce job</p>
<p>insert into table  tb2_cp select * from tb2;  </p>
<h3><span id="reference-hive-to-hbase">Reference  </span></h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" data-id="cljnns25s0000sq9aca136t58" data-title="hadoop本地环境搭建" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_env/spark环境搭建" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T01:48:42.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdate-spark-spark-env/">bigdate/spark/spark_env</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">spark环境搭建</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#hive">Hive</a></li>
<li><a href="#install-spark">Install Spark</a><ul>
<li><a href="#spark-local">Spark local</a></li>
<li><a href="#spark-standalone">spark standalone</a></li>
</ul>
</li>
<li><a href="#spark-yarn-deploy">Spark yarn deploy</a><ul>
<li><a href="#spark-envsh">spark-env.sh</a></li>
<li><a href="#spark-defaultsconf">spark-defaults.conf</a></li>
</ul>
<ul>
<li><a href="#spark-connect-hive">Spark connect Hive</a></li>
<li><a href="#spark-to-hbase">spark to  hbase</a></li>
</ul>
</li>
<li><a href="#spark%E6%B5%8B%E8%AF%95">Spark测试</a><ul>
<li><a href="#-spark-local">–spark local</a></li>
<li><a href="#-spark-standalone">–spark standalone</a></li>
<li><a href="#-spark-yarn">–spark yarn</a></li>
<li><a href="#spark-thrift-sever">Spark Thrift Sever</a></li>
</ul>
</li>
<li><a href="#spark-debug">Spark Debug</a><ul>
<li><a href="#yarn">YARN</a><ul>
<li><a href="#driver">Driver</a></li>
<li><a href="#executor">Executor</a></li>
</ul>
</li>
<li><a href="#standalone">Standalone</a><ul>
<li><a href="#master">Master</a></li>
<li><a href="#worker">Worker</a></li>
</ul>
</li>
<li><a href="#problems">Problems</a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#sparksumit-parameters">SparkSumit Parameters</a><ul>
<li><a href="#-jars">–jars</a></li>
<li><a href="#-files">–files</a><ul>
<li><a href="#usage1">usage1</a></li>
<li><a href="#usage2">usage2</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-compile">Spark Compile</a><ul>
<li><a href="#spark2">Spark2</a></li>
<li><a href="#spark3">Spark3</a></li>
</ul>
</li>
<li><a href="#spark3-1">Spark3</a><ul>
<li><a href="#connect-hive">connect hive</a></li>
<li><a href="#spark2%E5%8D%87%E7%BA%A7%E5%88%B0spark3">spark2升级到spark3</a><ul>
<li><a href="#fasterxml%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98">fasterxml不兼容问题</a></li>
<li><a href="#codehaus%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98">codehaus报错问题</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-windows-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">Spark Windows 环境搭建</a></li>
<li><a href="#spark%E6%8F%90%E4%BA%A4%E6%9C%AC%E5%9C%B0jar%E5%8C%85%E5%88%B0%E8%BF%9C%E7%A8%8Bhadoop%E9%9B%86%E7%BE%A4">spark提交本地jar包到远程Hadoop集群</a><ul>
<li><a href="#linuxmac-remote-cluster">Linux+mac + remote cluster</a></li>
<li><a href="#windows-remote-kerberos-cluster">Windows + remote kerberos cluster</a></li>
</ul>
</li>
<li><a href="#spark-%E8%BF%9E%E6%8E%A5tdh-hive">Spark 连接tdh hive</a><ul>
<li><a href="#classnotfound">ClassnotFound</a></li>
<li><a href="#jdk%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98">jdk版本问题</a></li>
<li><a href="#kerberos">kerberos</a></li>
<li><a href="#%E6%97%B6%E9%92%9F%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98">时钟同步问题</a></li>
<li><a href="#%E6%97%B6%E5%8C%BA%E8%AE%BE%E7%BD%AE%E9%97%AE%E9%A2%98">时区设置问题</a></li>
<li><a href="#tdh-hive%E9%85%8D%E7%BD%AE%E5%B1%9E%E6%80%A7">tdh hive配置属性</a></li>
<li><a href="#client-%E7%AB%AF%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1-unknownhost-%E5%BC%82%E5%B8%B8">Client 端提交任务 unknownhost 异常</a></li>
<li><a href="#%E5%A4%96%E7%BD%91tdh60-%E5%8F%A6%E4%B8%80%E4%B8%AA%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95">外网tdh6.0 另一个集群测试</a></li>
<li><a href="#spark-%E8%BF%9E%E6%8E%A5-inceptor-%E7%9A%84%E6%96%B9%E6%B3%95">spark 连接 inceptor 的方法</a></li>
<li><a href="#%E6%96%87%E6%A1%A3%E6%89%8B%E5%86%8C">文档手册</a></li>
</ul>
</li>
<li><a href="#hive-on-spark">hive on spark</a></li>
<li><a href="#hive-partition">hive partition</a><ul>
<li><a href="#%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA">动态分区</a></li>
</ul>
</li>
<li><a href="#hive-hplsql">hive HPLSQL</a></li>
<li><a href="#others">Others</a><ul>
<li><a href="#spark-3-%E6%96%B0%E7%89%B9%E6%80%A7httpssparkapacheorgreleasesspark-release-3-0-0htmlspma2c6h128736390070a07c17h6eldk">Spark 3 新特性</a></li>
<li><a href="#spark-sql%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6">spark sql小文件合并</a></li>
</ul>
</li>
<li><a href="#references-1">References</a></li>
</ul>
<!-- tocstop -->

<h2><span id="hive">Hive</span></h2><p>nohup .&#x2F;bin&#x2F;hive –service metastore &amp;<br>nohup .&#x2F;bin&#x2F;hive –service hiveserver2 &amp;</p>
<h2><span id="install-spark">Install Spark</span></h2><h3><span id="spark-local">Spark  local</span></h3><h3><span id="spark-standalone">spark  standalone</span></h3><p>vi  $spark_home&#x2F;conf&#x2F;slaves</p>
<p>sh $spark_home&#x2F;sbin&#x2F;start-all.sh</p>
<h2><span id="spark-yarn-deploy">Spark  yarn deploy</span></h2><h4><span id="spark-envsh">spark-env.sh</span></h4><p>vi   $spark_home&#x2F;conf&#x2F;spark-env.sh</p>
<p>HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop<br>YARN_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop</p>
<p>core-site.xml</p>
<p>hive-site.xml</p>
<p> hdfs-site.xml </p>
<p> yarn-site.xml</p>
<p>export  HADOOP_USER_NAME&#x3D;hdfs<br>export  SPARK_HOME&#x3D;&#x2F;opt&#x2F;spark&#x2F;test&#x2F;spark-2.4.0-bin-hadoop2.7<br>HADOOP_CONF_DIR&#x3D;&#x2F;opt&#x2F;spark&#x2F;test&#x2F;spark-2.4.0-bin-hadoop2.7&#x2F;conf<br>YARN_CONF_DIR&#x3D;&#x2F;opt&#x2F;spark&#x2F;test&#x2F;spark-2.4.0-bin-hadoop2.7&#x2F;conf</p>
<p><strong>注意</strong></p>
<p>cp  mysql-connector-java-8.0.12.jar   $spark_home&#x2F;jars&#x2F;</p>
<h4><span id="spark-defaultsconf">spark-defaults.conf</span></h4><p>cp spark-defaults.conf.template spark-defaults.conf</p>
<p>vi $spark_home&#x2F;conf&#x2F;spark-defaults.conf</p>
<p>#上传jar包到指定hdfs位置</p>
<p>spark.yarn.jars&#x3D;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;spark&#x2F;jars&#x2F;jars&#x2F;*</p>
<p>spark.sql.shuffle.partitions.num 200 (修改默认属性值)</p>
<h3><span id="spark-connect-hive">Spark connect Hive</span></h3><p>Hive 1.2</p>
<p>spark-2.4.0-bin-hadoop2.7</p>
<p>cp  $hive_home&#x2F;conf&#x2F;hive-site.xml  $spark_home&#x2F;conf&#x2F;</p>
<p><strong>Error</strong>: Hive Schema version 1.2.0 does not match metastore’s schema version 2.3.0</p>
<p>修改hive-site.xml  hive.metastore.schema.verification 为false</p>
<h3><span id="spark-to-hbase">spark  to  hbase</span></h3><p>ETL</p>
<p>mysql  -&gt;  hive -&gt; spark sql  ?</p>
<p>machine learning  ?</p>
<p>spark + machine learning  ?</p>
<p>spark + AI ?</p>
<h2><span id="spark测试">Spark测试</span></h2><h3><span id="spark-local">–spark local</span></h3><p>.&#x2F;bin&#x2F;spark-submit –class org.apache.spark.examples.SparkPi <br>    –master local[2] <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g <br>    examples&#x2F;jars&#x2F;spark-examples*.jar <br>    10</p>
<p>.&#x2F;bin&#x2F;spark-shell <br>    –master local[2] <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g</p>
<p>val ds1 &#x3D; Seq(1,2,3).toDS<br>ds1.count</p>
<p>.&#x2F;bin&#x2F;spark-sql <br>    –master local[2] <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g</p>
<h3><span id="spark-standalone">–spark standalone</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  spark://Jon:7077 \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --total-executor-cores  2 \</span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-shell \</span><br><span class="line">    --master spark://Jon:7077 \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory  1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --total-executor-cores  2</span><br><span class="line"></span><br><span class="line">val ds1 = Seq(1,2,3).toDS</span><br><span class="line">ds1.count</span><br><span class="line"></span><br><span class="line">./bin/spark-shell \</span><br><span class="line">    --master spark://Jon:7077 \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --total-executor-cores  2</span><br></pre></td></tr></table></figure>



<h3><span id="spark-yarn">–spark yarn</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  2 \</span><br><span class="line">   --executor-cores 1</span><br><span class="line"></span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 4 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line"></span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 4 \</span><br><span class="line">   hdfs://localhost:9000/spark/tmp/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line">   </span><br><span class="line">   ./bin/spark-submit --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 2 \</span><br><span class="line">   --principal *** \</span><br><span class="line">   --keytab  ***   \</span><br><span class="line">   --files /opt/test/test.json </span><br><span class="line">   bdata-compute-spark-jdbc-***.jar \</span><br><span class="line">   test.json</span><br><span class="line">   </span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sh  test.sh  &gt; test.sh.log    2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./bin/spark-submit --class com.zhou.spark.sql.hive.TestHive  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test123  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">/Users/zhouqingfeng/Desktop/mydirect/github/distributedStudy/spark/sourcecode/projects/spark_all_test/target/spark_all_test-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/home/etl/bdata/env/spark_env/bin/spark-submit \</span><br><span class="line">    --class com.zhou.spark.sql.hive.TestHive  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test1234567  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 8g \</span><br><span class="line">    --executor-memory 6g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    --conf spark.sql.warehouse.dir=hdfs://10.20.29.103:8020/user/hive/warehouse   \</span><br><span class="line">    ./spark_all_test-jar-with-dependencies.jar \</span><br><span class="line">    thrift://node103:9083 nameservice1 namenode101  namenode161  node103:8020  node104:8020</span><br><span class="line">		</span><br><span class="line">1、读取hive大表不跨集群		</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">    --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test1234567  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 8g \</span><br><span class="line">    --executor-memory 6g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    --conf spark.sql.warehouse.dir=hdfs://10.20.30.112:8020/user/hive/warehouse   \</span><br><span class="line">    ./beta1192_bdata-compute-spark-jdbc-1.0.1.jar \</span><br><span class="line">    ***.json  thrift://bdp-1.rdc.com:9083,thrift://bdp-2.rdc.com:9083</span><br><span class="line">参数1: json文件</span><br><span class="line">参数2: thrift server地址，对应是hive-site.xml参数hive.metastore.uris</span><br><span class="line"></span><br><span class="line">2、读取hive大表跨集群：hive所在集群和spark计算所在集群是两个集群</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">    --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn   \</span><br><span class="line">    --name  test1234567  \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 8g \</span><br><span class="line">    --executor-memory 6g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --num-executors 8 \</span><br><span class="line">    --conf spark.sql.warehouse.dir=hdfs://10.20.29.103:8020/user/hive/warehouse   \</span><br><span class="line">    ./beta1192_bdata-compute-spark-jdbc-1.0.1.jar \</span><br><span class="line">    /home/etl/test/testHive.json  thrift://node103:9083 nameservice1 namenode101  namenode161  node103:8020  node104:8020</span><br><span class="line">		</span><br><span class="line">参数1: json文件</span><br><span class="line">参数2: thrift server地址，对应是hive-site.xml参数hive.metastore.uris		</span><br><span class="line">参数3: nameservice1  namespace名字，对应是core-site.xml参数fs.defaultFS的值hdfs://nameservice1</span><br><span class="line">参数4: namenode1，namenode1名字，对应是hdfs-site.xml参数dfs.ha.namenodes.nameservice1</span><br><span class="line">参数5: namenode2，namenode2名字，对应是hdfs-site.xml参数dfs.ha.namenodes.nameservice1</span><br><span class="line">参数6: namenode1地址，对应是hdfs-site.xml参数dfs.namenode.rpc-address.nameservice1.namenode1</span><br><span class="line">参数7: namenode2地址，对应是hdfs-site.xml参数dfs.namenode.rpc-address.nameservice1.namenode2</span><br></pre></td></tr></table></figure>

<p>.&#x2F;bin&#x2F;spark-shell <br>    –master yarn <br>    –deploy-mode client <br>    –driver-memory 4g <br>    –executor-memory 2g <br>    –executor-cores 1</p>
<p>val ds1 &#x3D; Seq(1,2,3).toDS<br>ds1.count</p>
<p>.&#x2F;bin&#x2F;spark-sql <br>    –master yarn <br>    –deploy-mode client <br>    –driver-memory 1g <br>    –executor-memory 1g <br>    –executor-cores 1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 --num-executors 2</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>





<h3><span id="spark-thrift-sever">Spark Thrift Sever</span></h3><p>.&#x2F;sbin&#x2F;start-thriftserver.sh <br>    –master yarn <br>    –deploy-mode client <br>    –driver-memory 512m <br>    –executor-memory 512m <br>    –executor-cores 1   –num-executors 3</p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;   -n hive </p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;10.26.119.144:10000&#x2F; </p>
<p>或者</p>
<p>.&#x2F;bin&#x2F;beeline<br>!connect jdbc:hive2:&#x2F;&#x2F;localhost:10000</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;10.26.119.144:10000</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;10.20.30.111:10000</p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;bdp-2:10000&#x2F; </p>
<p>&#x2F;&#x2F;开启kerberos</p>
<p>!connect  jdbc:hive2:&#x2F;&#x2F;10.20.30.111:10000&#x2F;test_zq;principal&#x3D;hive&#x2F;<a href="mailto:&#x62;&#x64;&#112;&#x32;&#64;&#x54;&#69;&#x53;&#84;&#x2e;&#x43;&#x4f;&#77;">&#x62;&#x64;&#112;&#x32;&#64;&#x54;&#69;&#x53;&#84;&#x2e;&#x43;&#x4f;&#77;</a>;authentication&#x3D;kerberos</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://bdp-3:10000/default</span><br></pre></td></tr></table></figure>

<p>用户可以远程连接thrift server, 提交spark sql</p>
<h2><span id="spark-debug">Spark Debug</span></h2><h3><span id="yarn">YARN</span></h3><h4><span id="driver">Driver</span></h4><p>spark.driver.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>（1）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   --driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5009 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br><span class="line">   </span><br><span class="line">   ./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   --driver-java-options &quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5009&quot; \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5009</p>
<p>(3) 加断点：</p>
<p>起始类：org.apache.spark.deploy.SparkSubmit</p>
<p>(4) debug idea remote!</p>
<h4><span id="executor">Executor</span></h4><p>spark.executor.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>（1）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  1 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   --conf &quot;spark.executor.extraJavaOptions=-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5010&quot; \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5010</p>
<p>(3) 加断点：</p>
<p>起始类：CoarseGrainedExecutorBackend.main( )</p>
<p>可以在ShuffleMapTask 和ResultTask 添加，executor启动后，执行这两类任务</p>
<p>(4) debug idea remote!</p>
<h3><span id="standalone">Standalone</span></h3><h4><span id="master">Master</span></h4><p>(1)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#调试Master，在master节点的spark-env.sh中添加SPARK_MASTER_OPTS变量</span><br><span class="line">export SPARK_MASTER_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10000&quot;</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;10000</p>
<p>(3) 加断点：</p>
<p>起始类：org.apache.spark.deploy.master.Master</p>
<p>(4) debug idea remote!</p>
<h4><span id="worker">Worker</span></h4><p>(1)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#调试Worker，在worker节点的spark-env.sh中添加SPARK_WORKER_OPTS变量</span><br><span class="line">export SPARK_WORKER_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10001&quot;</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;10001</p>
<p>(3) 加断点：</p>
<p>起始类：org.apache.spark.deploy.worker.Worker</p>
<p>(4) debug idea remote!</p>
<h3><span id="problems">Problems</span></h3><p>执行调试的源码一定要和对应的包(测试包&#x2F;测试脚本)一致</p>
<h3><span id="references">References</span></h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/breg/p/8427199.html">https://www.cnblogs.com/breg/p/8427199.html</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a69d46a6b923?tdsourcetag=s_pcqq_aiomsg">https://www.jianshu.com/p/a69d46a6b923?tdsourcetag=s_pcqq_aiomsg</a></p>
<p><a target="_blank" rel="noopener" href="https://medium.com/agile-lab-engineering/spark-remote-debugging-371a1a8c44a8">spark remote debugging</a></p>
<p><a target="_blank" rel="noopener" href="https://www.iteblog.com/archives/1192.html">过往记忆</a></p>
<h2><span id="sparksumit-parameters">SparkSumit Parameters</span></h2><h3><span id="jars">–jars</span></h3><p>Comma-separated list of jars to include on the driver<br>                              and executor classpaths.</p>
<p>多个依赖jar包 以, 分割，jar包路径设置为client端本地文件路径即可，最终所有依赖jar 会放置在driver端和executor端的classpath路径下。</p>
<h3><span id="files">–files</span></h3><p>Comma-separated list of files to be placed in the working<br>                              directory of each executor. File paths of these files<br>                              in executors can be accessed via SparkFiles.get(fileName).</p>
<p>多个依赖文件以,, 分割，jar包路径设置为client端本地文件路径，最终所有依赖文件 会放置在每个executor节点的临时文件路径下，executor可以通过SparkFiles.get(fileName) 读取到对应文件名字的依赖文件。（类似于广播变量，广播该文件到每一个执行节点，）</p>
<h4><span id="usage1">usage1</span></h4><p>每个Executor 节点获取文件，local | yarn client| yarn cluster 都支持：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jdbcDF.foreach(new ForeachFunction() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void call(Object row) throws Exception &#123;</span><br><span class="line">        System.out.println(row.toString());</span><br><span class="line"></span><br><span class="line">        FileReader fr = new FileReader(SparkFiles.get(fileName));</span><br><span class="line">        BufferedReader br = new BufferedReader(fr);</span><br><span class="line">        String line = br.readLine();</span><br><span class="line">        while(line != null) &#123;</span><br><span class="line">            System.out.println(line);</span><br><span class="line">            line = br.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h4><span id="usage2">usage2</span></h4><p>直接通过filename 获取文件，不能通过SparkFiles.get(fileName) 获取文件，</p>
<p><strong>只能用在yarn cluster模式</strong>，driver端直接通过filename 获取文件(driver节点也对应一个集群节点)，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class com.hundsun.rdc.bdata.compute.jdbc.TaskCommandRunner  \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">   --num-executors  5 \</span><br><span class="line">   --executor-cores 2 \</span><br><span class="line">   --principal *** \</span><br><span class="line">   --keytab  ***   \</span><br><span class="line">   --files /opt/test/test.json </span><br><span class="line">   bdata-compute-spark-jdbc-***.jar \</span><br><span class="line">   test.json</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">   //yarn cluster 模式，driver端直接通过filename:test.json 获取到对应文件 </span><br><span class="line">   //test.json被广播到所有executor节点上，</span><br><span class="line">   FileReader fr = new FileReader(&quot;test.json&quot;);</span><br><span class="line">   BufferedReader br = new BufferedReader(fr);</span><br><span class="line">   String line = br.readLine();</span><br><span class="line">   while(line != null) &#123;</span><br><span class="line">   System.out.println(line);</span><br><span class="line">   line = br.readLine();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>





<h2><span id="spark-compile">Spark Compile</span></h2><h3><span id="spark2">Spark2</span></h3><p>.&#x2F;dev&#x2F;make-distribution.sh –name spark-2.4.7-bin-hadoop2.7  –tgz  -Phadoop-2.7 -Phive -Phive-thriftserver -Pyarn</p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/2.4.7/building-spark.html">https://spark.apache.org/docs/2.4.7/building-spark.html</a></p>
<h3><span id="spark3">Spark3</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./dev/make-distribution.sh --name spark-3.0.1-bin-hadoop2.7.7  --tgz  -Phive -Phive-thriftserver  -Pyarn</span><br><span class="line"></span><br><span class="line">修改hive 版本1.2.2 编译失败</span><br></pre></td></tr></table></figure>



<h2><span id="spark3">Spark3</span></h2><h3><span id="connect-hive">connect hive</span></h3><p>spark包的 hive版本和线上Hive版本不一致，出现Invalid method name: ‘get_table_req’</p>
<p>(hive 版本需要 &gt; 2.3)</p>
<p>Solutions: </p>
<p>1、<a target="_blank" rel="noopener" href="https://github.com/apache/spark/pull/27161">https://github.com/apache/spark/pull/27161</a></p>
<p>add configuration: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.sql.hive.metastore.version=1.2.2 </span><br><span class="line">--conf spark.sql.hive.metastore.jars=/root/hive-1.2.2-lib/*</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shenyunsese/article/details/111299659">替换spark3 jars，未验证成功</a></p>
<p>2、重新编译Spark3，指定Hive版本，compile failed</p>
<h3><span id="spark2升级到spark3">spark2升级到spark3</span></h3><p>spark2升级到 Spark3 版本3.0.3，scala2.12.10，hadoop版本2.6.4</p>
<h4><span id="fasterxml不兼容问题">fasterxml不兼容问题</span></h4><p>Fasterxml.jackson.module 升级到jackson-module-scala_2.12:2.10.5</p>
<h4><span id="codehaus报错问题">codehaus报错问题</span></h4><p>添加dependency，org.codehaus.janino:janino:3.0.8  和 org.codehaus.janino:commons-compiler:3.0.8</p>
<h2><span id="spark-windows-环境搭建">Spark Windows 环境搭建</span></h2><p>hadoop 本地2.7 spark local模式，连接cdh6.3.2  hive（cdh开启了kerberos验证）</p>
<p>1、下载hadoop包，如hadoop-2.7.7.tar.gz</p>
<p>2、下载winutils.exe，放置在hadoop_home&#x2F;bin目录下</p>
<p>3、spark main class 添加环境变量 HADOOP_HOME&#x3D;D:\software\hadoop2.7;HADOOP_USER_NAME&#x3D;HDFS(或者对应其他用户)</p>
<p>4、copy core-site.xml hdfs-site.xml hive-site.xml yarn-site.xml 到项目的resources目录</p>
<p>注意：一定要查看是否生效，或者出于调试考虑，为了保障一定能生效，可以手动加载这几个配置文件，如下所示：</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“core-site.xml”))</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“hdfs-site.xml”))</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“hive-site.xml”))</p>
<p>sparkSession.sparkContext.hadoopConfiguration.addResource(new Path(“yarn-site.xml”))</p>
<p>5、Spark driver class：</p>
<p>由于sparksession 初始化的时候，会连接hive thrift server，获取Hive Catalog</p>
<p>同时由于hive 开启了kerberos验证，因此 sparksession 初始化前，需要先进行kerberos验证：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(&quot;java.security.krb5.conf&quot;,  &quot;&quot;)  //c:/users/zhouqf/cdh6/krb5.con</span><br><span class="line">val conf = new Configuration</span><br><span class="line">conf.set(&quot;fs.trash.interval&quot;, &quot;14400&quot;)  //不会更改hdfs实际参数</span><br><span class="line">conf.set(&quot;hadoop.security.authentication&quot;,  &quot;Kerberos&quot;)</span><br><span class="line">import org.apache.hadoop.security.UserGroupInformation  //(hadoop-common包)</span><br><span class="line">UserGroupInformation.setConfiguration(conf)</span><br><span class="line">//hive/bdp2@TEST.COM   c:/users/zhouqf/cdh6/hive.keytab </span><br><span class="line">UserGroupInformation.loginUserFromKeytab(&quot;principal&quot;,  &quot;keytabfile&quot;) </span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p>6、需要修改本地 windows机器上的文件权限，报错提示:&#x2F;tmp&#x2F;hive 没有权限</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/34196302/the-root-scratch-dir-tmp-hive-on-hdfs-should-be-writable-current-permissions">The root scratch dir: &#x2F;tmp&#x2F;hive on HDFS should be writable. Current permissions are: rw-rw-rw- (on Windows)</a></p>
<p>假如winutils.exe 所在目录为D:\winutils\bin\winutils.exe，</p>
<p>在windows 上可登陆powershell 执行命令，cd  \winutils\bin,  然后.\winutils.exe  chmod 777 D:\tmp\hive</p>
<p>spark中的三种参数配置：</p>
<p>spark自身相关，如spark.sql.warehouse.dir</p>
<p>hadoop相关：–conf spark.hadoop.abc.def&#x3D;xyz   represents adding hadoop property “abc.def&#x3D;xyz”,</p>
<p>Hive 相关：–conf spark.hive.abc&#x3D;xyz  represents adding hive property “hive.abc&#x3D;xyz”.</p>
<h2><span id="spark提交本地jar包到远程hadoop集群">spark提交本地jar包到远程Hadoop集群</span></h2><h3><span id="linuxmac-remote-cluster">Linux+mac + remote cluster</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.launcher.SparkAppHandle;</span><br><span class="line">import org.apache.spark.launcher.SparkLauncher;</span><br><span class="line"></span><br><span class="line">import java.util.HashMap;</span><br><span class="line"></span><br><span class="line">public class TestSparkLauncher &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; envParams = new HashMap&lt;&gt;();</span><br><span class="line">//        envParams.put(&quot;YARN_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">//        envParams.put(&quot;HADOOP_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_HOME&quot;, &quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;, &quot;1&quot;);</span><br><span class="line">//        envParams.put(&quot;JAVA_HOME&quot;, &quot;/usr/java&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SparkLauncher launcher = new SparkLauncher(envParams)</span><br><span class="line">                .setAppResource(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;)</span><br><span class="line">                .setMainClass(&quot;org.apache.spark.examples.SparkPi&quot;)</span><br><span class="line">                .setMaster(&quot;yarn&quot;)</span><br><span class="line">                .setDeployMode(&quot;client&quot;)</span><br><span class="line">                .addAppArgs(&quot;20&quot;);</span><br><span class="line"></span><br><span class="line">        String appName = &quot;sparklancherTest&quot;;</span><br><span class="line">        launcher.setConf(&quot;spark.executor.memory&quot;, &quot;1g&quot;);</span><br><span class="line">        launcher.setAppName( appName );</span><br><span class="line">        SparkAppHandle sparkHandle = launcher.startApplication();</span><br><span class="line"></span><br><span class="line">        while(!sparkHandle.getState().isFinal()) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(1000);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="windows-remote-kerberos-cluster">Windows + remote kerberos cluster</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public class TestSparkLauncher &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; envParams = new HashMap&lt;&gt;();</span><br><span class="line">        envParams.put(&quot;YARN_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">//        envParams.put(&quot;HADOOP_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_HOME&quot;, &quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;, &quot;1&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_SUBMIT_OPTS&quot;, &quot;-Djava.security.krb5.conf=D:/projects/spark-compute/conf/krb5.conf&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SparkLauncher launcher = new SparkLauncher(envParams)</span><br><span class="line">                .setAppResource(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;)</span><br><span class="line">                .setMainClass(&quot;org.apache.spark.examples.SparkPi&quot;)</span><br><span class="line">                .setMaster(&quot;yarn&quot;)</span><br><span class="line">                .setDeployMode(&quot;client&quot;)</span><br><span class="line">                .addAppArgs(&quot;20&quot;);</span><br><span class="line">        launcher.addSparkArg(&quot;--keytab&quot;, &quot;D:/projects/spark-compute/conf/hive.keytab&quot;);</span><br><span class="line">        launcher.addSparkArg(&quot;--principal&quot;, &quot;hive@TEST.COM&quot;);        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String appName = &quot;sparklancherTest&quot;;</span><br><span class="line">        launcher.setConf(&quot;spark.executor.memory&quot;, &quot;1g&quot;);</span><br><span class="line">        launcher.setAppName( appName );</span><br><span class="line">        SparkAppHandle sparkHandle = launcher.startApplication();</span><br><span class="line"></span><br><span class="line">        while(!sparkHandle.getState().isFinal()) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(1000);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="spark-连接tdh-hive">Spark 连接tdh hive</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">tdh版本6.2</span><br><span class="line">本地测试需要在resouces添加hive-site.xml、core-site.xml、hdfs-site.xml</span><br><span class="line">本地连接时，core-site.xml 放置在本地，也需要修改属性 为本地目录hadoop.security.group.mapping.ldap.bind.password.file=/etc/hdfs1/conf/ldap-conn-pass.txt</span><br><span class="line"></span><br><span class="line">10.20.30.50  root   BData.COM </span><br><span class="line">spark_home:  /opt/bdata/env/spark_env</span><br><span class="line"></span><br><span class="line">./bin/spark-sql  \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client  \</span><br><span class="line">--executor-memory 1G  \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">--executor-cores 1  \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--conf spark.yarn.appMasterEnv.JAVA_HOME=&quot;/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--conf &quot;spark.executorEnv.JAVA_HOME=/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--hiveconf hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider \</span><br><span class="line">--principal hive@TDH  \</span><br><span class="line">--keytab /home/tdh/hive.keytab</span><br><span class="line"></span><br><span class="line">//spark-submit</span><br><span class="line"></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">--class com.**.TaskRunner \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client  \</span><br><span class="line">--executor-memory 1G  \</span><br><span class="line">--total-executor-cores 1 \</span><br><span class="line">--executor-cores 1  \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--conf spark.yarn.appMasterEnv.JAVA_HOME=&quot;/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--conf &quot;spark.executorEnv.JAVA_HOME=/usr/java/jdk1.8.0_241&quot; \</span><br><span class="line">--conf spark.hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider \</span><br><span class="line">--principal hive@TDH  \</span><br><span class="line">--keytab /home/tdh/hive.keytab</span><br></pre></td></tr></table></figure>

<h3><span id="classnotfound">ClassnotFound</span></h3><p>跳过tdh guardian 权限管理，</p>
<p>配置hive 参数hive.security.authorization.manager&#x3D;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</p>
<h3><span id="jdk版本问题">jdk版本问题</span></h3><p>Tdh6.2 需要指定JDK版本 &gt;&#x3D; 1.8.0_241</p>
<p>–conf spark.yarn.appMasterEnv.JAVA_HOME&#x3D;”&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_241” <br>–conf “spark.executorEnv.JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_241” \</p>
<h3><span id="kerberos">kerberos</span></h3><p>如果需要指定krb5.conf, 配置   –conf spark.driver.extraJavaOptions&#x3D;-Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;tdh-test&#x2F;krb5.conf </p>
<h3><span id="时钟同步问题">时钟同步问题</span></h3><p>如下表示同步当前server时间与192.168.60.14服务器一致</p>
<p>cd &#x2F;bin&#x2F;<br>systemctl stop ntpd.service<br>ntpdate  192.168.60.14<br>systemctl start  ntpd.service<br>systemctl status  ntpd.service  </p>
<h3><span id="时区设置问题">时区设置问题</span></h3><p>mv &#x2F;etc&#x2F;localtime &#x2F;etc&#x2F;localtime.bak<br>ln -s &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai  &#x2F;etc&#x2F;localtime </p>
<p>删除软连接： unlink   &#x2F;etc&#x2F;localtime</p>
<h3><span id="tdh-hive配置属性">tdh hive配置属性</span></h3><p>定义hive conf property,覆盖hive-site.xml属性</p>
<p>Spark-submit:</p>
<p>–conf spark.hive.security.authorization.manager&#x3D;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</p>
<p>Spark-sql:</p>
<p>–hiveconf hive.security.authorization.manager&#x3D;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider</p>
<h3><span id="client-端提交任务-unknownhost-异常">Client 端提交任务 unknownhost 异常</span></h3><p>Spark-submit 提交yarn，注册生成applicationmaster之后，去跟driver(client模式，driver位于client端)交互，但是总是无法识别driver对应Hostname，查看centos  &#x2F;etc&#x2F;hosts是配置过hostname的，而且ping 命令是通的，好久思考之后，发现问题还是因为driver端&#x2F;etc&#x2F;hosts配置的问题引起的，修改之后，问题就解决了。</p>
<p><strong>错误写法</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1  localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">10.20.29.125  mysql-125</span><br></pre></td></tr></table></figure>

<p><strong>正确写法</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1  mysql-125  localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="外网tdh60-另一个集群测试">外网tdh6.0 另一个集群测试</span></h3><p>使用spark-submit 测试：</p>
<p>Spark-submit 命令 </p>
<p>（1）修改hive-site.xml属性 hive.security.authorization.manager</p>
<p>–conf spark.hadoop.hive.security.authorization.manager&#x3D;”org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider”</p>
<p>这样写才能生效，使用以下写法报错了，似乎不行。</p>
<p>–conf spark.hive.security.authorization.manager&#x3D;”org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider”</p>
<p>（2）</p>
<p>copy  core-site.xml对应属性文件到client 对应目录，要与core-site.xml中的目录一致<br>–conf  net.topology.script.file.name&#x3D;&#x2F;usr&#x2F;lib&#x2F;transwarp&#x2F;scripts&#x2F;rack_map.sh<br>–conf  hadoop.security.group.mapping.ldap.bind.password.file&#x3D;&#x2F;etc&#x2F;hdfs1&#x2F;conf&#x2F;ldap-conn-pass.txt</p>
<h3><span id="spark-连接-inceptor-的方法">spark 连接 inceptor 的方法</span></h3><p>官方文档：</p>
<p><a target="_blank" rel="noopener" href="https://nj.transwarp.cn:8180/?p=3382">https://nj.transwarp.cn:8180/?p=3382</a></p>
<h3><span id="文档手册">文档手册</span></h3><p><a target="_blank" rel="noopener" href="http://support.transwarp.cn/t/topic/3262">http://support.transwarp.cn/t/topic/3262</a></p>
<h2><span id="hive-on-spark">hive on spark</span></h2><p>spark2.4</p>
<p>1、spark源码编译编译： .&#x2F;dev&#x2F;make-distribution.sh –name “hadoop2-without-hive” –tgz “-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided,orc-provided” （必须拥有<strong>不</strong>包含Hive jar 的Spark版本 。Spark的发行版本为了兼顾Spark SQL都会包含有Hive相关的jar,所以我们需要通过源码重新编译,去重相关的jar.）</p>
<p>2、配置Hive</p>
<p>cp scala-library-2.11.8.jar   $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>cp spark-core_2.11-2.0.2.jar  $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>cp spark-network-common_2.11-2.0.2.jar  $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>cp spark-unsafe_2.11-2.4.7.jar   $HIVE_HOME&#x2F;lib&#x2F;</p>
<p>hive-site.xml  修改配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.execution.engine&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;spark&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;spark.yarn.jars&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://localhost:9000/spark/jars/jars2/*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、hive on spark 任务参数：</p>
<p>set hive.execution.engine&#x3D;spark;  （      Expects one of [mr, tez, spark]    ）</p>
<p>set spark.executor.memory&#x3D;6g;<br>set spark.executor.cores&#x3D;3;<br>set spark.executor.instances&#x3D;40;</p>
<p>set spark.master&#x3D;yarn-client; </p>
<p>set spark.serializer&#x3D;org.apache.spark.serializer.KryoSerializer;</p>
<p>4、任务详细日志查看</p>
<p>hive  –hiveconf  hive.root.logger&#x3D;DEBUG,console  -e  “select count(1) from   test_db1.test_tb1”</p>
<p>5、references:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/pucao_cug/article/details/72783688">https://blog.csdn.net/pucao_cug/article/details/72783688</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/339da2b6d480">https://www.jianshu.com/p/339da2b6d480</a></p>
<h2><span id="hive-partition">hive partition</span></h2><h3><span id="动态分区">动态分区</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">create table test_part2(id int, name string) </span><br><span class="line">partitioned by (country string, province string );</span><br><span class="line"></span><br><span class="line">--静态分区 指定分区字段的值</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;sh&#x27;)  select id, name from test_part1 where id = 1 and name = &#x27;lisa&#x27;;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;)  values(2, &#x27;dal&#x27;)</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;) select 2 as id, &#x27;cate&#x27; as name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--半动态分区(一部分静态分区 + 一部分动态分区)</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--全动态分区</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-sql \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;tez&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-shell   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;insert into table test_zhou.test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;newyork&#x27; as province&quot;)</span><br></pre></td></tr></table></figure>



<h2><span id="hive-hplsql">hive HPLSQL</span></h2><p>1、配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive-site.xml</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;m1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>





<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">hplsql-site.xml</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.default&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hive2conn&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The default connection profile&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.hiveconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.jdbc.HiveDriver;jdbc:hive://&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Hive embedded JDBC (not requiring HiveServer)&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 配置项hive.execution.engine默认设置为mr,若使用spark作为引擎时,则设置为spark --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.init.hiveconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;</span><br><span class="line">     set mapred.job.queue.name=default;</span><br><span class="line">     set hive.execution.engine=spark; </span><br><span class="line">     use default;</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Statements for execute after connection to the database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.convert.hiveconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Convert SQL statements before execution&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.hive2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hive.jdbc.HiveDriver;jdbc:hive2://localhost:10000;zhouqingfeng;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;HiveServer2 JDBC connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 配置项hive.execution.engine默认设置为mr,若使用spark作为引擎时,则设置为spark --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.init.hive2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;</span><br><span class="line">     set mapred.job.queue.name=default;</span><br><span class="line">     set hive.execution.engine=spark; </span><br><span class="line">     use default;</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Statements for execute after connection to the database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.convert.hive2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Convert SQL statements before execution&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.db2conn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.ibm.db2.jcc.DB2Driver;jdbc:db2://localhost:50001/dbname;user;password&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;IBM DB2 connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.tdconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.teradata.jdbc.TeraDriver;jdbc:teradata://localhost/database=dbname,logmech=ldap;user;password&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Teradata connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.conn.mysqlconn&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver;jdbc:mysql://localhost/test;user;password&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;MySQL connection&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.dual.table&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;default.dual&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Single row, single column table for internal operations&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.insert.values&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;native&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;How to execute INSERT VALUES statement: native (default) and select&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.onerror&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;exception&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Error handling behavior: exception (default), seterror and stop&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.temp.tables&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;native&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Temporary tables: native (default) and managed&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.temp.tables.schema&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Schema for managed temporary tables&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hplsql.temp.tables.location&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/zhouqingfeng/tmp&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;LOcation for managed temporary tables in HDFS&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 下面两项需要按实际情况修改 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">&lt;value&gt;localhost&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2、启动HiveServer2和Metastore服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh <span class="variable">$HIVE_HOME</span>/bin/hive  --service  metastore</span><br><span class="line">sh <span class="variable">$HIVE_HOME</span>/bin/hive  --service  hiveserver2</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、test:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/hplsql -e &quot;CURRENT_DATE+1&quot; </span><br><span class="line">./bin/hplsql  -e &#x27;select count(1) from test_db1.test_tb2&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>4、reference:</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/4d2035377753">https://www.jianshu.com/p/4d2035377753</a></p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/37831928/apache-hplsql-permission-denied-while-running-hplsql-query">https://stackoverflow.com/questions/37831928/apache-hplsql-permission-denied-while-running-hplsql-query</a></p>
<p><a target="_blank" rel="noopener" href="http://www.hplsql.org/if">http://www.hplsql.org/if</a></p>
<p>5、usage:</p>
<p><strong>define a function:</strong></p>
<p>cat   test_sql&#x2F;test_func1.sql</p>
<p>CREATE FUNCTION hello(text STRING)<br> RETURNS STRING<br>BEGIN<br> RETURN ‘Hello, ‘ || text || ‘!’;<br>END;</p>
<p>– Invoke the function<br>PRINT hello(‘world’);</p>
<p>hplsql -f  test_sql&#x2F;test_func1.sql</p>
<p><strong>define a <a target="_blank" rel="noopener" href="http://www.hplsql.org/udf-sproc">Procedures</a></strong> :</p>
<p>cat   test_sql&#x2F;test_proc1.sql</p>
<p>CREATE PROCEDURE set_message(IN name STRING, OUT result STRING)<br>BEGIN<br> SET result &#x3D; ‘Hello, ‘ || name || ‘!’;<br>END;</p>
<p>– Call the procedure and print the results<br>DECLARE str STRING;<br>CALL set_message(‘Jack’, str);<br>PRINT str;</p>
<p>hplsql -f  test_sql&#x2F;test_proc1.sql </p>
<p><strong>EXECUTE Statement</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DECLARE cnt INT;</span><br><span class="line">EXECUTE &#x27;SELECT COUNT(1) FROM test_db1.test_tb1&#x27; INTO cnt;</span><br><span class="line">PRINT cnt</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>hplsql  -e  “DECLARE cnt INT;EXECUTE ‘SELECT COUNT(1) FROM test_db1.test_tb1’ INTO cnt;PRINT cnt”</p>
<h2><span id="others">Others</span></h2><h3><span id="spark-3-新特性"></span></h3><p><strong>1、<a target="_blank" rel="noopener" href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html?spm=a2c6h.12873639.0.0.70a07c17h6ELDK">自适应查询执行（Adaptive Query Execution）</a></strong></p>
<p>spark.sql.adaptive.enabled &#x3D; true</p>
<p> As soon as one or more of these stages finish materialization, the framework marks them complete in the physical query plan and updates the logical query plan accordingly, with the runtime statistics retrieved from completed stages. Based on these new statistics, the framework then runs the optimizer (with a selected list of logical optimization rules), the physical planner, as well as the physical optimization rules, which include the regular physical rules and the adaptive-execution-specific rules, such as coalescing partitions, skew join handling, etc.</p>
<p>In Spark 3.0, the AQE framework is shipped with three features:</p>
<ul>
<li>Dynamically coalescing shuffle partitions</li>
<li>Dynamically switching join strategies</li>
<li>Dynamically optimizing skew joins</li>
</ul>
<p>允许spark 在运行过程中 根据已经执行完的stage的统计信息，动态优化调整逻辑执行计划和物理执行计划，主要包括以下三个方面的优化：</p>
<p>动态合并shuffle partition的数目；</p>
<p>动态调整 join 策略，sortmergejoin -》 broadcasthashjoin</p>
<p>动态优化产生倾斜的Join</p>
<p><strong>2、动态分区修剪（Dynamic Partition Pruning）</strong></p>
<p>spark.sql.optimizer.dynamicPartitionPruning.enabled </p>
<p>所谓的动态分区裁剪就是基于运行时（run time）推断出来的信息来进一步进行分区裁剪</p>
<p>3、对深度学习的增强，包括支持GPU计算等</p>
<p>4、对k8s更好的整合</p>
<p>5、数据湖delta lake更好的整合</p>
<p>6、For the Scala API, Spark 3.0.1 uses Scala 2.12. Python 2 and Python 3 prior to version 3.6 support is deprecated as of Spark 3.0.0.</p>
<h3><span id="spark-sql小文件合并">spark sql小文件合并</span></h3><p>(1) 对于原始数据进行按照分区字段进行shuffle，可以规避小文件问题。但有可能引入数据倾斜的问题；</p>
<p>(2) sql中引入 distribute by ，指定分区字段或分区表达式</p>
<p>(3) 已知倾斜key的情况，将数据分为两部分处理，倾斜部分按rand()函数 重分区，未倾斜部分常规处理</p>
<p>(4) 对于Spark 2.4 以上版本的用户，sql中 可以使用<a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/SPARK-24940">HINT提示</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert into select /*+ REPARTITION(2) */  id,name  from tb1 where id &gt; 0</span><br><span class="line">insert into select /*+ COALESCE(2) */  id,name  from tb1 where id &gt; 0</span><br></pre></td></tr></table></figure>

<p>(5) spark3.0以上开启自适应查询执行：</p>
<p>spark.sql.adaptive.enabled&#x3D;true;</p>
<p>spark.sql.adaptive.coalescePartitions.enabled&#x3D;true;</p>
<p>spark.sql.adaptive.coalescePartitions.parallelismFirst &#x3D; false;</p>
<p>spark.sql.adaptive.advisoryPartitionSizeInBytes &#x3D; 64m;</p>
<h2><span id="references">References</span></h2><p><a href>waterdrop 如何更好的使用spark</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/apache/incubator-seatunnel">apache seatunnel</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" data-id="cljnnbsuy0000gl9a87717mmh" data-title="spark环境搭建" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-tools/draw/在线画图/processon" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T01:30:56.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/">processon</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2><span id="工具">工具</span></h2><p><a target="_blank" rel="noopener" href="https://www.processon.com/">免费在线流程图思维导图</a>:<a target="_blank" rel="noopener" href="https://www.processon.com/">https://www.processon.com/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/" data-id="cljnm9mja0000jg9a8gxe7mpu" data-title="processon" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/prometheus/prometheus使用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T17:43:13.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/">prometheus使用</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Grafana:  </p>
<p>bin&#x2F;grafana-server</p>
<p><a target="_blank" rel="noopener" href="http://localhost:3000/">http://localhost:3000/</a></p>
<p>Node_exporter:</p>
<p>curl  <a target="_blank" rel="noopener" href="http://localhost:9100/metrics">http://localhost:9100/metrics</a></p>
<p>prometheus:</p>
<p>.&#x2F;prometheus –config.file&#x3D;prometheus.yml</p>
<p><a target="_blank" rel="noopener" href="http://localhost:9090/">http://localhost:9090</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/" data-id="cljn5iqgg00007j9a6mcm3wwz" data-title="prometheus使用" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/ansible/ansible使用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T15:58:16.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-ansible/">system/linux/ansible</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/">ansible使用</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">环境搭建</a><ul>
<li><a href="#%E5%9F%BA%E4%BA%8Ealpine-%E9%95%9C%E5%83%8F%E6%90%AD%E5%BB%BA">基于alpine 镜像搭建</a><ul>
<li><a href="#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4">安装步骤</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E4%BA%8Ecentos-7-%E9%95%9C%E5%83%8F%E6%90%AD%E5%BB%BA">基于centos 7 镜像搭建</a><ul>
<li><a href="#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4-1">安装步骤</a></li>
</ul>
</li>
<li><a href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%A4%9A%E7%BB%93%E7%82%B9">虚拟机多结点</a><ul>
<li><a href="#virturebox%E7%BD%91%E5%8D%A1%E9%85%8D%E7%BD%AE">virturebox网卡配置</a></li>
<li><a href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE">虚拟机网络配置</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99-1">参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8">入门使用</a><ul>
<li><a href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">配置文件</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9D%97">常用模块</a><ul>
<li><a href="#shell-%E6%A8%A1%E5%9D%97">shell 模块</a></li>
<li><a href="#script-%E6%A8%A1%E5%9D%97">script 模块</a></li>
</ul>
</li>
<li><a href="#playbook">Playbook</a><ul>
<li><a href="#%E6%89%A7%E8%A1%8Cshell">执行shell</a></li>
<li><a href="#%E6%89%A7%E8%A1%8Cscript">执行script</a></li>
</ul>
</li>
<li><a href="#become_user%E7%94%A8%E6%B3%95">Become_user用法</a><ul>
<li><a href="#%E7%94%A8%E6%B3%95">用法</a></li>
<li><a href="#%E6%80%BB%E4%BD%93%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B">总体操作流程</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="环境搭建">环境搭建</span></h2><h3><span id="基于alpine-镜像搭建">基于alpine 镜像搭建</span></h3><p>alpine:3.17.3 版本，启动多个docker容器，模拟练习使用ansible</p>
<h4><span id="安装步骤">安装步骤</span></h4><p>1、按如下目录结构创建文件</p>
<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.08.18.png">

<p>&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[defaults]</span><br><span class="line">; 是否启用主机密钥检查: https://docs.ansible.com/ansible/latest/inventory_guide/connection_details.html#managing-host-key-checking</span><br><span class="line">; 初期使用密码连接分发公钥时先设置为False</span><br><span class="line">; 对应的环境变量设置: export ANSIBLE_HOST_KEY_CHECKING=False</span><br><span class="line">host_key_checking = False</span><br><span class="line">interpreter_python = /usr/bin/python</span><br></pre></td></tr></table></figure>

<p>&#x2F;etc&#x2F;ansible&#x2F;hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[myvirtualmachines]</span><br><span class="line">192.0.2.50</span><br><span class="line">192.0.2.51</span><br><span class="line">192.0.2.52</span><br></pre></td></tr></table></figure>

<p>Dockerfile:  构造生成 example-ansible-master 容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM example-ansible-node:latest</span><br><span class="line"></span><br><span class="line">RUN apk add --no-cache ansible openssh sshpass</span><br><span class="line"></span><br><span class="line">RUN mkdir -p /etc/ansible &amp;&amp; \</span><br><span class="line">    ssh-keygen -t rsa -P &quot;&quot; -f ~/.ssh/id_rsa</span><br></pre></td></tr></table></figure>

<p>Dockerfile.node：生成 镜像example-ansible-node</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:3.17.3</span><br><span class="line"></span><br><span class="line">RUN echo &quot;http://mirrors.aliyun.com/alpine/latest-stable/main/&quot; &gt; /etc/apk/repositories &amp;&amp; \</span><br><span class="line">    echo &quot;http://mirrors.aliyun.com/alpine/latest-stable/community/&quot; &gt;&gt; /etc/apk/repositories</span><br><span class="line"></span><br><span class="line">RUN apk update &amp;&amp; \</span><br><span class="line">    apk add --no-cache openssh-server tzdata python3 &amp;&amp; \</span><br><span class="line">    cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \</span><br><span class="line">    sed -i &quot;s/#PermitRootLogin.*/PermitRootLogin yes/g&quot; /etc/ssh/sshd_config &amp;&amp; \</span><br><span class="line">    ssh-keygen -A &amp;&amp; \</span><br><span class="line">    echo &quot;root:123456&quot; | chpasswd</span><br><span class="line"></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line">CMD [&quot;/usr/sbin/sshd&quot;, &quot;-D&quot;]</span><br></pre></td></tr></table></figure>



<p>docker-compose.yml：启动四个容器：</p>
<p>example-ansible-master，example-ansible-node1，example-ansible-node2，example-ansible-node3</p>
<p>master安装ansible，运行ansible命令，控制操作其他三个结点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  net-ansible:</span><br><span class="line">    ipam:</span><br><span class="line">      driver: default</span><br><span class="line">      config:</span><br><span class="line">        - subnet: 192.0.2.0/24</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  master:</span><br><span class="line">    build:</span><br><span class="line">      dockerfile: Dockerfile</span><br><span class="line">      context: .</span><br><span class="line">    container_name: example-ansible-master</span><br><span class="line">    hostname: ansible</span><br><span class="line">    volumes:</span><br><span class="line">      - ./etc/ansible:/etc/ansible</span><br><span class="line">      - ./ansible:/server/scripts/ansible</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.49</span><br><span class="line"></span><br><span class="line">  node1:</span><br><span class="line">    image: example-ansible-node</span><br><span class="line">    hostname: host1</span><br><span class="line">    container_name: example-ansible-node1</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.50</span><br><span class="line">  node2:</span><br><span class="line">    image: example-ansible-node</span><br><span class="line">    hostname: host2</span><br><span class="line">    container_name: example-ansible-node2</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.51</span><br><span class="line">  node3:</span><br><span class="line">    image: example-ansible-node</span><br><span class="line">    hostname: host3</span><br><span class="line">    container_name: example-ansible-node3</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.52</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>2、启动容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 构建host1、host2、host3的镜像</span><br><span class="line">docker build -t example-ansible-node -f ./Dockerfile.node .</span><br><span class="line"></span><br><span class="line"># 启动四个容器</span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、登录ansible容器，分发ssh公钥</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#进入ansible容器 (master)</span><br><span class="line">docker exec -it example-ansible-master ash</span><br><span class="line"></span><br><span class="line"># 执行下面命令分发ssh公钥</span><br><span class="line">ansible myvirtualmachines -k -m authorized_key -a &quot;user=root key=&#x27;&#123;&#123; lookup(&#x27;file&#x27;, &#x27;~/.ssh/id_rsa.pub&#x27;) &#125;&#125;&#x27;&quot;</span><br><span class="line">输入密码123456</span><br></pre></td></tr></table></figure>

<p>4、测试连通性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">; 测试连接</span><br><span class="line">ansible all -m ping</span><br><span class="line">ansible myvirtualmachines -m ping</span><br></pre></td></tr></table></figure>

<h4><span id="参考资料">参考资料</span></h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624172594">用Docker搭建Ansible练习环境</a></p>
<p>如何使用容器模拟物理机学习使用ansible?<br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/goloving/p/15032636.html">https://www.cnblogs.com/goloving/p/15032636.html</a><br><a target="_blank" rel="noopener" href="http://mactech.sheridanc.on.ca/team/mark-galaszkiewicz/getting-started-docker-ansible/">http://mactech.sheridanc.on.ca/team/mark-galaszkiewicz/getting-started-docker-ansible/</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shgh_2004/article/details/80599515">https://blog.csdn.net/shgh_2004/article/details/80599515</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1632980">https://cloud.tencent.com/developer/article/1632980</a></p>
<h3><span id="基于centos-7-镜像搭建">基于centos 7 镜像搭建</span></h3><p>Centos 7.9，启动多个docker容器，模拟练习使用ansible</p>
<p>&#x2F;Users&#x2F;****&#x2F;Desktop&#x2F;mydirect&#x2F;github&#x2F;ansible&#x2F;env_build2</p>
<h4><span id="安装步骤">安装步骤</span></h4><p>1、按如下目录结构创建文件</p>
<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.08.18.png">

<p>&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[defaults]</span><br><span class="line">; 是否启用主机密钥检查: https://docs.ansible.com/ansible/latest/inventory_guide/connection_details.html#managing-host-key-checking</span><br><span class="line">; 初期使用密码连接分发公钥时先设置为False</span><br><span class="line">; 对应的环境变量设置: export ANSIBLE_HOST_KEY_CHECKING=False</span><br><span class="line">host_key_checking = False</span><br><span class="line">interpreter_python = /usr/bin/python</span><br></pre></td></tr></table></figure>

<p>&#x2F;etc&#x2F;ansible&#x2F;hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[myvirtualmachines]</span><br><span class="line">192.0.2.50</span><br><span class="line">192.0.2.51</span><br><span class="line">192.0.2.52</span><br></pre></td></tr></table></figure>

<p>Dockerfile:  构造生成 example-ansible-cent-master 容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM example-ansible-cent-node:latest</span><br><span class="line"></span><br><span class="line">RUN yum install  -y ansible openssh sshpass</span><br><span class="line"></span><br><span class="line">RUN mkdir -p /etc/ansible &amp;&amp; \</span><br><span class="line">    ssh-keygen -t rsa -P &quot;&quot; -f ~/.ssh/id_rsa</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Dockerfile.node：生成 镜像example-ansible-cent-node</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">FROM centos:centos7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RUN yum install -y openssh-server tzdata python3 &amp;&amp; \</span><br><span class="line">    cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \</span><br><span class="line">    sed -i &quot;s/#PermitRootLogin.*/PermitRootLogin yes/g&quot; /etc/ssh/sshd_config &amp;&amp; \</span><br><span class="line">    ssh-keygen -A &amp;&amp; \</span><br><span class="line">    echo &quot;root:123456&quot; | chpasswd</span><br><span class="line"></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line">CMD [&quot;/usr/sbin/sshd&quot;, &quot;-D&quot;]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>docker-compose.yml：启动四个容器：</p>
<p>example-ansible-cent-master，example-ansible-cent-node1，</p>
<p>example-ansible-cent-node2，example-ansible-cent-node3</p>
<p>master安装ansible，运行ansible命令，控制操作其他三个结点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  net-ansible:</span><br><span class="line">    ipam:</span><br><span class="line">      driver: default</span><br><span class="line">      config:</span><br><span class="line">        - subnet: 192.0.2.0/24</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  master:</span><br><span class="line">    build:</span><br><span class="line">      dockerfile: Dockerfile</span><br><span class="line">      context: .</span><br><span class="line">    container_name: example-ansible-cent-master</span><br><span class="line">    hostname: ansible</span><br><span class="line">    volumes:</span><br><span class="line">      - ./etc/ansible:/etc/ansible</span><br><span class="line">      - ./ansible:/server/scripts/ansible</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.49</span><br><span class="line"></span><br><span class="line">  node1:</span><br><span class="line">    image: example-ansible-cent-node</span><br><span class="line">    hostname: host1</span><br><span class="line">    container_name: example-ansible-cent-node1</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.50</span><br><span class="line">  node2:</span><br><span class="line">    image: example-ansible-cent-node</span><br><span class="line">    hostname: host2</span><br><span class="line">    container_name: example-ansible-cent-node2</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.51</span><br><span class="line">  node3:</span><br><span class="line">    image: example-ansible-cent-node</span><br><span class="line">    hostname: host3</span><br><span class="line">    container_name: example-ansible-cent-node3</span><br><span class="line">    networks:</span><br><span class="line">      net-ansible:</span><br><span class="line">        ipv4_address: 192.0.2.52</span><br></pre></td></tr></table></figure>

<p>2、启动容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 构建host1、host2、host3的镜像</span><br><span class="line">docker build -t example-ansible-cent-node -f ./Dockerfile.node .</span><br><span class="line"></span><br><span class="line"># 启动四个容器</span><br><span class="line">docker-compose up -d</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、登录ansible容器，修改yum源，安装依赖，分发ssh公钥</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#进入ansible容器 (master)</span><br><span class="line">docker exec -it example-ansible-cent-master bash</span><br><span class="line"></span><br><span class="line">#安装依赖</span><br><span class="line">yum install -y net-tools openssh-clients wget</span><br><span class="line">#修改yum源</span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">wget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line">yum makecache</span><br><span class="line">#安装ansible和ssh</span><br><span class="line">yum install -y ansible openssh sshpass</span><br><span class="line"></span><br><span class="line"># 执行下面命令分发ssh公钥</span><br><span class="line">ansible myvirtualmachines -k -m authorized_key -a &quot;user=root key=&#x27;&#123;&#123; lookup(&#x27;file&#x27;, &#x27;~/.ssh/id_rsa.pub&#x27;) &#125;&#125;&#x27;&quot;</span><br><span class="line">输入密码123456</span><br></pre></td></tr></table></figure>

<p>4、测试连通性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">; 测试连接</span><br><span class="line">ansible all -m ping</span><br><span class="line">ansible myvirtualmachines -m ping</span><br></pre></td></tr></table></figure>

<p>5、所有结点关闭和启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker-compose stop</span><br><span class="line">docker-compose start</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="虚拟机多结点">虚拟机多结点</span></h3><p>使用virturebox和centos7搭建多台虚拟机</p>
<p>网络配置需要配置两块网卡：其一保证可访问外网；其二分配固定ip，虚拟机之间互相访问</p>
<h4><span id="virturebox网卡配置">virturebox网卡配置</span></h4><img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.41.12.png">





<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.41.19.png">



<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.42.09.png">



<img src="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/截屏2023-07-04 00.42.24.png">





<h4><span id="虚拟机网络配置">虚拟机网络配置</span></h4><p>cat  &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-enp0s3 (动态ip:dhcp)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TYPE=static</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=dhcp</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=enp0s3</span><br><span class="line">UUID=55fa0f47-7b70-40c7-a4c1-59e912cf5cc9</span><br><span class="line">DEVICE=enp0s3</span><br><span class="line">ONBOOT=yes</span><br></pre></td></tr></table></figure>

<p>cat  &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-enp0s8 （静态：固定IP）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">TYPE=static</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=enp0s8</span><br><span class="line">#UUID=55fa0f47-7b70-40c7-a4c1-59e912cf5cc9</span><br><span class="line">DEVICE=enp0s8</span><br><span class="line">ONBOOT=yes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">IPADDR=192.168.56.101</span><br><span class="line">#NETMASK=255.255.255.0</span><br><span class="line">#GATEWAY=192.168.56.1</span><br></pre></td></tr></table></figure>

<h4><span id="参考资料">参考资料</span></h4><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/341328334">VirtualBox虚拟机配置双网卡同时连接内外网</a></p>
<h2><span id="入门使用">入门使用</span></h2><p>ansible是一个配置管理工具，通过ssh(或密码)管理多台主机，不是类似mysql的服务，底层基于python实现</p>
<h3><span id="配置文件">配置文件</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ansible主要文件：</span><br><span class="line"></span><br><span class="line">/etc/ansible/hosts 主机清单文件</span><br><span class="line">/etc/ansible/ansible.cfg 主配置文件</span><br><span class="line"></span><br><span class="line">ansible-doc -l 模块文档查看</span><br><span class="line">ansible-doc ping 模块用法</span><br><span class="line">ansible-doc -s ping  模块用法</span><br></pre></td></tr></table></figure>



<h3><span id="常用模块">常用模块</span></h3><h4><span id="shell-模块">shell 模块</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible myvirtualmachines -m shell -a &#x27;echo $HOSTNAME&#x27;</span><br></pre></td></tr></table></figure>

<h4><span id="script-模块">script 模块</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;&gt; /tmp/test1.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">echo  `date`</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ansible myvirtualmachines -m script -a /tmp/test1.sh</span><br></pre></td></tr></table></figure>



<h3><span id="playbook">Playbook</span></h3><p>cat &#x2F;etc&#x2F;ansible&#x2F;hosts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[myvirtualmachines]</span><br><span class="line">192.0.2.50</span><br><span class="line">192.0.2.51</span><br><span class="line">192.0.2.52</span><br><span class="line"></span><br><span class="line">[test1]</span><br><span class="line">host1 ansible_ssh_host=192.0.2.50 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22</span><br><span class="line">host2 ansible_ssh_host=192.0.2.51 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22</span><br><span class="line">host3 ansible_ssh_host=192.0.2.52 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22</span><br></pre></td></tr></table></figure>

<h4><span id="执行shell">执行shell</span></h4><p>ansible-playbook hello.yml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat hello.yml</span><br><span class="line"></span><br><span class="line">- hosts: test1</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: tom</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: &#x27;echo 123\|&#x27;</span><br></pre></td></tr></table></figure>



<h4><span id="执行script">执行script</span></h4><p>ansible-playbook test1.yml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat /tmp/test1.sh   </span><br><span class="line">#!/bin/bash</span><br><span class="line">echo 12333</span><br><span class="line"></span><br><span class="line">vi test1.yml</span><br><span class="line">- hosts: test1</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: root</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/test1.sh    </span><br></pre></td></tr></table></figure>



<h3><span id="become_user用法">Become_user用法</span></h3><p>以非root用户执行playbook</p>
<h4><span id="用法">用法</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#命令行用法：适用于全局 (-u remote-user覆盖playbook定义的remote-user)</span><br><span class="line">ansible-playbook  -u tom -b --become-method sudo   hello2.yml </span><br><span class="line"></span><br><span class="line">cat hello2.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: root</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line"></span><br><span class="line">cat test1.sh </span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/hosts</span><br><span class="line">#123</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">#作用与play下的单个task</span><br><span class="line">ansible-playbook     hello4_2.yml </span><br><span class="line"></span><br><span class="line">cat hello4_2.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br><span class="line">      </span><br><span class="line">cat test1.sh </span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; /etc/hosts</span><br><span class="line">#123</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#作用与单个play，应用到play下的每个task</span><br><span class="line">ansible-playbook     hello3.yml </span><br><span class="line"></span><br><span class="line">cat hello3.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: tom</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50  </span><br><span class="line"></span><br><span class="line">#作用与单个play，应用到play下的每个task, 比hello3更精简</span><br><span class="line">cat hello4.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50  </span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">#作用与单个play，应用到play下的每个task, 比hello4 多了sudo</span><br><span class="line">cat hello5.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: sudo /usr/sbin/shutdown -h 23:50 </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">总体来说，hello4最精简，不需要sudo, 不需要设置remote_user， hello4_2控制精度最高，become_user对应的是单个task</span><br><span class="line"></span><br><span class="line">become_method 默认值sudo，因此可以继续精简，如下是最简洁的写法 (hello3_3.yml)</span><br><span class="line">cat hello3_3.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">其他用法：</span><br><span class="line">第一种：</span><br><span class="line">ansible-playbook -u tom hello4_3.yml </span><br><span class="line"></span><br><span class="line">cat hello4_3.yml </span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  remote_user: root</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br><span class="line"></span><br><span class="line">第二种：</span><br><span class="line">ansible-playbook hello3_2.yml </span><br><span class="line"></span><br><span class="line">cat hello3_2.yml </span><br><span class="line"> </span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  remote_user: tom</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become_user: root</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50 </span><br><span class="line"></span><br><span class="line">第三种是错误的：如下所示</span><br><span class="line">####如下是错误的写法，要避免</span><br><span class="line">ansible-playbook -u tom hello4_4.yml</span><br><span class="line">ansible-playbook  hello4_4.yml</span><br><span class="line">- hosts: test2</span><br><span class="line">  remote_user: root</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_user: tom</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br></pre></td></tr></table></figure>



<h4><span id="总体操作流程">总体操作流程</span></h4><p>1、首先新建用户（比如tom），加入&#x2F;etc&#x2F;sudoers文件，确保新用户（tom）可以执行sudo su -</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#visudo 添加一行</span><br><span class="line">tom     ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure>

<p>2、编辑 ansible的hosts文件信息，将服务器地址、用户等信息加入hosts文件中,</p>
<p>ansible_become_pass 等价于用户手动执行sudo command要输入的pwd</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[test2]</span><br><span class="line">cent2 ansible_ssh_host=192.168.56.102 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22 ansible_become_pass=&quot;tom&quot;</span><br><span class="line">cent3 ansible_ssh_host=192.168.56.103 ansible_ssh_user=&quot;tom&quot; ansible_ssh_pass=&quot;tom&quot; ansible_ssh_port=22 ansible_become_pass=&quot;tom&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、修改 playbook yml文件，可以针对单个task， 单个play，或者所有play，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">--单个task</span><br><span class="line"></span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      script: /tmp/ansible/test1.sh</span><br><span class="line">      become_method: sudo</span><br><span class="line">      become: yes</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">--单个play</span><br><span class="line">- hosts: test2</span><br><span class="line">  gather_facts: no</span><br><span class="line">  become_method: sudo</span><br><span class="line">  become: yes</span><br><span class="line">  tasks:</span><br><span class="line">    - name: hello world</span><br><span class="line">      shell: /usr/sbin/shutdown -h 23:50  </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">--所有task</span><br><span class="line">ansible-playbook  -u tom -b --become-method sudo   hello2.yml </span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4><span id="参考文献">参考文献</span></h4><p><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_15735145/5547297">play-book 普通用户提权</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_privilege_escalation.html">Understanding privilege escalation: become</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/" data-id="cljn4kj610000rx9adxvgh8tg" data-title="ansible使用" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-tools/java/maven/maven远程仓库配置" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/tools/java/maven/maven%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T14:01:21.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/tools-java-maven/">tools/java/maven</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/tools/java/maven/maven%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/">maven远程仓库配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->



<!-- tocstop -->

<p>修改 MAVEN_HOME&#x2F;conf&#x2F;settings.xml文件，</p>
<p>1、本地仓库位置 修改</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;localRepository&gt;$&#123;user.home&#125;/.m2/repository&lt;/localRepository&gt;</span><br><span class="line">默认位置 ~/.m2/repository </span><br></pre></td></tr></table></figure>



<p>2、远程仓库地址 配置</p>
<p>当本地仓库找不到对应依赖包时，从配置的远程仓库中拉取依赖包、下载到本地。</p>
<p>如下示例在profiles  配置了两个远程仓库，id分别为repo、alimaven，id必须是唯一的，不能重复。</p>
<p>activeProfiles 指明两个仓库都是激活状态，都可用于项目构建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&lt;profiles&gt;</span><br><span class="line">    &lt;!--</span><br><span class="line">     | Specifies a set of introductions to the build process, to be activated using one or more of the</span><br><span class="line">     | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/&gt;</span><br><span class="line">     | or the command line, profiles have to have an ID that is unique.</span><br><span class="line">     |</span><br><span class="line">     | An encouraged best practice for profile identification is to use a consistent naming convention</span><br><span class="line">     | for profiles, such as &#x27;env-dev&#x27;, &#x27;env-test&#x27;, &#x27;env-production&#x27;, &#x27;user-jdcasey&#x27;, &#x27;user-brett&#x27;, etc.</span><br><span class="line">     | This will make it more intuitive to understand what the set of introduced profiles is attempting</span><br><span class="line">     | to accomplish, particularly when you only have a list of profile id&#x27;s for debug.</span><br><span class="line">   --&gt;</span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;repo&lt;/id&gt;</span><br><span class="line">      &lt;repositories&gt;</span><br><span class="line">          &lt;repository&gt;</span><br><span class="line">              &lt;id&gt;repo&lt;/id&gt;</span><br><span class="line">              &lt;name&gt;Central Repository&lt;/name&gt;</span><br><span class="line">              &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt;</span><br><span class="line">      &lt;!--         &lt;layout&gt;default&lt;/layout&gt;</span><br><span class="line">              &lt;snapshots&gt;</span><br><span class="line">                  &lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">              &lt;/snapshots&gt; --&gt;</span><br><span class="line">          &lt;/repository&gt;</span><br><span class="line">      &lt;/repositories&gt;</span><br><span class="line">    &lt;/profile&gt;</span><br><span class="line"></span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;alimaven&lt;/id&gt;</span><br><span class="line">      &lt;repositories&gt;</span><br><span class="line">          &lt;repository&gt;</span><br><span class="line">           &lt;id&gt;alimaven&lt;/id&gt;</span><br><span class="line">           &lt;name&gt;alimaven&lt;/name&gt;</span><br><span class="line">           &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span><br><span class="line">      &lt;!--         &lt;layout&gt;default&lt;/layout&gt;</span><br><span class="line">              &lt;snapshots&gt;</span><br><span class="line">                  &lt;enabled&gt;false&lt;/enabled&gt;</span><br><span class="line">              &lt;/snapshots&gt; --&gt;</span><br><span class="line">          &lt;/repository&gt;</span><br><span class="line">      &lt;/repositories&gt;</span><br><span class="line">    &lt;/profile&gt;</span><br><span class="line"></span><br><span class="line">&lt;/profiles&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- activeProfiles</span><br><span class="line">   | List of profiles that are active for all builds.</span><br><span class="line">  &lt;/activeProfiles&gt;</span><br><span class="line">  --&gt;</span><br><span class="line"></span><br><span class="line">&lt;activeProfiles&gt;</span><br><span class="line">    &lt;activeProfile&gt;repo&lt;/activeProfile&gt;</span><br><span class="line">    &lt;activeProfile&gt;alimaven&lt;/activeProfile&gt;</span><br><span class="line">&lt;/activeProfiles&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/tools/java/maven/maven%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/" data-id="cljmy0mmf0000v29a26p20mk6" data-title="maven远程仓库配置" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/command/linux常用命令" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/system/linux/command/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T13:55:00.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-command/">system/linux/command</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/system/linux/command/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">linux常用命令</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E7%8E%AF%E5%A2%83">环境：</a></li>
<li><a href="#%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C">文件操作</a><ul>
<li><a href="#find">Find</a></li>
</ul>
</li>
<li><a href="#%E5%8E%8B%E7%BC%A9%E5%92%8C%E8%A7%A3%E5%8E%8B%E7%BC%A9">压缩和解压缩</a></li>
<li><a href="#vim">VIM</a></li>
<li><a href="#%E7%AE%A1%E7%90%86%E7%94%A8%E6%88%B7%E7%BB%84">管理用户组</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE">网络配置</a></li>
<li><a href="#%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86">软件包管理</a></li>
<li><a href="#%E5%86%85%E6%A0%B8">内核</a><ul>
<li><a href="#grub">GRUB</a></li>
</ul>
</li>
<li><a href="#%E8%BF%9B%E7%A8%8B">进程</a></li>
<li><a href="#%E5%88%86%E5%8C%BA">分区</a></li>
<li><a href="#%E8%B5%84%E6%96%99">资料</a></li>
</ul>
<!-- tocstop -->

<h3><span id="环境">环境：</span></h3><p>linux版本：</p>
<p>内核：<a target="_blank" rel="noopener" href="https://kernel.org/">https://kernel.org/</a></p>
<p>发行版：Redhat\centos\federa\  ubuntu\debian  后两种有界面</p>
<p>init 3  进入登录</p>
<p>Init 0 关机</p>
<p>虚拟机安装：<a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">virtualbox</a></p>
<p>os:  <a target="_blank" rel="noopener" href="http://isoredirect.centos.org/centos/7/isos/x86_64/">centos</a></p>
<h3><span id="文件操作">文件操作</span></h3><p>Mv  :  重命名 或移动 文件（目录）</p>
<p>Ctrl + l  &#x3D;  clear</p>
<p>cd - 返回上一级目录</p>
<h4><span id="find">Find</span></h4><p>find &#x2F;etc  -name init</p>
<p>find &#x2F; -size +204800  # 在根目录查找&gt;100m的文件，100MB &#x3D;&#x3D; 102400KB&#x3D;&#x3D;204800数据快</p>
<h3><span id="压缩和解压缩">压缩和解压缩</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c 打tar包 打包</span><br><span class="line">x解tar包 解包</span><br><span class="line">f 文件</span><br><span class="line">j bzip2解压缩</span><br><span class="line">z gzip压缩</span><br></pre></td></tr></table></figure>

<p>tar jxf  &#x2F;tmp&#x2F;etc-backup.tar.bz2  -C &#x2F;tmp&#x2F;test1</p>
<p>tar jcf  &#x2F;tmp&#x2F;etc-backup.tar.bz2  &#x2F;etc</p>
<p>tar zxf  &#x2F;tmp&#x2F;etc-backup.tar.gz  -C  &#x2F;tmp&#x2F;test2</p>
<p>tar zcf  &#x2F;tmp&#x2F;etc-backup.tar.gz  &#x2F;etc</p>
<p>tar cf   &#x2F;tmp&#x2F;etc.tar  &#x2F;etc     &#x2F;&#x2F;打tar包</p>
<p>gzip  etc.tar         &#x2F;&#x2F;.tar.gz</p>
<p>gzip -d  etc.tar.gz   &#x2F;&#x2F;.tar.gz -&gt; .tar</p>
<p>zip -r etc.zip &#x2F;etc   zip压缩</p>
<p>unzip  etc.zip  -d    &#x2F;tmp&#x2F;test1</p>
<h3><span id="vim">VIM</span></h3><p>配置文件 vi &#x2F;etc&#x2F;virc</p>
<p>正常编辑模式： -i -I -a -A -o -O   hjkl （上下左右） </p>
<p>yy 3yy  y$(光标到行的末尾) 复制</p>
<p>dd 3dd d$ 剪切</p>
<p>u（撤销） ctrl+r 恢复动作</p>
<p>输入set nu 显示行号</p>
<p>x   3x 删除多个字符</p>
<p>r + 新字符 替换</p>
<p>11 + G  移动到指定第11行  g + g移动到首行  G移动到最后一行</p>
<p>shift+^ 行首  shift + $ 行尾</p>
<p>命令模式： </p>
<p>输入set nu 显示行号</p>
<p>输入set nonu 不显示行号</p>
<p>wq 保存 退出</p>
<p>!command 执行linux命令</p>
<p>&#x2F;x  查找字符串 x, n向下查找，shift + n 向上查找</p>
<p>%s&#x2F;old&#x2F;new  所有行第一个替换    s&#x2F;old&#x2F;new 当前行第一个替换</p>
<p>%s&#x2F;old&#x2F;new&#x2F;g  所有行替换 (&#x2F;g全局所有行)</p>
<p>3,5s&#x2F;old&#x2F;new&#x2F;g (第三到第五行替换)</p>
<p>可视模式：</p>
<p>v</p>
<p>shift +v</p>
<p>ctrl + v</p>
<h3><span id="管理用户组">管理用户组</span></h3><p>&#x2F;etc&#x2F;passwd  用户</p>
<p>&#x2F;etc&#x2F;shadow  密码</p>
<p> &#x2F;etc&#x2F;group 用户组</p>
<p>useradd  useradd -g group1 user1</p>
<p>userdel</p>
<p>passwd user1 修改密码</p>
<p>usermod 修改用户属性   usermod -g group1 user1</p>
<p>chage</p>
<p>groupadd</p>
<p>groupdel</p>
<p>su - tom 切换到用户tom</p>
<p>sudo   以普通用户身份登录的状态下，执行某些只有root才能执行的命令，比如sudo shutdown</p>
<p>实现方式，直接执行visudo命令 对文件&#x2F;etc&#x2F;sudoers修改、授权</p>
<p>visudo    为用户添加执行某些root命令的权限，实质是修改 &#x2F;etc&#x2F;sudoers文件 </p>
<p>chown</p>
<p>chgrp</p>
<p>chmod  u g o a  u+x u-x</p>
<p>目录权限：x 进入目录</p>
<p>  rx 进入目录，查看文件</p>
<p> wx 进入目录，删除文件，修改文件名</p>
<h3><span id="网络配置">网络配置</span></h3><p>修改网卡显示名字</p>
<p>vi  &#x2F;etc&#x2F;default&#x2F;grub, 在GRUB_CMDLINE_LINUX 增加 biosdevname&#x3D;0 net.ifnames&#x3D;1</p>
<p>grub2-mkconfig  -o  &#x2F;boot&#x2F;grub2&#x2F;grub.cfg    &#x2F;&#x2F;重新生成grub配置文件</p>
<p>reboot</p>
<p>mii-tool etho  查看网卡物理网络连接情况</p>
<p>route  -n 查看路由 网关</p>
<p>修改ip:  ifconfig  eth0  10.0.2.4  netmask 255.255.255.0</p>
<p>ifup  启动网卡</p>
<p>route add  增加路由</p>
<p>route del   减少一条路由</p>
<p>ping </p>
<p>traceroute -w 1  <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a>   </p>
<p>mtr</p>
<p>nslookup  域名查看  nslookup   <a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a></p>
<p>telnet   telnet  ww.baidu.com   80</p>
<p>tcpdump     查看网络数据包  tcpdump  -i any  -n port   80</p>
<p>​                     tcpdump -i any  -n host 10.0.0.1</p>
<p>​                     tcpdump -i any  -n host 10.0.0.1 and port  80</p>
<p>​                     tcpdump -i any  -n host 10.0.0.1 and port  80  -w  &#x2F;tmp&#x2F;file1</p>
<p>netstat  netstat   -tlunp</p>
<p>ss</p>
<p> service network restart  (systemctl   restart  NetworkManager.service)</p>
<p>&#x2F;etc&#x2F;sysconfig&#x2F;netwok-scripts&#x2F;ifcfg-eth0   </p>
<p>​     BOOTPROTO(dhcp|none)  IPADDR  GATEWAY   NETMASK  DNS1  DNS2  DNS3</p>
<p>​	</p>
<p>hostnamectl  set-hostname c7.test11</p>
<p>vi  &#x2F;etc&#x2F;hosts</p>
<p>Selinux  强制访问控制 </p>
<p>​     getenforce</p>
<p>​     setenforce  0</p>
<p>​     &#x2F;etc&#x2F;selinux&#x2F;sysconfig</p>
<p>​    </p>
<h3><span id="软件包管理">软件包管理</span></h3><p>rpm  -qa  查看</p>
<p>rpm  -i   安装</p>
<p>rpm  -e 卸载</p>
<p>yum 简化rpm包的下载、安装、依赖</p>
<p>配置yum 源  &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo,   <a target="_blank" rel="noopener" href="https://developer.aliyun.com/mirror/centos?spm=a2c6h.13651102.0.0.39491b11mLBS4h">国内aliyun yum源</a></p>
<p>yum -y install</p>
<p>yum remove</p>
<p>yum list</p>
<p>yum update  ***   更新软件包</p>
<p>源代码编译安装</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget  https://openresty.org/download/openresty-1.21.4.1.tar.gz</span><br><span class="line">tar -zxvf openresty-1.21.4.1.tar.gz</span><br><span class="line">cd openresty-1.21.4.1</span><br><span class="line">./configure  --prefix=/usr/local/openresty  (prefix 指定安装目录)</span><br><span class="line">make -j2  (-j2 使用2个核进行编译)</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perl: https://www.cpan.org/src/5.0/perl-5.36.1.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>epel    扩展包  yum install  epel-release  -y</p>
<h3><span id="内核">内核</span></h3><p>uname -r 查看内核版本</p>
<p>lscpu  查看cpu</p>
<h4><span id="grub">GRUB</span></h4><p>（GRand Unified Bootloader，大一统启动加载器），是一个<a target="_blank" rel="noopener" href="https://wiki.archlinuxcn.org/wiki/Arch_%E7%9A%84%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B#%E5%BC%95%E5%AF%BC%E5%8A%A0%E8%BD%BD%E7%A8%8B%E5%BA%8F">引导加载程序</a></p>
<p>vi  &#x2F;etc&#x2F;default&#x2F;grub</p>
<p>ls &#x2F;etc&#x2F;grub.d&#x2F;</p>
<p>vi  &#x2F;boot&#x2F;grub2&#x2F;grub.cfg</p>
<p>grub2-mkconfig  -o  &#x2F;boot&#x2F;grub2&#x2F;grub.cfg    &#x2F;&#x2F;重新生成grub配置文件</p>
<h3><span id="进程">进程</span></h3><p>ps</p>
<p>ps -eLf</p>
<p>pstree 进程树</p>
<p>top</p>
<p>top -p pid    只查某一个进程</p>
<p>nice  启动时指定进程优先级</p>
<p>renice  运行中指定进程优先级</p>
<p>.&#x2F;a.sh &amp;    加&amp;表示后端运行，进程放入后端</p>
<p>jobs  查看正在运行的后端进程</p>
<p>fg+ 数字(jobs显示的)  进程调回前端</p>
<p>bg+ 数字(jobs显示的)  进程调入后端</p>
<p>ctrl + z 正在前端运行的进程调入后端，并进行stop状态，通过jobs  fg后续继续执行</p>
<p>kill -l</p>
<p>ctrl+c</p>
<p>kill -9  pid</p>
<p>nohup  + cmd + &amp; </p>
<p>守护进程 daemon</p>
<p>screen   </p>
<p>​         screen 进入screen环境</p>
<p>​         screen  -ls    查看后端有哪些screen环境下的进程</p>
<p>​         ctrl + a  d   进程调入后端</p>
<p>​          screen -r id  进程调回前端</p>
<p>进程日志   </p>
<p>​     &#x2F;var&#x2F;log&#x2F;messages  </p>
<p>​     &#x2F;var&#x2F;log&#x2F;dmesg   内核</p>
<p>​     &#x2F;var&#x2F;log&#x2F;secure</p>
<p>​     &#x2F;var&#x2F;log&#x2F;cron</p>
<p>systemctl   start&#x2F;stop&#x2F;enable&#x2F;disable  </p>
<p>Systemctl    </p>
<p>top</p>
<p>free</p>
<p>fdisk -l  磁盘分区</p>
<p>parted -l</p>
<p>df -h</p>
<p>du -sh 实际占用空间</p>
<p>ls -lh</p>
<p>dd  if&#x3D;&#x2F;dev&#x2F;zero  bs&#x3D;4M  count&#x3D;10 of&#x3D;bfile    复制产生一个文件</p>
<p>文件系统  ext4  xfs ntfs</p>
<p>ls -i  查看i节点</p>
<p>ln   硬链接   i节点相同，不能跨分区</p>
<p>ln -s  软链接  i节点不同，</p>
<p>getfacl   文件访问权限控制</p>
<p>setfacl  -m u:user1:r  afile   赋读权</p>
<h3><span id="分区">分区</span></h3><p>fdisk  分区, 大于2T 要使用parted 进行分区</p>
<p>mkfs.ext4   文件系统格式化</p>
<p>mkdir  &#x2F;mnt&#x2F;sdc1  mount  &#x2F;dev&#x2F;sdc1  &#x2F;mnt&#x2F;sdc1    挂载文件系统</p>
<p>vi  &#x2F;etc&#x2F;fstab   文件里修改，对挂载固化，重启自动挂载</p>
<p>磁盘配额 quota  限制用户对磁盘的使用</p>
<h3><span id="资料">资料</span></h3><p><a target="_blank" rel="noopener" href="https://time.geekbang.org/course/detail/100029601-101469">极客时间 linux实战技能 100讲</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/system/linux/command/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" data-id="cljmxdm1q00001h9a94fi72lf" data-title="linux常用命令" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-system/linux/host/monitor/linux主机监控" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T10:01:05.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/">linux主机监控</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#cpu%E8%B4%9F%E8%BD%BD">cpu负载</a><ul>
<li><a href="#%E5%B9%B3%E5%9D%87%E8%B4%9F%E8%BD%BD">平均负载</a></li>
<li><a href="#cpu%E4%BD%BF%E7%94%A8%E5%88%86%E6%9E%90">cpu使用分析</a><ul>
<li><a href="#%E8%BF%9B%E7%A8%8Bcpu%E4%BD%BF%E7%94%A8%E7%BB%9F%E8%AE%A1">进程cpu使用统计</a></li>
</ul>
</li>
<li><a href="#%E8%BF%9B%E7%A8%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2">进程上下文切换</a></li>
<li><a href="#cpu%E4%BD%BF%E7%94%A8%E7%8E%87">CPU使用率</a><ul>
<li><a href="#%E6%9F%A5%E6%89%BE%E7%83%AD%E7%82%B9%E5%87%BD%E6%95%B0">查找热点函数</a></li>
</ul>
</li>
<li><a href="#%E7%9F%AD%E6%97%B6%E8%BF%9B%E7%A8%8B%E7%9B%91%E6%8E%A7">短时进程监控</a><ul>
<li><a href="#execsnoop">execsnoop</a></li>
</ul>
</li>
<li><a href="#%E4%B8%8D%E5%8F%AF%E4%B8%AD%E6%96%AD%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B">不可中断进程和僵尸进程</a></li>
<li><a href="#%E8%BD%AF%E4%B8%AD%E6%96%AD">软中断</a></li>
<li><a href="#%E7%A1%AC%E4%B8%AD%E6%96%AD">硬中断</a></li>
</ul>
</li>
<li><a href="#%E5%86%85%E5%AD%98">内存</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%89">定义</a></li>
<li><a href="#%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F">内存泄漏</a></li>
<li><a href="#swap-%E4%BA%A4%E6%8D%A2">Swap 交换</a></li>
</ul>
</li>
<li><a href="#io">IO</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87">性能指标</a></li>
</ul>
</li>
<li><a href="#%E7%BD%91%E7%BB%9C">网络</a><ul>
<li><a href="#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">性能测试</a></li>
<li><a href="#dns">DNS</a></li>
<li><a href="#%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90">流量分析</a></li>
<li><a href="#nat">NAT</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E7%A1%80">基础</a><ul>
<li><a href="#%E7%B3%BB%E7%BB%9F">系统</a></li>
<li><a href="#cpu">CPU</a></li>
<li><a href="#%E7%A3%81%E7%9B%98">磁盘</a><ul>
<li><a href="#%E7%A3%81%E7%9B%98raid">磁盘raid</a></li>
</ul>
</li>
<li><a href="#%E5%AE%B9%E5%99%A8">容器</a><ul>
<li><a href="#references">References:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E8%B5%84%E6%96%99">资料</a></li>
</ul>
<!-- tocstop -->

<h2><span id="cpu负载">cpu负载</span></h2><h3><span id="平均负载">平均负载</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Load average，系统的平均活跃进程数。它反应了系统的整体负载情况。 uptime显示三个数值，分别指过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载；理想情况下，平均负载等于逻辑 CPU 个数</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>uptime或top查看 CPU负载情况， （watch -d uptime  -d 参数表示高亮显示变化的区域）</p>
<p>定义：平均负载，表示的是活跃进程数，</p>
<p>单位时间内，系统中处于可运行状态和不可中断状态的平均进程数</p>
<p>可运行状态的进程：正在使用cpu或者正在等待cpu的进程，即ps aux命令下STAT处于R状态的进程 </p>
<p>不可中断状态的进程：处于内核态关键流程中的进程，且不可被打断，如等待硬件设备IO响应，ps命令D状态的进程</p>
<p>1、CPU密集型</p>
<p>2、IO 密集型</p>
<p>3、大量进程</p>
<h3><span id="cpu使用分析">cpu使用分析</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">含义：cpu非空闲时间占总 CPU 时间的百分比。cpu状态(us、ni、sys、id、wa、hi、si、st、guest)</span><br><span class="line">工具：top  vmstat  mpstat  sar  /proc/stat</span><br><span class="line">说明：/proc/stat是其他性能工具的数据来源</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>分析工具：安装stress(压测)  sysstat（分析）</p>
<p>stress   -c 1</p>
<p>stress   -c 8        spawn N workers spinning on sqrt()  模拟8个进程，同时在执行任务sqrt</p>
<p>stress   -d 2        spawn N workers spinning on write()&#x2F;unlink() 模拟2个进程，同时在执行磁盘写操作</p>
<h4><span id="进程cpu使用统计">进程cpu使用统计</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具：top pidstat  ps</span><br><span class="line">说明：top和ps按cpu使用率对进程排序，pidstat只显示实际用了cpu的进程</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>cpu监控：mpstat   -P ALL 5  2      打印2次，每5s间隔打印一次，监控所有cpu,</p>
<p>进程级别监控：</p>
<p>​     pidstat  -u 5  2    进程cpu使用统计，打印2次，每5s间隔打印一次</p>
<p>​     pidstat -d 5 2    进程IO统计(磁盘读写速率)，打印2次，每5s间隔打印一次</p>
<p>​     pidstat  -r  5  2    进程内存使用统计，打印2次，每5s间隔打印一次</p>
<h3><span id="进程上下文切换">进程上下文切换</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vmstat 3      整个系统上下文切换次数 整体情况</span><br><span class="line">pidstat -wt   5   上下文切换次数  进程级别 + 线程       -w 进程切换  -t具体到线程统计</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>进程的切换只能发生在内核态。所以，</p>
<p>进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，</p>
<p>还包括了内核堆栈、寄存器等内核空间的状态</p>
<p>所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I&#x2F;O、内存等系统资源不足时，就会发生自愿上下文切换。</p>
<p>而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换</p>
<p>压测工具 sysbench：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 以10个线程运行5分钟的基准测试，模拟多线程切换的问题</span><br><span class="line">sysbench --threads=10 --max-time=300 threads run</span><br></pre></td></tr></table></figure>



<p>vmstat 3      整个系统上下文切换次数 整体情况</p>
<p>pidstat -wt   5   上下文切换次数  进程级别 + 线程       -w 进程切换  -t具体到线程统计</p>
<p>cpu中断情况查看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-d 参数表示高亮显示变化的区域</span><br><span class="line">watch -d cat /proc/interrupts</span><br><span class="line">           CPU0       CPU1</span><br><span class="line">RES:    2450431    5279697   Rescheduling interrupts</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>cpu上下文切换排查思路：</p>
<p>首先通过uptime查看系统负载，然后使用mpstat结合pidstat来初步判断到底是cpu计算量大还是进程争抢过大或者是io过多，接着使用vmstat分析切换次数，以及切换类型，来进一步判断到底是io过多导致问题还是进程争抢激烈导致问题。</p>
<h3><span id="cpu使用率">CPU使用率</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间。</span><br><span class="line">nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的    CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低。</span><br><span class="line"></span><br><span class="line">system（通常缩写为 sys），代表内核态 CPU 时间。</span><br><span class="line">idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）。</span><br><span class="line">iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间。</span><br><span class="line">irq（通常缩写为 hi），代表处理硬中断的 CPU 时间。</span><br><span class="line">softirq（通常缩写为 si），代表处理软中断的 CPU 时间。</span><br><span class="line">steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间。</span><br><span class="line">guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间。guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间。</span><br></pre></td></tr></table></figure>

<h4><span id="查找热点函数">查找热点函数</span></h4><p>perf top -g  -p pid    查找热点函数，定位引起cpu使用率高的具体函数</p>
<p>perf record -g  -p pid </p>
<p>perf report </p>
<p>centos install apache benchmark:</p>
<p>yum install -y  httpd-tools</p>
<p> ab -c 10 -n 100 <a target="_blank" rel="noopener" href="http://192.168.56.101:10000/">http://192.168.56.101:10000/</a>    并发10个请求测试Nginx性能，总共测试100个请求</p>
<p>perf  top -g -p 3927   正常查看调用链</p>
<p>perf record -g  -p 3927  导出到文件</p>
<p>perf  report     &#x2F;&#x2F;查看， 如果进程是在容器中，可以如下先挂载文件，再查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#进程在容器中的情况下，可以把容器目录挂载到宿主机</span><br><span class="line">mkdir /tmp/foo</span><br><span class="line">PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; &lt;container-name&gt;)</span><br><span class="line">PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; app)</span><br><span class="line">bindfs /proc/$PID/root /tmp/foo  ( yum -y install bindfs)</span><br><span class="line">perf report --symfs /tmp/foo</span><br><span class="line"></span><br><span class="line"># 使用完成后不要忘记解除绑定</span><br><span class="line">umount /tmp/foo/</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="短时进程监控">短时进程监控</span></h3><p>通过pstree 查看父子进程关系</p>
<p>perf record -g </p>
<p>perf  report     &#x2F;&#x2F;查看， 如果进程是在容器中，可以如上先挂载文件，再查看（如上）</p>
<h4><span id="execsnoop">execsnoop</span></h4><h3><span id="不可中断进程和僵尸进程">不可中断进程和僵尸进程</span></h3><p>僵尸进程,  查看调用链 pstree  -aps  pid</p>
<h3><span id="软中断">软中断</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具：top mpstat   /proc/softirqs </span><br><span class="line">说明：top使用率，其他提供各种软中断在每个cpu运行的累积次数</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>ps aux | grep softirq</p>
<p>cat &#x2F;proc&#x2F;softirqs </p>
<p>Linux 中的中断处理程序分为上半部和下半部：</p>
<p>上半部对应硬件中断，用来快速处理中断。</p>
<p>下半部对应软中断，用来异步处理上半部未完成的工作。</p>
<h3><span id="硬中断">硬中断</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">工具： vmstat    /proc/interrupts</span><br><span class="line">说明：vmstat总的中断次数，/proc/interrupts提供各种中断在每个cpu运行的累积次数</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="内存">内存</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">系统已用、可用、剩余内存</span><br><span class="line">free -h | vmstat | sar –r | cat /proc/meminfo</span><br><span class="line"></span><br><span class="line">进程虚拟内存、常驻内存、共享</span><br><span class="line">top  | ps aux | </span><br><span class="line">pidstat  -r  5  2  (进程内存使用统计，打印2次，每5s间隔打印一次)</span><br><span class="line"></span><br><span class="line">进程内存分布</span><br><span class="line">pmap</span><br><span class="line"></span><br><span class="line">缓存/缓冲区用量</span><br><span class="line">Free | vmstat | sar | cachestat</span><br><span class="line"></span><br><span class="line">Swap可用空间和剩余空间</span><br><span class="line">Free | sar</span><br><span class="line"></span><br><span class="line">Swap换入和换出</span><br><span class="line">vmstat</span><br><span class="line"></span><br><span class="line">内存泄漏检测</span><br><span class="line">memleak</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="定义">定义</span></h3><p>Buffer </p>
<p>Cache </p>
<p>Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中</p>
<p>cachestat 提供了整个操作系统缓存的读写命中情况</p>
<p>cachetop 提供了每个进程的缓存命中情况</p>
<p>pmap pid  进程的内存分布</p>
<p>available &#x3D; free_pages - total_reserved + pagecache + SReclaimable</p>
<p>(空闲内存减去所有zones的lowmem reserve和high watermark，再加上page cache和slab中可以回收的部分)</p>
<p>Cache &#x3D; page cache + SReclaimable</p>
<pre><code>total  Total installed memory (MemTotal and SwapTotal in /proc/meminfo)

used   Used memory (calculated as total - free - buffers - cache)

free   Unused memory (MemFree and SwapFree in /proc/meminfo)
</code></pre>
<p>​    </p>
<pre><code>buffers
   Memory used by kernel buffers (Buffers in /proc/meminfo)
cache  
   Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)
</code></pre>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93841288">知乎</a>  <a target="_blank" rel="noopener" href="https://lotabout.me/2021/Linux-Available-Memory/">内存去哪里</a></p>
<h3><span id="内存泄漏">内存泄漏</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">应用程序可以访问的用户内存空间，由只读段、数据段、堆、栈以及文件映射段等组成。</span><br><span class="line">其中，堆内存和文件映射段，需要应用程序来动态管理内存段，所以我们必须小心处理。不仅要会用标准库函数 malloc() 来动态分配内存，还要记得在用完内存后，调用库函数 free() 来释放它们。</span><br></pre></td></tr></table></figure>



<p>pmap pid  进程的内存分布</p>
<p>&#x2F;usr&#x2F;share&#x2F;bcc&#x2F;tools&#x2F;memleak -a -p  pid</p>
<p>memleak 可以跟踪系统或指定进程的内存分配、释放请求，然后定期输出一个未释放内存和相应调用栈的汇总情况（默认 5 秒）</p>
<h3><span id="swap-交换">Swap 交换</span></h3><p>Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。</p>
<p>Swap 说白了就是把一块磁盘空间或者一个本地文件</p>
<p>所谓换出，就是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存。</p>
<p>而换入，则是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来。</p>
<p>在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，来释放文件页和匿名页，以便把内存分配给更需要的进程使用。</p>
<p>文件页的回收比较容易理解，直接清空缓存，或者把脏数据写回磁盘后，再释放缓存就可以了。</p>
<p>而对不常访问的匿名页，则需要通过 Swap 换出到磁盘中，这样在下次访问的时候，再次从磁盘换入到内存中就可以了。</p>
<h2><span id="io">IO</span></h2><h3><span id="性能指标">性能指标</span></h3><p>五个常见指标: 使用率、饱和度、IOPS、吞吐量以及响应时间。这五个指标，是衡量磁盘性能的基本指标。<br>• 使用率，是指磁盘处理 I&#x2F;O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I&#x2F;O 存在性能瓶颈。<br>• 饱和度，是指磁盘处理 I&#x2F;O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I&#x2F;O 请求。<br>• IOPS（Input&#x2F;Output Per Second），是指每秒的 I&#x2F;O 请求数。<br>• 吞吐量，是指每秒的 I&#x2F;O 请求大小。<br>• 响应时间，是指 I&#x2F;O 请求从发出到收到响应的间隔时间。</p>
<p>磁盘IO性能</p>
<p>iostat -d -x 1</p>
<p>iotop</p>
<p>pidstat -d 1</p>
<p>sar -d</p>
<p>dstat</p>
<p>其他：</p>
<p>文件系统空间用量  df -h</p>
<p>索引节点使用 df -ih</p>
<p>磁盘大小 lsblk</p>
<h2><span id="网络">网络</span></h2><table>
<thead>
<tr>
<th align="left"><strong>指标</strong></th>
<th><strong>工具</strong></th>
<th align="right"><strong>说明</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">吞吐量</td>
<td>sar -n  DEV | nethogs</td>
<td align="right">网络接口、进程的网络吞吐量</td>
</tr>
<tr>
<td align="left">PPS</td>
<td>sar | proc&#x2F;net&#x2F;dev</td>
<td align="right">网络接口PPS</td>
</tr>
<tr>
<td align="left">连接数</td>
<td>netstat -ltunp | ss</td>
<td align="right">查看端口、网络连接数</td>
</tr>
<tr>
<td align="left">端口占用</td>
<td>lsof -i:22 | netstat -ltunp</td>
<td align="right">查看端口占用</td>
</tr>
<tr>
<td align="left">端口连通性</td>
<td>telnet 192.68.56.1 22</td>
<td align="right">查看远程服务端口是否是通的</td>
</tr>
<tr>
<td align="left">延迟</td>
<td>ping</td>
<td align="right">测试网络延迟</td>
</tr>
<tr>
<td align="left">路由</td>
<td>route | traceroute</td>
<td align="right">查看路由并测试链路信息</td>
</tr>
<tr>
<td align="left">DNS</td>
<td>dig | nslookup</td>
<td align="right">排查DNS解析问题</td>
</tr>
<tr>
<td align="left">防火墙</td>
<td>iptables</td>
<td align="right">配置和管理防火墙</td>
</tr>
<tr>
<td align="left">ip地址</td>
<td>ifconfig</td>
<td align="right"></td>
</tr>
<tr>
<td align="left">网络抓包</td>
<td>tcpdump | wireshark</td>
<td align="right">抓包分析网络流量</td>
</tr>
</tbody></table>
<p>sar -n DEV 1</p>
<p>nethogs  -d 2   按进程实时统计网络带宽利用率（yum install -y nethogs）</p>
<p>dstat  性能查询工具，包括网络收发情况</p>
<h3><span id="性能测试">性能测试</span></h3><p>tcp&#x2F;udp  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># -s表示启动服务端，-i表示汇报间隔，-p表示监听端口</span><br><span class="line">$ iperf3 -s -i 1 -p 10000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># -c表示启动客户端，192.168.56.100为目标服务器的IP</span><br><span class="line"># -b表示目标带宽(单位是bits/s)</span><br><span class="line"># -t表示测试时间</span><br><span class="line"># -P表示并发数，-p表示目标服务器监听端口</span><br><span class="line">$ iperf3 -c 192.168.56.101 -b 1G -t 15 -P 2 -p 10000</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>http性能：</p>
<p>ab -c 1000 -n 10000 <a target="_blank" rel="noopener" href="http://192.168.0.30/">http://192.168.0.30/</a></p>
<p>应用负载性能：</p>
<p>wrk\Jmeter\LoadRunner</p>
<h3><span id="dns">DNS</span></h3><p>cat &#x2F;etc&#x2F;resolv.conf   dns 服务器配置</p>
<p>ping  163.com</p>
<p>nslookup  163.com   域名解析</p>
<p>time nslookup  163.com   域名解析消耗时间</p>
<p>yum -y install dnsmasq; systemctl start  dnsmasq     dnsmasq 增加DNS缓存</p>
<p>nslookup -type&#x3D;PTR 35.190.27.188 8.8.8.8  根据ip解析域名(反解析)</p>
<h3><span id="流量分析">流量分析</span></h3><p>tcpdump &#x2F; Wireshark</p>
<p>tcpdump使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-nn ，表示不解析抓包中的域名（即不反向解析）、协议以及端口号。</span><br><span class="line">udp port 53 ，表示只显示 UDP 协议的端口号（包括源端口和目的端口）为 53 的包。</span><br><span class="line">host 35.190.27.188 ，表示只显示 IP 地址（包括源地址和目的地址）为 35.190.27.188 的包。</span><br><span class="line">结果保存到ping.pcap文件</span><br><span class="line"></span><br><span class="line">tcpdump -nn udp port 53 or host 35.190.27.188 -w ping.pcap</span><br><span class="line"></span><br><span class="line">--tcpdump 的输出格式</span><br><span class="line">时间戳 协议 源地址.源端口 &gt; 目的地址.目的端口 网络包详细信息</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Wireshark</p>
<p>wireshark的使用推荐阅读林沛满的《Wireshark网络分析就这么简单》和《Wireshark网络分析的艺术》</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -nn host example.com -w web.pcap</span><br><span class="line">curl http://example.com</span><br><span class="line"></span><br><span class="line">使用 Wireshark 打开 web.pcap</span><br></pre></td></tr></table></figure>



<h3><span id="nat">NAT</span></h3><p>Network Address Translation</p>
<p>NAT 技术可以重写 IP 数据包的源 IP 或者目的 IP，被普遍地用来解决公网 IP 地址短缺的问题。它的主要原理就是，网络中的多台主机，通过共享同一个公网 IP 地址，来访问外网资源。</p>
<p>NAT 的主要目的，是实现地址转换。根据实现方式的不同，</p>
<p>NAT 可以分为三类：</p>
<p>静态 NAT，即内网 IP 与公网 IP 是一对一的永久映射关系；</p>
<p>动态 NAT，即内网 IP 从公网 IP 池中，动态选择一个进行映射；</p>
<p>网络地址端口转换 NAPT（Network Address and Port Translation），即把内网 IP 映射到公网 IP 的不同端口上，让多个内网 IP 可以共享同一个公网 IP 地址。</p>
<p>网络性能</p>
<p>sar -n DEV</p>
<h2><span id="基础">基础</span></h2><h3><span id="系统">系统</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">版本：cat /etc/redhat-release</span><br></pre></td></tr></table></figure>



<h3><span id="cpu">CPU</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">架构：arch | uname -a</span><br><span class="line">cpu核数和型号： lscpu</span><br></pre></td></tr></table></figure>

<p>64位和32位处理器：64表示cpu可以处理的最大位数</p>
<p>cpu架构：x86（pc、服务器）  arm(移动端，如高通骁龙)  </p>
<p>cpu核数： 一个CPU可以包含若干个物理核，通过超线程HT（Hyper-Threading）技术可以将一个物理核变成两个逻辑处理核。vCPU（virtual CPU）是ECS实例的虚拟处理核</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查看物理CPU个数</span><br><span class="line">cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l</span><br><span class="line"></span><br><span class="line"># 查看每个物理CPU中core的个数(即核数)</span><br><span class="line">cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br><span class="line"></span><br><span class="line"># 查看逻辑CPU的个数</span><br><span class="line">cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l</span><br></pre></td></tr></table></figure>



<h3><span id="磁盘">磁盘</span></h3><p>磁盘大小:  lsblk</p>
<p>磁盘使用情况：df -h</p>
<h4><span id="磁盘raid">磁盘raid</span></h4><p>在Linux系统中，常见的RAID（冗余磁盘阵列）级别有以下几种类型：</p>
<ol>
<li><p>RAID 0（条带化）：<br>RAID 0将多个物理磁盘组合成一个逻辑卷，实现数据的条带化分布。它提供了较高的读写性能，但没有冗余功能，即一个磁盘损坏将导致数据完全丢失。</p>
</li>
<li><p>RAID 1（镜像）：<br>RAID 1通过将数据同时写入两个或多个磁盘来实现数据的镜像备份。即使一个磁盘故障，数据仍然可以从其他镜像磁盘恢复。RAID 1提供了很好的数据冗余和容错性能，但对存储空间有一定的浪费。</p>
</li>
<li><p>RAID 5：<br>RAID 5通过将数据和奇偶校验分布在多个磁盘上来提供冗余和容错性能。数据条带化并使用奇偶校验进行分布，允许从单个磁盘故障中恢复数据。RAID 5需要至少3个磁盘，并且具有相对较高的读取性能和适度的写入性能。</p>
</li>
<li><p>RAID 6：<br>RAID 6类似于RAID 5，但它使用两个奇偶校验进行冗余，提供更高的容错能力。RAID 6需要至少4个磁盘，并且可以从两个磁盘故障中恢复数据。它比RAID 5更安全，但对写入性能有一定的影响。</p>
</li>
<li><p>RAID 10（1+0）：<br>RAID 10是将RAID 1和RAID 0组合在一起的级别。它通过将数据镜像和条带化结合，提供了较高的读写性能和冗余能力。RAID 10需要至少4个磁盘，但对存储空间有一定的浪费。</p>
</li>
<li><p>RAID 50：<br>RAID 50是将RAID 5和RAID 0结合在一起的级别。它通过条带化和奇偶校验的组合提供了较高的性能和容错性能。RAID 50需要至少6个磁盘。</p>
</li>
</ol>
<p>这些是常见的Linux RAID级别，每个级别都有自己的特点、优势和限制。选择适当的RAID级别取决于数据安全性、性能需求和可用的硬件资源。注意，某些高级RAID级别可能需要专用的RAID控制器支持。</p>
<h3><span id="容器">容器</span></h3><p>OCI（Open Container Initiative）即开放的容器运行时<code>规范</code>，目的在于定义一个容器运行时及镜像的相关标准和规范，其中包括</p>
<ul>
<li>runtime-spec：容器的生命周期管理</li>
<li>image-spec：镜像的生命周期管理</li>
</ul>
<p>runc(run container)&#96;是一个基于OCI标准实现的一个轻量级容器运行工具，用来创建和运行容器。而Containerd是用来维持通过runc创建的容器的运行状态。即runc用来创建和运行容器，containerd作为常驻进程用来管理容器。</p>
<p><code>containerd（container daemon）</code>是一个daemon进程用来管理和运行容器，可以用来拉取&#x2F;推送镜像和管理容器的存储和网络。其中可以调用runc来创建和运行 容器。</p>
<p>docker 和runc、containerd的 关系：</p>
<img src="/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/1.png">







<h4><span id="references">References:</span></h4><p><a target="_blank" rel="noopener" href="https://www.huweihuang.com/kubernetes-notes/runtime/runtime.html">分析OCI，CRI，runc，containerd，cri-containerd，dockershim等组件说明及调用关系</a></p>
<p><a target="_blank" rel="noopener" href="https://chinalhr.github.io/post/docker-runc/">Docker容器运行时引擎-runC分析</a></p>
<h2><span id="资料">资料</span></h2><p>极客时间：<a target="_blank" rel="noopener" href="https://time.geekbang.org/column/intro/100020901?tab=catalog">Linux性能优化实战</a>，多操作，多实践，反复阅读并实践</p>
<p>网络分析：Wireshark网络分析就这么简单 (林沛满）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/system/linux/host/monitor/linux%E4%B8%BB%E6%9C%BA%E7%9B%91%E6%8E%A7/" data-id="cljm9suky00002a9a4i41bbbm" data-title="linux主机监控" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-tools/github/hexo/simple_usage" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/tools/github/hexo/simple_usage/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T09:06:37.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/tools-github-hexo/">tools/github/hexo</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/tools/github/hexo/simple_usage/">simple_usage</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#install-and-usage">Install And Usage</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E9%97%AE%E9%A2%98">其他问题</a><ul>
<li><a href="#%E7%9B%AE%E5%BD%95%E4%B8%8D%E6%98%BE%E7%A4%BA">目录不显示</a></li>
<li><a href="#%E5%9B%BE%E7%89%87%E4%B8%8D%E6%98%BE%E7%A4%BA">图片不显示</a></li>
<li><a href="#github%E8%AE%BF%E9%97%AE%E6%85%A2%E9%97%AE%E9%A2%98">Github访问慢问题</a></li>
<li><a href="#github-token-%E8%AE%BE%E7%BD%AE">Github token 设置</a></li>
<li><a href="#think">Think</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="install-and-usage">Install And Usage</span></h2><p>环境： macos 12.4  | Intel Core i7 | hexo: 6.3.0  | hexo-cli: 4.3.1  | node: 16.14.2</p>
<p>1、github 新建仓库 username.github.io</p>
<p>2、install node.js:  brew install node  &#x2F; <a target="_blank" rel="noopener" href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a></p>
<p>3、install hexo<br>npm install -g hexo-cli</p>
<p>4、指定目录，初始化<br>mkdir &#x2F;Users&#x2F;username&#x2F;Desktop&#x2F;mydirect&#x2F;gitblog&#x2F;cont<br>hexo init</p>
<p>(出现问题，找不到或无法加载主类 install： npm install，hexo s )<br>5、本地运行<br>hexo s</p>
<p>6、部署到github<br>a、新建仓库  username.github.io（username 为github用户名）<br>b、修改根目录配置文件 _congif.yml</p>
<p>deploy:<br>  type: git<br>  repo: <a target="_blank" rel="noopener" href="https://github.com/username/username.github.io.git">https://github.com/username/username.github.io.git</a><br>  branch: master</p>
<p>c、<br>npm install hexo-deployer-git –save<br>hexo clean<br>hexo deploy</p>
<p>d、查看效果<br><a target="_blank" rel="noopener" href="https://username.github.io/">https://username.github.io/</a></p>
<p>7、发布一篇博客</p>
<p>a.</p>
<p>hexo new  ‘HBase概览’  -p  nosql&#x2F;hbase&#x2F;HBase概览</p>
<p>hexo new  ‘realtime_doc’  -p  bigdata&#x2F;warehouse&#x2F;realtime_doc</p>
<p>b.<br>source&#x2F;_posts&#x2F;nosql&#x2F;hbase&#x2F;目录下找到刚才建立的 HBase概览.md文件, 进行编辑</p>
<p>title: Hello post #文章标题<br>date: 2014-08-05 11:15:00 #发表时间<br>categories: #分类<br>toc: true</p>
<p>tags: #标签，多个标签时可以用[标签1,标签2]的方式，或者”- 标签“的方式每行一个。</p>
<p>#这里是正文，用markdown语法写。</p>
<p>c. 编辑完文章后依次执行hexo g和hexo s,然后访问localhost:4000来预览效果<br>d. 没有问题了以后，执行hexo d来同步到GitHub</p>
<h2><span id="其他问题">其他问题</span></h2><p>其他：<br>ssh免密码登录配置<br>更换主题：<a target="_blank" rel="noopener" href="https://hexo.io/themes/">https://hexo.io/themes/</a> 下载，安装，<br>下载项目至博客项目下的themes目录中，文件夹命名为material，并在博客配置文件_config.yml中指定使用该主题<br>hexo 使用：<a target="_blank" rel="noopener" href="http://ijiaober.github.io/2014/08/04/hexo/hexo-03/">http://ijiaober.github.io/2014/08/04/hexo/hexo-03/</a></p>
<p>–如何上传source .md 到github ?<br>–图片显示  <a target="_blank" rel="noopener" href="https://chiselee.cn/2020/02/04/HexoNotShowpic/">https://chiselee.cn/2020/02/04/HexoNotShowpic/</a><br>–目录显示  <a target="_blank" rel="noopener" href="http://kuangqi.me/tricks/enable-table-of-contents-on-hexo/">http://kuangqi.me/tricks/enable-table-of-contents-on-hexo/</a></p>
<p>npm install <a target="_blank" rel="noopener" href="https://github.com/CodeFalling/hexo-asset-image">https://github.com/CodeFalling/hexo-asset-image</a> –save</p>
<h3><span id="目录不显示">目录不显示</span></h3><p>1、安装hex-toc 插件：npm install  hexo-toc –save</p>
<p>2、修改_config.yml:  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">toc:</span><br><span class="line">  maxdepth: 6</span><br></pre></td></tr></table></figure>

<p>3、在需要显示目录的地方 添加一行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- toc --&gt;</span><br></pre></td></tr></table></figure>



<h3><span id="图片不显示">图片不显示</span></h3><p>1、安装一个图片路径转换的插件，这个插件名字是hexo-asset-image</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-asset-image --save</span><br></pre></td></tr></table></figure>

<p>2、修改node_modules&#x2F;hexo-asset-image&#x2F;index.js，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">&#x27;use strict&#x27;;</span><br><span class="line">var cheerio = require(&#x27;cheerio&#x27;);</span><br><span class="line"></span><br><span class="line">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span><br><span class="line">function getPosition(str, m, i) &#123;</span><br><span class="line">  return str.split(m, i).join(m).length;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">var version = String(hexo.version).split(&#x27;.&#x27;);</span><br><span class="line">hexo.extend.filter.register(&#x27;after_post_render&#x27;, function(data)&#123;</span><br><span class="line">  var config = hexo.config;</span><br><span class="line">  if(config.post_asset_folder)&#123;</span><br><span class="line">    	var link = data.permalink;</span><br><span class="line">	if(version.length &gt; 0 &amp;&amp; Number(version[0]) == 3)</span><br><span class="line">	   var beginPos = getPosition(link, &#x27;/&#x27;, 1) + 1;</span><br><span class="line">	else</span><br><span class="line">	   var beginPos = getPosition(link, &#x27;/&#x27;, 3) + 1;</span><br><span class="line">	// In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.</span><br><span class="line">	var endPos = link.lastIndexOf(&#x27;/&#x27;) + 1;</span><br><span class="line">    link = link.substring(beginPos, endPos);</span><br><span class="line"></span><br><span class="line">    var toprocess = [&#x27;excerpt&#x27;, &#x27;more&#x27;, &#x27;content&#x27;];</span><br><span class="line">    for(var i = 0; i &lt; toprocess.length; i++)&#123;</span><br><span class="line">      var key = toprocess[i];</span><br><span class="line"> </span><br><span class="line">      var $ = cheerio.load(data[key], &#123;</span><br><span class="line">        ignoreWhitespace: false,</span><br><span class="line">        xmlMode: false,</span><br><span class="line">        lowerCaseTags: false,</span><br><span class="line">        decodeEntities: false</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      $(&#x27;img&#x27;).each(function()&#123;</span><br><span class="line">		if ($(this).attr(&#x27;src&#x27;))&#123;</span><br><span class="line">			// For windows style path, we replace &#x27;\&#x27; to &#x27;/&#x27;.</span><br><span class="line">			var src = $(this).attr(&#x27;src&#x27;).replace(&#x27;\\&#x27;, &#x27;/&#x27;);</span><br><span class="line">			if(!/http[s]*.*|\/\/.*/.test(src) &amp;&amp;</span><br><span class="line">			   !/^\s*\//.test(src)) &#123;</span><br><span class="line">			  // For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed.</span><br><span class="line">			  // In addition, to support multi-level local directory.</span><br><span class="line">			  var linkArray = link.split(&#x27;/&#x27;).filter(function(elem)&#123;</span><br><span class="line">				return elem != &#x27;&#x27;;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  var srcArray = src.split(&#x27;/&#x27;).filter(function(elem)&#123;</span><br><span class="line">				return elem != &#x27;&#x27; &amp;&amp; elem != &#x27;.&#x27;;</span><br><span class="line">			  &#125;);</span><br><span class="line">			  if(srcArray.length &gt; 1)</span><br><span class="line">				srcArray.shift();</span><br><span class="line">			  src = srcArray.join(&#x27;/&#x27;);</span><br><span class="line">			  $(this).attr(&#x27;src&#x27;, config.root + link + src);</span><br><span class="line">			  console.info&amp;&amp;console.info(&quot;update link as:--&gt;&quot;+config.root + link + src);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;else&#123;</span><br><span class="line">			console.info&amp;&amp;console.info(&quot;no src attr, skipped...&quot;);</span><br><span class="line">			console.info&amp;&amp;console.info($(this));</span><br><span class="line">		&#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      data[key] = $.html();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、修改_config.yml文件，新建文章时自动创建一个与文章名相同的文件夹用来存放图片文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_asset_folder: true</span><br></pre></td></tr></table></figure>

<p>比如新建文章simple_usage时， 执行 hexo new  ‘simple_usage’  -p  tools&#x2F;github&#x2F;hexo&#x2F;simple_usage，</p>
<p>自动创建目录simple_usage，该目录存放图片，</p>
<p><strong>注意 注意</strong></p>
<p><strong>特别需要注意的是</strong>：markdown中插入图片要使用img的用法，并且要使用相对路径，否则不显示，示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img  src=&quot;simple_usage/截屏2023-07-03 16.34.09.png&quot;&gt;</span><br></pre></td></tr></table></figure>

<img src="/2023/07/03/tools/github/hexo/simple_usage/截屏2023-07-03 16.34.09.png">



<p>参考资料:  </p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/542101567">hexo图片不显示</a></p>
<p><a target="_blank" rel="noopener" href="https://juejin.cn/post/7006594302604214280">解决hexo引用本地图片无法显示的问题</a></p>
<h3><span id="github访问慢问题">Github访问慢问题</span></h3><p>1、手动设置 hosts</p>
<p>从网站查询github.com对应ip，更新&#x2F;etc&#x2F;hosts文件，示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.30.255.113  github.com</span><br></pre></td></tr></table></figure>

<p>2、编码定时 自动拉取，并更新hosts文件</p>
<p>从github下载<a target="_blank" rel="noopener" href="https://github.com/oldj/SwitchHosts/releases/tag/v4.1.2">SwitchHosts</a>，install，add one rule.</p>
<img src="/2023/07/03/tools/github/hexo/simple_usage/截屏2023-07-03 18.15.54.png">



<p>参考资料：</p>
<p><a target="_blank" rel="noopener" href="https://juejin.cn/post/7019683061977579557">https://juejin.cn/post/7019683061977579557</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/isevenluo/github-hosts">https://github.com/isevenluo/github-hosts</a></p>
<h3><span id="github-token-设置">Github token 设置</span></h3><p><a target="_blank" rel="noopener" href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens">github access token设置</a></p>
<h3><span id="think">Think</span></h3><p>所有资料仅为参考，要根据实际问题具体分析</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/tools/github/hexo/simple_usage/" data-id="cljmm9dpf00008t9abge5cb7s" data-title="simple_usage" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/nosql/hbase/HBase概览" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T07:44:53.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/">HBase概览</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#hbase">HBase</a><ul>
<li><a href="#one">One</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="hbase">HBase</span></h2><h3><span id="one">One</span></h3><img src="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/截屏2023-07-03 15.49.39.png">


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/" data-id="cljmkav790000qj9ack1hbz8f" data-title="HBase概览" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdate-spark-spark-env/">bigdate/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">hadoop本地环境搭建</a>
          </li>
        
          <li>
            <a href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">spark环境搭建</a>
          </li>
        
          <li>
            <a href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/">processon</a>
          </li>
        
          <li>
            <a href="/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/">prometheus使用</a>
          </li>
        
          <li>
            <a href="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/">ansible使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>