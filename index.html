<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://qingfengzhou.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-java/designpattern/designpattern_pactice1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/20/java/designpattern/designpattern_pactice1/" class="article-date">
  <time class="dt-published" datetime="2023-07-20T08:35:04.000Z" itemprop="datePublished">2023-07-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/20/java/designpattern/designpattern_pactice1/">designpattern_pactice1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/20/java/designpattern/designpattern_pactice1/" data-id="clkbsqtpo0001ss9a71pi1mjm" data-title="designpattern_pactice1" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-java/jvm/jvm_basic" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/20/java/jvm/jvm_basic/" class="article-date">
  <time class="dt-published" datetime="2023-07-20T08:33:13.000Z" itemprop="datePublished">2023-07-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/java-jvm/">java/jvm</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/20/java/jvm/jvm_basic/">jvm_basic</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#classloader">Classloader</a><ul>
<li><a href="#%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B">类加载过程</a></li>
<li><a href="#%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8">类加载器</a></li>
<li><a href="#%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6">类加载机制</a></li>
</ul>
</li>
<li><a href="#%E5%86%85%E5%AD%98">内存</a><ul>
<li><a href="#%E5%86%85%E5%AD%98%E5%88%92%E5%88%86">内存划分</a></li>
<li><a href="#heap%E5%88%92%E5%88%86">heap划分</a></li>
<li><a href="#memoryerror">MemoryError</a></li>
</ul>
</li>
<li><a href="#gc">GC</a></li>
<li><a href="#references">References</a></li>
</ul>
<!-- tocstop -->

<h2><span id="classloader">Classloader</span></h2><img src="/2023/07/20/java/jvm/jvm_basic/截屏2023-07-20 17.08.42.png">







<h3><span id="类加载过程">类加载过程</span></h3><img src="/2023/07/20/java/jvm/jvm_basic/截屏2023-07-20 17.09.06.png">



<p>加载、链接(验证、准备、解析)、初始化</p>
<p>加载：JVM 读取 Class 文件，通过一个类的全限定名获取定义此类的二进制字节流，将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。</p>
<p>验证：主要用于确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，保证被加载类的正确性，进一步保障虚拟机自身的安全，只有通过验证的 Class 文件才能被 JVM 加载。</p>
<p>主要包括四种验证：文件格式验证，元数据验证，字节码验证，符号引用验证。</p>
<p>准备：主要工作是在方法区中为类变量分配内存空间并设置类中变量的初始值。</p>
<p>初始值指不同数据类型的默认值，这里需要注意 final 类型的变量和非 final 类型的变量在准备阶段的数据初始化过程不同。</p>
<p>解析：</p>
<p>JVM 将常量池内的符号引用转换为直接引用的过程。事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。</p>
<p>解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的CONSTANT Class info、CONSTANT Fieldref info、CONSTANT Methodref info等。</p>
<p>初始化：</p>
<p>主要通过执行类构造器的<clinit>方法为类进行初始化。<clinit>方法是在编译阶段由编译器自动收集类中静态语句块和变量的赋值操作组成的。在准备阶段，类中静态成员变量已经完成了默认初始化，而在初始化阶段，<clinit>方法将对静态成员变量进行显示初始化。</clinit></clinit></clinit></p>
<h3><span id="类加载器">类加载器</span></h3><h3><span id="类加载机制">类加载机制</span></h3><img src="/2023/07/20/java/jvm/jvm_basic/截屏2023-07-20 17.30.58.png">



<p>双亲委派机制</p>
<p>Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，Java虚拟机采用的是双亲委派模式，即把请求交由父类处理，它是一种任务委派模式。</p>
<p>双亲委派机制指一个类在收到类加载请求后不会尝试自己加载这个类，而是把该类加载请求向上委派给其父类去完成，其父类在接收到该类加载请求后又会将其委派给自己的父类，以此类推，这样所有的类加载请求都被向上委派到启动类加载器中。若父类加载器在接收到类加载请求后发现自己也无法加载该类（通常原因是该类的 Class 文件在父类的类加载路径中不存在），则父类会将该信息反馈给子类并向下委派子类加载器加载该类，直到该类被成功加载，若找不到该类，则 JVM 会抛出 ClassNotFoud 异常。</p>
<h2><span id="内存">内存</span></h2><h3><span id="内存划分">内存划分</span></h3><img src="/2023/07/20/java/jvm/jvm_basic/截屏2023-07-20 17.43.34.png">





<p>当我们通过前面的：类的加载 → 验证 → 准备 → 解析 → 初始化 这几个阶段完成后，执行引擎就会对我们的类进行使用，同时执行引擎将会使用到我们的运行时数据区。</p>
<p>　　运行时数据区处在JVM 的内存区域。我们通过磁盘或者网络IO得到的数据，都需要先加载到内存中，然后CPU从内存中获取数据进行读取，也就是说内存充当了CPU和磁盘之间的桥梁。</p>
<p>　　JVM 的运行时数据区分为<strong>线程私有区域（程序计数器、虚拟机栈、本地方法栈）</strong>、</p>
<p>​       <strong>线程共享区域（堆、方法区）</strong>和直接内存，如图所示：</p>
<p><strong>方法区</strong>：</p>
<p>在jdk7及以前，习惯上把方法区，称为永久代。jdk8开始，使用元空间取代了永久代。</p>
<p>JDK 1.8后，元空间存放在堆外内存中。它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等</p>
<p><strong>程序计数器：</strong>存储指向下一条指令的地址</p>
<p>本地方法栈： 执行native 方法，Native Method Stack 中登记 native方法</p>
<img src="/2023/07/20/java/jvm/jvm_basic/截屏2023-07-20 17.43.41.png">



<h3><span id="heap划分">heap划分</span></h3><p>堆内存划分</p>
<p>　　Minor GC：新生代的GC （eden区满）。</p>
<p>　　Major GC：老年代的GC。</p>
<p>　　Full GC：整堆收集，收集整个Java堆和方法区的垃圾收集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-Xms10m：设置最小堆内存为10M。</span><br><span class="line">-Xmx10m：设置最大堆内存为10M。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<img src="/2023/07/20/java/jvm/jvm_basic/截屏2023-07-20 18.12.03.png">





<h3><span id="memoryerror">MemoryError</span></h3><img src="/2023/07/20/java/jvm/jvm_basic/截屏2023-07-20 17.44.39.png">









<h2><span id="gc">GC</span></h2><p>。。。。</p>
<p>打印GC</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印GC </span></span><br><span class="line">-XX:+PrintGCDetails</span><br><span class="line"></span><br><span class="line"><span class="comment"># GC 回收统计</span></span><br><span class="line"><span class="comment"># 进程号为4827的进程的垃圾回收统计，每1000ms即1s打印一次统计情况，共打印5次</span></span><br><span class="line">jstat -gc 4827 1000 5</span><br><span class="line"> S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT   </span><br><span class="line">1024.0 1024.0  25.6   0.0    8192.0   1697.9   20480.0     2628.7   10880.0 10366.4 1408.0 1205.9      6    0.015   0      0.000    0.015</span><br><span class="line"></span><br><span class="line">S0C：第一个Survivor区的大小（KB）</span><br><span class="line">S1C：第二个Survivor区的大小（KB）</span><br><span class="line">S0U：第一个Survivor区的使用大小（KB）</span><br><span class="line">S1U：第二个Survivor区的使用大小（KB）</span><br><span class="line">EC：Eden区的大小（KB）</span><br><span class="line">EU：Eden区的使用大小（KB）</span><br><span class="line">OC：Old区大小（KB）</span><br><span class="line">OU：Old区的使用大小（KB）</span><br><span class="line">MC：方法区的大小（KB）</span><br><span class="line">MU：方法区的使用大小</span><br><span class="line">CCSC：压缩类的空间大小（KB）</span><br><span class="line">CCSU：压缩类的空间使用大小（KB）</span><br><span class="line">YGC：年青代垃圾回收次数</span><br><span class="line">YGCT：年轻代垃圾回收消耗时间</span><br><span class="line">FGC：年老代垃圾回收次数</span><br><span class="line">FGT：年老代垃圾回收消耗时间</span><br><span class="line">GCT：垃圾回收消耗总时间</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="references">References</span></h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ruoli-0/p/13882894.html#_label5">java jvm 博客园</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/20/java/jvm/jvm_basic/" data-id="clkbsqtpj0000ss9acl8q6m2i" data-title="jvm_basic" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/" rel="tag">java</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-java/concurrent/jiketime_java并发" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/" class="article-date">
  <time class="dt-published" datetime="2023-07-19T06:03:10.000Z" itemprop="datePublished">2023-07-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/java-concurrent/">java/concurrent</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/">jiketime_java并发</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E5%B9%B6%E5%8F%91%E6%BA%90%E5%A4%B4">并发源头</a></li>
<li><a href="#synchronize">synchronize</a><ul>
<li><a href="#%E6%A1%88%E4%BE%8B">案例</a><ul>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%951">解决方法1</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%952">解决方法2</a></li>
<li><a href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%953">解决方法3</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#wait-notify">wait-notify</a><ul>
<li><a href="#usage">Usage</a></li>
<li><a href="#others">Others</a><ul>
<li><a href="#wait-%E5%92%8C-sleep-%E5%8C%BA%E5%88%AB">wait 和 sleep 区别</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%AE%A1%E7%A8%8B">管程</a><ul>
<li><a href="#usage">usage</a><ul>
<li><a href="#code-test">Code Test</a></li>
</ul>
</li>
<li><a href="#others-1">Others</a></li>
</ul>
</li>
<li><a href="#thread">Thread</a><ul>
<li><a href="#%E7%8A%B6%E6%80%81"><strong>状态</strong></a></li>
<li><a href="#%E8%B0%83%E7%94%A8%E6%A0%88">调用栈</a></li>
<li><a href="#others-2">Others</a></li>
</ul>
</li>
<li><a href="#%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB">并发工具类</a><ul>
<li><a href="#reentrantlock">ReentrantLock</a><ul>
<li><a href="#%E5%AF%B9%E6%AF%94synchronized"><strong>对比synchronized</strong></a></li>
<li><a href="#usage-1">Usage</a></li>
<li><a href="#%E7%89%B9%E7%82%B9">特点</a><ul>
<li><a href="#%E5%8F%AF%E9%87%8D%E5%85%A5">可重入</a></li>
<li><a href="#%E5%85%AC%E5%B9%B3%E9%94%81">公平锁</a></li>
</ul>
</li>
<li><a href="#others-3">Others</a><ul>
<li><a href="#%E5%90%8C%E6%AD%A5%E5%92%8C%E5%BC%82%E6%AD%A5">同步和异步</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#semaphore">Semaphore</a><ul>
<li><a href="#usage-2">Usage</a><ul>
<li><a href="#%E4%BA%92%E6%96%A5%E9%94%81">互斥锁</a></li>
<li><a href="#%E9%99%90%E6%B5%81%E5%99%A8">限流器</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#reentrantreadwritelock">ReentrantReadWriteLock</a><ul>
<li><a href="#usage-3">Usage</a></li>
</ul>
</li>
<li><a href="#stampedlock">StampedLock</a><ul>
<li><a href="#%E5%86%99%E9%94%81-%E6%82%B2%E8%A7%82%E8%AF%BB%E9%94%81">写锁、悲观读锁</a></li>
<li><a href="#%E4%B9%90%E8%A7%82%E8%AF%BB">乐观读</a><ul>
<li><a href="#%E7%BB%8F%E5%85%B8%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F">经典使用方式</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a><ul>
<li><a href="#stampedlock-%E8%AF%BB%E6%A8%A1%E6%9D%BF">StampedLock 读模板</a></li>
<li><a href="#%E5%86%99%E6%A8%A1%E7%89%88">写模版</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%BA%BF%E7%A8%8B%E5%8D%8F%E8%B0%83">线程协调</a><ul>
<li><a href="#countdownlatch">CountDownLatch</a><ul>
<li><a href="#%E5%9C%BA%E6%99%AF">场景</a></li>
<li><a href="#example">Example</a></li>
</ul>
</li>
<li><a href="#cyclicbarrier">CyclicBarrier</a><ul>
<li><a href="#%E5%9C%BA%E6%99%AF-1">场景</a></li>
<li><a href="#example-1">Example</a><ul>
<li><a href="#%E4%BD%BF%E7%94%A8completablefuture-%E5%BC%82%E6%AD%A5%E6%89%A7%E8%A1%8C">使用CompletableFuture 异步执行</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%AE%B9%E5%99%A8">容器</a><ul>
<li><a href="#%E5%90%8C%E6%AD%A5%E5%AE%B9%E5%99%A8">同步容器</a><ul>
<li><a href="#%E8%BF%AD%E4%BB%A3%E5%99%A8">迭代器</a></li>
</ul>
</li>
<li><a href="#%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8">并发容器</a><ul>
<li><a href="#copyonwritearraylist">CopyOnWriteArrayList</a><ul>
<li><a href="#%E6%B3%A8%E6%84%8F">注意</a></li>
</ul>
</li>
<li><a href="#set">Set</a></li>
<li><a href="#map">Map</a><ul>
<li><a href="#hashmap">HashMap</a></li>
<li><a href="#treemap">TreeMap</a></li>
<li><a href="#linkedhashmap">LinkedHashMap</a></li>
<li><a href="#hashtable">HashTable</a></li>
<li><a href="#concurrenthashmap">ConcurrentHashMap</a></li>
<li><a href="#concurrentskiplistmap">ConcurrentSkipListMap</a></li>
</ul>
</li>
<li><a href="#queue">Queue</a><ul>
<li><a href="#%E6%9C%89%E7%95%8C%E9%98%9F%E5%88%97">有界队列</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#cas">CAS</a><ul>
<li><a href="#usage-4">Usage</a></li>
</ul>
</li>
<li><a href="#%E7%BA%BF%E7%A8%8B%E6%B1%A0">线程池</a><ul>
<li><a href="#%E8%B0%83%E7%94%A8">调用</a></li>
</ul>
</li>
<li><a href="#futuretask">FutureTask</a><ul>
<li><a href="#usage-5">Usage</a></li>
<li><a href="#example-2">Example</a></li>
</ul>
</li>
<li><a href="#completablefuture">CompletableFuture</a><ul>
<li><a href="#example-3">Example</a></li>
<li><a href="#usage-6">Usage</a><ul>
<li><a href="#%E5%88%9B%E5%BB%BA">创建</a></li>
<li><a href="#%E8%AE%BE%E7%BD%AE%E7%8B%AC%E7%AB%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0">设置独立线程池</a></li>
<li><a href="#%E9%80%9A%E8%BF%87future%E8%8E%B7%E5%8F%96%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C">通过future获取执行结果</a></li>
<li><a href="#completionstage-%E6%8E%A5%E5%8F%A3">CompletionStage 接口</a><ul>
<li><a href="#1%E6%8F%8F%E8%BF%B0%E4%B8%B2%E8%A1%8C%E5%85%B3%E7%B3%BB">（1）描述串行关系</a></li>
<li><a href="#2%E6%8F%8F%E8%BF%B0and%E8%81%9A%E5%90%88%E5%85%B3%E7%B3%BB">（2）描述And聚合关系</a></li>
<li><a href="#3%E6%8F%8F%E8%BF%B0or%E8%81%9A%E5%90%88%E5%85%B3%E7%B3%BB">（3）描述or聚合关系</a></li>
<li><a href="#4%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8">（4）处理异常</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#completionservice">CompletionService</a><ul>
<li><a href="#example-4">Example</a></li>
<li><a href="#usage-7">Usage</a><ul>
<li><a href="#%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95">构造方法</a></li>
<li><a href="#method">method</a></li>
<li><a href="#others-4">Others</a></li>
</ul>
</li>
<li><a href="#%E6%80%BB%E7%BB%93-1">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h1><span id="并发源头">并发源头</span></h1><p>可见性：多核cpu缓存导致， </p>
<p>原子性：线程切换导致，解决方法：</p>
<p>有序性：编译优化导致指令重排序，</p>
<p>常见解决方法：volatile 修饰 变量，synchronized 同步，线程顺序执行</p>
<h1><span id="synchronize">synchronize</span></h1><p>修饰静态方法、非静态方法、代码快，锁定的对象不一样</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class X &#123;</span><br><span class="line">  // 修饰非静态方法, 锁定的当前实例对象this</span><br><span class="line">  synchronized void foo() &#123;</span><br><span class="line">    // 临界区</span><br><span class="line">  &#125;</span><br><span class="line">  // 修饰静态方法, 锁定的是当前类的class对象, 对象例子Class X</span><br><span class="line">  synchronized static void bar() &#123;</span><br><span class="line">    // 临界区</span><br><span class="line">  &#125;</span><br><span class="line">  // 修饰代码块, 锁定一个指定的对象，对应例子 对象Obj</span><br><span class="line">  Object obj = new Object()；</span><br><span class="line">  void baz() &#123;</span><br><span class="line">    synchronized(obj) &#123;</span><br><span class="line">      // 临界区</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>



<h2><span id="案例">案例</span></h2><p>假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作：账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元，最后我们期望的结果应该是账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是 300 元。</p>
<p>我们假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作。这两个线程分别在两颗 CPU 上同时执行，那它们是互斥的吗？我们期望是，但实际上并不是。因为线程 1 锁定的是账户 A 的实例（A.this），而线程 2 锁定的是账户 B 的实例（B.this），所以这两个线程可以同时进入临界区 transfer()。同时进入临界区的结果是什么呢？线程 1 和线程 2 都会读到账户 B 的余额为 200，导致最终账户 B 的余额可能是 300（线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖），可能是 100（线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖），就是不可能是 200。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">package test.zhou.java.concurrent;</span><br><span class="line"></span><br><span class="line">public class AccountTest &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        Account a = new Account(200);</span><br><span class="line">        Account b = new Account(200);</span><br><span class="line">        Account c = new Account(200);</span><br><span class="line">        Thread t1 = new Thread(()-&gt;&#123;</span><br><span class="line">            a.transfer(b, 100);</span><br><span class="line">        &#125;);</span><br><span class="line">        Thread t2 = new Thread(()-&gt;&#123;</span><br><span class="line">            b.transfer(c, 100);</span><br><span class="line">        &#125;);</span><br><span class="line">        t1.start();</span><br><span class="line">        t2.start();</span><br><span class="line"></span><br><span class="line">        t1.join();</span><br><span class="line">        t2.join();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        System.out.println(a.balance);</span><br><span class="line">        System.out.println(b.balance);</span><br><span class="line">        System.out.println(c.balance);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Account &#123;</span><br><span class="line"></span><br><span class="line">    public int balance;</span><br><span class="line"></span><br><span class="line">    public Account(int balance) &#123;</span><br><span class="line">        this.balance = balance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // 转账</span><br><span class="line">    synchronized void transfer(</span><br><span class="line">            Account target, int amt) &#123;</span><br><span class="line">        if (this.balance &gt;= amt) &#123;</span><br><span class="line">            this.balance -= amt;</span><br><span class="line">            target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>账户转账结果错误，</p>
<h3><span id="解决方法1">解决方法1</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class Account &#123;</span><br><span class="line">  private int balance;</span><br><span class="line">  // 转账</span><br><span class="line">  void transfer(Account target, int amt)&#123;</span><br><span class="line">    // 锁定转出账户</span><br><span class="line">    synchronized(this) &#123;              </span><br><span class="line">      // 锁定转入账户</span><br><span class="line">      synchronized(target) &#123;           </span><br><span class="line">        if (this.balance &gt; amt) &#123;</span><br><span class="line">          this.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="解决方法2">解决方法2</span></h3><p>方法1 可能出现死锁（一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象）</p>
<p>死锁条件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1、互斥，共享资源 X 和 Y 只能被一个线程占用；</span><br><span class="line">2、占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X；</span><br><span class="line">3、不可抢占，其他线程不能强行抢占线程 T1 占有的资源；</span><br><span class="line">4、循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。</span><br></pre></td></tr></table></figure>



<p>一次性申请所有资源：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class Allocator &#123;</span><br><span class="line">  private List&lt;Object&gt; als =</span><br><span class="line">    new ArrayList&lt;&gt;();</span><br><span class="line">  // 一次性申请所有资源</span><br><span class="line">  synchronized boolean apply(</span><br><span class="line">    Object from, Object to)&#123;</span><br><span class="line">    if(als.contains(from) ||</span><br><span class="line">         als.contains(to))&#123;</span><br><span class="line">      return false;  </span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      als.add(from);</span><br><span class="line">      als.add(to);  </span><br><span class="line">    &#125;</span><br><span class="line">    return true;</span><br><span class="line">  &#125;</span><br><span class="line">  // 归还资源</span><br><span class="line">  synchronized void free(</span><br><span class="line">    Object from, Object to)&#123;</span><br><span class="line">    als.remove(from);</span><br><span class="line">    als.remove(to);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Account &#123;</span><br><span class="line">  // actr应该为单例</span><br><span class="line">  private Allocator actr;</span><br><span class="line">  private int balance;</span><br><span class="line">  // 转账</span><br><span class="line">  void transfer(Account target, int amt)&#123;</span><br><span class="line">    // 一次性申请转出账户和转入账户，直到成功</span><br><span class="line">    while(!actr.apply(this, target))</span><br><span class="line">      ；</span><br><span class="line">    try&#123;</span><br><span class="line">      // 锁定转出账户</span><br><span class="line">      synchronized(this)&#123;              </span><br><span class="line">        // 锁定转入账户</span><br><span class="line">        synchronized(target)&#123;           </span><br><span class="line">          if (this.balance &gt; amt)&#123;</span><br><span class="line">            this.balance -= amt;</span><br><span class="line">            target.balance += amt;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      actr.free(this, target)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="解决方法3">解决方法3</span></h3><p>方法2  执行同步方法 actr.apply(this, target)，actr是单例，多线程操作需要等待获取actr锁，仍然可能存在瓶颈。</p>
<p>方法3 是先对资源进行排序，依次加锁，效率更高。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class Account &#123;</span><br><span class="line">  private int id;</span><br><span class="line">  private int balance;</span><br><span class="line">  // 转账</span><br><span class="line">  void transfer(Account target, int amt)&#123;</span><br><span class="line">    Account left = this        ①</span><br><span class="line">    Account right = target;    ②</span><br><span class="line">    if (this.id &gt; target.id) &#123; ③</span><br><span class="line">      left = target;           ④</span><br><span class="line">      right = this;            ⑤</span><br><span class="line">    &#125;                          ⑥</span><br><span class="line">    // 锁定序号小的账户</span><br><span class="line">    synchronized(left)&#123;</span><br><span class="line">      // 锁定序号大的账户</span><br><span class="line">      synchronized(right)&#123; </span><br><span class="line">        if (this.balance &gt; amt)&#123;</span><br><span class="line">          this.balance -= amt;</span><br><span class="line">          target.balance += amt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1><span id="wait-notify">wait-notify</span></h1><p>如果线程要求的条件不满足，则线程阻塞自己，进入等待状态；当线程要求的条件满足后，通知等待的线程重新执行。</p>
<p>在 Java 语言里，等待 - 通知机制可以有多种实现方式，比如 Java 语言内置的 synchronized 配合 wait()、notify()、notifyAll() 这三个方法就能轻松实现</p>
<p>线程等待 wait 状态：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在并发程序中，当一个线程进入临界区后，由于某些条件不满足，需要进入等待状态，Java 对象的 wait() 方法就能够满足这种需求。如上图所示，当调用 wait() 方法后，当前线程就会被阻塞，并且进入到右边的等待队列中，这个等待队列也是互斥锁的等待队列。 线程在进入等待队列的同时，会释放持有的互斥锁，线程释放锁后，其他线程就有机会获得锁，并进入临界区了。</span><br></pre></td></tr></table></figure>

<p>线程通知机制：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">当条件满足时调用 notify()，会通知等待队列（互斥锁的等待队列）中的线程，告诉它条件曾经满足过。</span><br><span class="line"></span><br><span class="line">因为 notify() 只能保证在通知时间点，条件是满足的。而被通知线程的执行时间点和通知的时间点基本上不会重合，所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）。这一点你需要格外注意。</span><br><span class="line"></span><br><span class="line">除此之外，还有一个需要注意的点，被通知的线程要想重新执行，仍然需要获取到互斥锁（因为曾经获取的锁在调用 wait() 时已经释放了）</span><br><span class="line"></span><br><span class="line">wait() 和 notify() 是object 对象的方法, sleep是线程的静态方法</span><br></pre></td></tr></table></figure>

<img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/截屏2023-07-19 16.19.32.png">

<img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/截屏2023-07-19 16.19.40.png">





<h2><span id="usage">Usage</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class Allocator &#123;</span><br><span class="line">  private List&lt;Object&gt; als;</span><br><span class="line">  // 一次性申请所有资源</span><br><span class="line">  synchronized void apply(</span><br><span class="line">    Object from, Object to)&#123;</span><br><span class="line">    // 经典写法</span><br><span class="line">    while(als.contains(from) ||</span><br><span class="line">         als.contains(to))&#123;</span><br><span class="line">      try&#123;</span><br><span class="line">        wait();</span><br><span class="line">      &#125;catch(Exception e)&#123;</span><br><span class="line">      &#125;   </span><br><span class="line">    &#125; </span><br><span class="line">    als.add(from);</span><br><span class="line">    als.add(to);  </span><br><span class="line">  &#125;</span><br><span class="line">  // 归还资源</span><br><span class="line">  synchronized void free(</span><br><span class="line">    Object from, Object to)&#123;</span><br><span class="line">    als.remove(from);</span><br><span class="line">    als.remove(to);</span><br><span class="line">    notifyAll();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="others">Others</span></h2><h3><span id="wait-和-sleep-区别">wait 和 sleep 区别</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">1. wait会释放锁 而sleep不会释放锁资源(如果占有锁).</span><br><span class="line">2. wait只能在同步方法和同步块中使用，而sleep任何地方都可以.</span><br><span class="line">3. wait无需捕捉异常，而sleep需要.</span><br><span class="line">4. sleep是Thread的方法，而wait是Object类的方法</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1><span id="管程">管程</span></h1><p>Monitor，来自操作系统领域</p>
<p>所谓管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发。翻译为 Java 领域的语言，就是管理类的成员变量和成员方法，让这个类是线程安全的。</p>
<p><strong>互斥 和 同步</strong></p>
<p>在并发编程领域，有两大核心问题：</p>
<p>一个是互斥，即同一时刻只允许一个线程访问共享资源；</p>
<p>另一个是同步，即线程之间如何通信、协作。</p>
<p><strong>互斥解决思路</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来。</span><br><span class="line">假如我们要实现一个线程安全的阻塞队列，一个最直观的想法就是：</span><br><span class="line">将线程不安全的队列封装起来，对外提供线程安全的操作方法，例如入队操作和出队操作。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>同步解决思路：</strong></p>
<p>引入了条件变量的概念，而且每个条件变量都对应有一个等待队列</p>
<img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/截屏2023-07-19 16.57.00.png">



<p>假设有个线程 T1 执行阻塞队列的出队操作，执行出队操作，需要注意有个前提条件，就是阻塞队列不能是空的（空队列只能出 Null 值，是不允许的），阻塞队列不空这个前提条件对应的就是管程里的条件变量。 如果线程 T1 进入管程后恰好发现阻塞队列是空的，那怎么办呢？等待啊，去哪里等呢？就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待。</p>
<p>再假设之后另外一个线程 T2 执行阻塞队列的入队操作，入队操作执行成功之后，“阻塞队列不空”这个条件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进入到入口等待队列里面。</p>
<h2><span id="usage">usage</span></h2><h3><span id="code-test">Code Test</span></h3><p>对于阻塞队列的入队操作，如果阻塞队列已满，就需要等待直到阻塞队列不满，所以这里用了notFull.await();</p>
<p>对于阻塞出队操作，如果阻塞队列为空，就需要等待直到阻塞队列不空，所以就用了notEmpty.await();</p>
<p>如果入队成功，那么阻塞队列就不空了，就需要通知条件变量：阻塞队列不空notEmpty对应的等待队列。</p>
<p>如果出队成功，那就阻塞队列就不满了，就需要通知条件变量：阻塞队列不满notFull对应的等待队列。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">public class BlockedQueue&lt;T&gt;&#123;</span><br><span class="line">  final Lock lock =</span><br><span class="line">    new ReentrantLock();</span><br><span class="line">  // 条件变量：队列不满  </span><br><span class="line">  final Condition notFull =</span><br><span class="line">    lock.newCondition();</span><br><span class="line">  // 条件变量：队列不空  </span><br><span class="line">  final Condition notEmpty =</span><br><span class="line">    lock.newCondition();</span><br><span class="line"></span><br><span class="line">  // 入队</span><br><span class="line">  void enq(T x) &#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    try &#123;</span><br><span class="line">      while (队列已满)&#123;</span><br><span class="line">        // 等待队列不满 </span><br><span class="line">        notFull.await();</span><br><span class="line">      &#125;  </span><br><span class="line">      // 省略入队操作...</span><br><span class="line">      //入队后,通知可出队</span><br><span class="line">      notEmpty.signal();</span><br><span class="line">    &#125;finally &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 出队</span><br><span class="line">  void deq()&#123;</span><br><span class="line">    lock.lock();</span><br><span class="line">    try &#123;</span><br><span class="line">      while (队列已空)&#123;</span><br><span class="line">        // 等待队列不空</span><br><span class="line">        notEmpty.await();</span><br><span class="line">      &#125;</span><br><span class="line">      // 省略出队操作...</span><br><span class="line">      //出队后，通知可入队</span><br><span class="line">      notFull.signal();</span><br><span class="line">    &#125;finally &#123;</span><br><span class="line">      lock.unlock();</span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="others">Others</span></h2><p><strong>和 Java 内置的管程方案（synchronized） 对比</strong></p>
<p>Java 内置的管程方案（synchronized）使用简单，synchronized 关键字修饰的代码块，在编译期会自动生成相关加锁和解锁的代码，但是仅支持一个条件变量；而 Java SDK 并发包实现的管程支持多个条件变量，不过并发包里的锁，需要开发人员自己进行加锁和解锁操作。</p>
<h1><span id="thread">Thread</span></h1><h2><span id="状态"><strong>状态</strong></span></h2><p>1、NEW  初始化状态，刚创建未调用start方法</p>
<p>2、RUNNABLE  可执行状态，调用start 方法</p>
<p>3、Blocked  阻塞状态 </p>
<p>线程阻塞等待同步锁</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Thread state for a thread blocked waiting for a monitor lock.</span><br><span class="line">A thread in the blocked state is waiting for a monitor lock to enter a synchronized block/method or reenter a synchronized block/method after calling Object</span><br></pre></td></tr></table></figure>

<p>4、Waiting   无时限等待</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Object.wait with no timeout</span><br><span class="line">Thread.join with no timeout  等待一个线程执行完</span><br><span class="line">LockSupport.park</span><br></pre></td></tr></table></figure>

<p>5、Timed_Waiting   有时限等待</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Thread.sleep</span><br><span class="line">Object.wait with timeout</span><br><span class="line">Thread.join with timeout</span><br><span class="line">LockSupport.parkNanos</span><br><span class="line">LockSupport.parkUntil</span><br></pre></td></tr></table></figure>

<p>6、Terminated  终止状态</p>
<p>thread1.interrupt()  中断一个线程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 线程处于wait状态， 调用interrupt() 方法，线程的中断状态会被清除 并且 抛出异常InterruptedException</span><br><span class="line">If this thread is blocked in an invocation of the wait(), wait(long), or wait(long, int) methods of the Object class, or of the join(), join(long), join(long, int), sleep(long), or sleep(long, int), methods of this class, then its interrupt status will be cleared and it will receive an InterruptedException.</span><br><span class="line"></span><br><span class="line">If this thread is blocked in an I/O operation upon an InterruptibleChannel then the channel will be closed, the thread&#x27;s interrupt status will be set, and the thread will receive a java.nio.channels.ClosedByInterruptException.</span><br><span class="line"></span><br><span class="line">If this thread is blocked in a java.nio.channels.Selector then the thread&#x27;s interrupt status will be set and it will return immediately from the selection operation, possibly with a non-zero value, just as if the selector&#x27;s wakeup method were invoked.</span><br></pre></td></tr></table></figure>





<p><strong>线程数量</strong></p>
<p>对于 CPU 密集型的计算场景，理论上“线程的数量 &#x3D;CPU 核数”就是最合适的。不过在工程上，线程的数量一般会设置为“CPU 核数 +1”，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证 CPU 的利用率。</p>
<p>对于 I&#x2F;O 密集型计算场景，最佳的线程数是与程序中 CPU 计算和 I&#x2F;O 操作的耗时比相关的，我们可以总结出这样一个公式：最佳线程数 &#x3D;CPU 核数 * [ 1 +（I&#x2F;O 耗时 &#x2F; CPU 耗时）]</p>
<h2><span id="调用栈">调用栈</span></h2><p>每个线程都有自己的调用栈，线程调用一次方法，产生一个栈帧，压入调用栈。每个栈帧里都有对应方法需要的参数、局部变量、返回地址。当方法返回时，对应的栈帧从调用栈弹出。</p>
<img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/截屏2023-07-20 09.10.43.png">

<img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/截屏2023-07-20 09.10.52.png">







<h2><span id="others">Others</span></h2><p><strong>java 进程异常和 线程栈信息</strong></p>
<p>1、Java VisualVM</p>
<p>2、jstack 使用.  </p>
<p>查看所有线程dump信息， jstack  -l   23027  &gt;&gt; &#x2F;tmp&#x2F;23027.txt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">1.根据端口号57638查看对应进程pid/name=1463/java     </span><br><span class="line"></span><br><span class="line">   netstat -apn | grep  57638  </span><br><span class="line"></span><br><span class="line">tcp        0      0 ::ffff:172.20.5.10:57638    :::*                        LISTEN      1463/java           </span><br><span class="line">tcp        0      0 ::ffff:172.20.5.10:57638    ::ffff:172.20.5.14:59204    ESTABLISHED 1463/java</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. top 查看占用内存最多的几个进程</span><br><span class="line"></span><br><span class="line">ps aux | grep applicationName  查看对应applicationName的pid， 第二列表示pid 1463</span><br><span class="line"></span><br><span class="line">root      1463 12.5  2.3 5531468 3057636 ?     Sl   Oct12 315:22</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3. top  -p pid -H 查看进程pid对应的线程信息，这里每个线程映射对应到linux的一个进程PID</span><br><span class="line">top  -p 1463 -H</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                      </span><br><span class="line"> 1463 root      20   0 5401m 2.9g  39m S  0.0  2.3   0:00.00 java                                                                                          </span><br><span class="line"> 1464 root      20   0 5401m 2.9g  39m S  0.0  2.3   0:01.69 java</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">4.  使用jstack查看对应线程的具体信息</span><br><span class="line">jstack pid | grep -A 10 hex(id)</span><br><span class="line">pid进程id,  -A 表示 取出对应行后，往后继续显示10行内容， </span><br><span class="line">hex(id) 表示线程PID对应的十六进制数字，全部用小写字母表示，</span><br><span class="line">可以借助计算器 或其他工具进行转换， 用python 转换： python -c &quot;print (hex(1464))&quot;</span><br><span class="line">jstack 1463 | grep -A 10 `python -c &quot;print (hex(1464))&quot;`</span><br><span class="line"></span><br><span class="line">&quot;main&quot; #1 prio=5 os_prio=0 tid=0x00007f15ac01e800 nid=0x5b8 waiting on condition [0x00007f15b0a37000]</span><br><span class="line">   java.lang.Thread.State: TIMED_WAITING (parking)</span><br><span class="line">at sun.misc.Unsafe.park(Native Method)</span><br><span class="line">- parking to wait for  &lt;0x0000000700016ba0&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)</span><br><span class="line">at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)</span><br><span class="line">at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1465)</span><br><span class="line">at org.apache.spark.rpc.netty.Dispatcher.awaitTermination(Dispatcher.scala:180)</span><br><span class="line">at org.apache.spark.rpc.netty.NettyRpcEnv.awaitTermination(NettyRpcEnv.scala:273)</span><br><span class="line">at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:231)</span><br><span class="line">at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:67)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>3、<a target="_blank" rel="noopener" href="https://arthas.aliyun.com/doc/advanced-use.html#as-sh-%E5%92%8C-arthas-boot-%E6%8A%80%E5%B7%A7">Arthas</a></p>
<h1><span id="并发工具类">并发工具类</span></h1><h2><span id="reentrantlock">ReentrantLock</span></h2><p>重入锁</p>
<h3><span id="对比synchronized"><strong>对比synchronized</strong></span></h3><p>1、能够响应中断。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。</p>
<p>2、支持超时。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。</p>
<p>3、非阻塞地获取锁。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。</p>
<p>对应下面三个方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 支持中断的API</span><br><span class="line">void lockInterruptibly() </span><br><span class="line">  throws InterruptedException;</span><br><span class="line">// 支持超时的API</span><br><span class="line">boolean tryLock(long time, TimeUnit unit) </span><br><span class="line">  throws InterruptedException;</span><br><span class="line">// 支持非阻塞获取锁的API</span><br><span class="line">boolean tryLock();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="usage">Usage</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class X &#123;</span><br><span class="line">  private final Lock rtl =</span><br><span class="line">  new ReentrantLock();</span><br><span class="line">  int value;</span><br><span class="line">  public void addOne() &#123;</span><br><span class="line">    // 获取锁</span><br><span class="line">    rtl.lock();  </span><br><span class="line">    try &#123;</span><br><span class="line">      value+=1;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      // 保证锁能释放</span><br><span class="line">      rtl.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="特点">特点</span></h3><h4><span id="可重入">可重入</span></h4><p>线程可以重复获取同一把锁。</p>
<p>例如下面代码中，当线程 T1 执行到 ① 处时，已经获取到了锁 rtl ，当在 ① 处调用 get() 方法时，会在 ② 再次对锁 rtl 执行加锁操作。此时，如果锁 rtl 是可重入的，那么线程 T1 可以再次加锁成功；如果锁 rtl 是不可重入的，那么线程 T1 此时会被阻塞。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class X &#123;</span><br><span class="line">  private final Lock rtl =</span><br><span class="line">  new ReentrantLock();</span><br><span class="line">  int value;</span><br><span class="line">  public int get() &#123;</span><br><span class="line">    // 获取锁</span><br><span class="line">    rtl.lock();         ②</span><br><span class="line">    try &#123;</span><br><span class="line">      return value;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      // 保证锁能释放</span><br><span class="line">      rtl.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  public void addOne() &#123;</span><br><span class="line">    // 获取锁</span><br><span class="line">    rtl.lock();  </span><br><span class="line">    try &#123;</span><br><span class="line">      value = 1 + get(); ①</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      // 保证锁能释放</span><br><span class="line">      rtl.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="公平锁">公平锁</span></h4><p>如果一个线程没有获得锁，就会进入等待队列，当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。如果是公平锁，唤醒的策略就是谁等待的时间长，就唤醒谁，很公平；如果是非公平锁，则不提供这个公平保证，有可能等待时间短的线程反而先被唤醒。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">//无参构造函数：默认非公平锁</span><br><span class="line">public ReentrantLock() &#123;</span><br><span class="line">    sync = new NonfairSync();</span><br><span class="line">&#125;</span><br><span class="line">//根据公平策略参数创建锁</span><br><span class="line">public ReentrantLock(boolean fair)&#123;</span><br><span class="line">    sync = fair ? new FairSync() </span><br><span class="line">                : new NonfairSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="others">Others</span></h3><h4><span id="同步和异步">同步和异步</span></h4><p>通俗点来讲就是调用方是否需要等待结果，如果需要等待结果，就是同步；如果不需要等待结果，就是异步。</p>
<p><strong>同步和异步：通常用来形容一次方法调用</strong>，针对的方法调用。同步方法调用一旦开始，调用者必须等到方法调用返回后，才能继续后面的行为；异步方法调用更像是一个消息传递，一旦开始，方法调用就会立即返回，调用者就可以继续后续的操作。再说的通俗一点，假设一段代码有两个方法：方法f1和方法f2，f1调用f2,<br>如果当f1调用f2后，f1必须等待f2完整执行完后，才能继续执行，那么这整个过程就是同步的；<br>如果当f1调用f2后, f2立即返回一个结果给f1(而实际f2内部可能并未执行完), f1收到f2返回的结果后, 继续执行后续操作，这整个过程可以认为是异步的, 和同步的区别在于 f1不需要真正等f2执行完，再去执行后续操作。  </p>
<p><strong>临界区用来表示一种公共资源或者说是共享数据，可以被多个线程使用，但是每一次</strong>，只能有一个线程使用它，一旦临界区资源被占用, 其他线程要想使用这个资源，就必须等待。<br><strong>阻塞和非阻塞 通常用来形容多线程之间的相互影响</strong>，指的是线程的状态。比如一个线程占用了临界区资源，那么所有需要这个资源的线程就必须等待，等待会导致线程挂起，这种情况就是阻塞。  </p>
<p><strong>同步阻塞io</strong> 小明去餐馆吃饭，叫了一个服务员，点了一份饭，之后小明一直在餐馆前台等待，在等待期间，小明什么事情也不能干(可能没带手机或者手机没电了，这里只是做了一个假设)，最后饭好了，服务员把饭给到小明，小明才离开餐馆。在这整个过程中，小明点餐可以看成是一个方法f1, 服务员通知后台厨房做饭可以看成是方法f2, f1调用了f2，由于f1必须等f2完整执行完才能继续执行，因此是同步的。同时, 由于f1在等待期间(出于各种各样原因被限制，只能等待)不能做别的任何事情，因此f1对应线程(调用f1的线程)是阻塞的。因此，整体来看，整个过程是同步阻塞的。  </p>
<p><strong>同步非阻塞nio</strong> 小明去餐馆吃饭，叫了一个服务员，点了一份饭，之后小明离开餐馆去做别的事情了，过了一段时间，小明回到餐馆前台询问饭是否做好了，如果饭好了，服务员把饭给到小明，小明就离开餐馆(如果饭没好，小明可以继续去干别的事情，过一段时间，再过来问一下，最后结果肯定饭会做好的)。在这整个过程中，小明点餐可以看成是一个方法f1, 服务员通知后台厨房做饭可以看成是方法f2, f1调用了f2，但是f1 不必等待 f2完整执行完才能继续执行(在这里，f1调用f2后，f2开始执行，同时f2立即返回一个结果给到f1, f1马上继续执行，基本上没有针对f2做任何等待。这里的例子可以看成是 f1调用f2后，f2执行的操作是后台厨房开始做饭，f2立即返回的结果对应的是一个票据，一个点餐号码，结果给到f1后，f1继续去做别的事情，同时f2也在继续做着自己的事情)。<strong>这里似乎好像是一个异步方法调用</strong>，其实不然，由于f1需要过一段时间，主动查询查询f2的结果(饭是否做好了)，因此仍然是同步的。同时，由于f1在等待期间可以继续去做别的事情(小明离开餐馆去做别的事情了)，因此f1对应线程(调用f1的线程)是非阻塞的，并没有处于阻塞等待的状态。因此，从整体来看，整个过程是同步非阻塞的。 <strong>相比同步阻塞，</strong>同步非阻塞其实也是一种等待，但是这里的等待并不是真的傻等下去，对应调用方线程调用完之后，可以继续执行下去，去干别的事情，过了一段时间，自己再去主动查看被调用方是否真的执行完了；而在同步阻塞中，对应调用方线程调用完之后，是一直在傻傻地等下去，也不去干别的事情，直到被调用方完整执行完， 对应调用方线程才往下继续执行。  </p>
<p><strong>异步非阻塞aio</strong> 小明去餐馆吃饭，叫了一个服务员，点了一份饭，之后小明离开餐馆去做别的事情了，过了一段时间，饭做好了，服务员通知小明(打电话或者其他各种通讯方式)来拿饭，小明获得通知后，去取饭，最后离开餐馆。和同步非阻塞nio的区别在于, 在同步非阻塞中，调用方f1仍然是主动获取到调用方f2的结果(f1过了一段时间主动查询f1), 对应例子就是 小明是自己主动去获取饭做好这条消息的(离开餐馆一段时间，自己再返回餐馆，主动询问服务员饭是否做好)，而在这里，异步非阻塞，调用方f1是被动获取到调用方f2的结果(f2完整执行完后，主动通知到f1自己的最终执行结果)，对应例子就是 小明是被动获取到饭做好这条消息的(离开餐馆一段时间后，服务员这边会给小明一个通知，饭做好了)。和同步非阻塞nio的相同点在于，对应线程(调用方法f1)都是非阻塞的，调用方对应线程 并不需要等待被调用发完全执行完，才去干别的事情。<br>平时去餐馆吃饭， 大部分情况下对应的是异步非阻塞了，当然了，其他两种情况也是有可能出现的。也有另外一种说法，<a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiexj/p/6874654.html">NIO相当于餐做好了自己去取，AIO相当于送餐上门。</a>这里其实是一致的，前者是主动拉取结果，而后者是被动获得通知，拿到结果。  </p>
<h2><span id="semaphore">Semaphore</span></h2><p>信号量模型是由 <strong>java.util.concurrent.Semaphore</strong> 实现的</p>
<p>semaphore 控制最多有多少个线程可以同时访问临界区.  acquire 和release 分别对应 down() - 和up() +</p>
<h3><span id="usage">Usage</span></h3><h4><span id="互斥锁">互斥锁</span></h4><p>作为互斥锁，保证临界区只有一个线程能够访问</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">static int count;</span><br><span class="line">//初始化信号量</span><br><span class="line">static final Semaphore s </span><br><span class="line">    = new Semaphore(1);</span><br><span class="line">//用信号量保证互斥    </span><br><span class="line">static void addOne() &#123;</span><br><span class="line">  s.acquire();</span><br><span class="line">  try &#123;</span><br><span class="line">    count+=1;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    s.release();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="限流器">限流器</span></h4><p>限制最多有count个线程同时访问临界区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import java.util.concurrent.Semaphore;</span><br><span class="line"></span><br><span class="line">//控制可以同时访问的线程个数</span><br><span class="line">//https://www.cnblogs.com/dolphin0520/p/3920397.html</span><br><span class="line">public class TestSemaphore &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        int N = 8;            //工人数</span><br><span class="line">        Semaphore semaphore = new Semaphore(3); //机器数目</span><br><span class="line">        for(int i=0;i&lt;N;i++)</span><br><span class="line">            new Worker(i,semaphore).start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class Worker extends Thread&#123;</span><br><span class="line">        private int num;</span><br><span class="line">        private Semaphore semaphore;</span><br><span class="line">        public Worker(int num,Semaphore semaphore)&#123;</span><br><span class="line">            this.num = num;</span><br><span class="line">            this.semaphore = semaphore;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void run() &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                semaphore.acquire();</span><br><span class="line">                System.out.println(&quot;工人&quot;+this.num+&quot;占用一个机器在生产...&quot;);</span><br><span class="line">                Thread.sleep(2000);</span><br><span class="line">                System.out.println(&quot;工人&quot;+this.num+&quot;释放出机器&quot;);</span><br><span class="line">                semaphore.release();</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="reentrantreadwritelock">ReentrantReadWriteLock</span></h2><p><strong>针对读多写少这种并发场景</strong>，Java SDK 并发包提供了读写锁——ReadWriteLock（实现类<strong>ReentrantReadWriteLock</strong>），非常容易使用，并且性能很好。</p>
<p>读写锁都遵守以下三条基本原则：</p>
<p>允许多个线程同时读共享变量；</p>
<p>只允许一个线程写共享变量；</p>
<p>如果一个写线程正在执行写操作，此时禁止读线程读共享变量。</p>
<p><strong>读写锁与互斥锁</strong>的一个重要区别就是读写锁<strong>允许多个线程同时读共享变</strong>量，而互斥锁是不允许的，这是读写锁在读多写少场景下性能优于互斥锁的关键。但读写锁的写操作是互斥的，当一个线程在写共享变量的时候，是不允许其他线程执行写操作和读操作。</p>
<h3><span id="usage">Usage</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class Cache&lt;K,V&gt; &#123;</span><br><span class="line">  final Map&lt;K, V&gt; m =</span><br><span class="line">    new HashMap&lt;&gt;();</span><br><span class="line">  final ReadWriteLock rwl =</span><br><span class="line">    new ReentrantReadWriteLock();</span><br><span class="line">  // 读锁</span><br><span class="line">  final Lock r = rwl.readLock();</span><br><span class="line">  // 写锁</span><br><span class="line">  final Lock w = rwl.writeLock();</span><br><span class="line">  // 读缓存</span><br><span class="line">  V get(K key) &#123;</span><br><span class="line">    r.lock();</span><br><span class="line">    try &#123; return m.get(key); &#125;</span><br><span class="line">    finally &#123; r.unlock(); &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 写缓存</span><br><span class="line">  V put(K key, V value) &#123;</span><br><span class="line">    w.lock();</span><br><span class="line">    try &#123; return m.put(key, v); &#125;</span><br><span class="line">    finally &#123; w.unlock(); &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="stampedlock">StampedLock</span></h2><p>支持三种模式：写锁、悲观读锁、乐观读</p>
<h3><span id="写锁-悲观读锁">写锁、悲观读锁</span></h3><p>写锁、悲观读锁 和ReadWriteLock 功能基本一致，</p>
<p>不同的是：StampedLock 里的写锁和悲观读锁加锁成功之后，都会返回一个 stamp；然后解锁的时候，需要传入这个 stamp</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">final StampedLock sl = </span><br><span class="line">  new StampedLock();</span><br><span class="line">  </span><br><span class="line">// 获取/释放悲观读锁示意代码</span><br><span class="line">long stamp = sl.readLock();</span><br><span class="line">try &#123;</span><br><span class="line">  //省略业务相关代码</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">  sl.unlockRead(stamp);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 获取/释放写锁示意代码</span><br><span class="line">long stamp = sl.writeLock();</span><br><span class="line">try &#123;</span><br><span class="line">  //省略业务相关代码</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">  sl.unlockWrite(stamp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="乐观读">乐观读</span></h3><p>乐观读这个操作是无锁的，所以相比较 ReadWriteLock 的读锁，乐观读的性能更好一些。</p>
<p>乐观读：判断是不是有写操作正在进行，如果正在进行写，就等待写完成，或者升级为 悲观读锁</p>
<h4><span id="经典使用方式">经典使用方式</span></h4><p>乐观读 升级为悲观读锁：</p>
<p>首先通过调用 tryOptimisticRead() 获取了一个 stamp，这里的 tryOptimisticRead() 就是我们前面提到的乐观读。之后将共享变量 x 和 y 读入方法的局部变量中，不过需要注意的是，由于 tryOptimisticRead() 是无锁的，所以共享变量 x 和 y 读入方法局部变量时，x 和 y 有可能被其他线程修改了。因此最后读完之后，还需要再次验证一下是否存在写操作，这个验证操作是通过调用 validate(stamp) 来实现的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class Point &#123;</span><br><span class="line">  private int x, y;</span><br><span class="line">  final StampedLock sl = </span><br><span class="line">    new StampedLock();</span><br><span class="line">  //计算到原点的距离  </span><br><span class="line">  int distanceFromOrigin() &#123;</span><br><span class="line">    // 乐观读</span><br><span class="line">    long stamp = </span><br><span class="line">      sl.tryOptimisticRead();</span><br><span class="line">    // 读入局部变量，</span><br><span class="line">    // 读的过程数据可能被修改</span><br><span class="line">    int curX = x, curY = y;</span><br><span class="line">    //判断执行读操作期间，</span><br><span class="line">    //是否存在写操作，如果存在，</span><br><span class="line">    //则sl.validate返回false</span><br><span class="line">    if (!sl.validate(stamp))&#123;</span><br><span class="line">      // 升级为悲观读锁</span><br><span class="line">      stamp = sl.readLock();</span><br><span class="line">      try &#123;</span><br><span class="line">        curX = x;</span><br><span class="line">        curY = y;</span><br><span class="line">      &#125; finally &#123;</span><br><span class="line">        //释放悲观读锁</span><br><span class="line">        sl.unlockRead(stamp);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return Math.sqrt(</span><br><span class="line">      curX * curX + curY * curY);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="总结">总结</span></h3><h4><span id="stampedlock-读模板">StampedLock 读模板</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">final StampedLock sl = </span><br><span class="line">  new StampedLock();</span><br><span class="line"></span><br><span class="line">// 乐观读</span><br><span class="line">long stamp = </span><br><span class="line">  sl.tryOptimisticRead();</span><br><span class="line">// 读入方法局部变量</span><br><span class="line">......</span><br><span class="line">// 校验stamp</span><br><span class="line">//是否存在写操作，如果存在，</span><br><span class="line">//则sl.validate返回false</span><br><span class="line">if (!sl.validate(stamp))&#123;</span><br><span class="line">  // 升级为悲观读锁</span><br><span class="line">  stamp = sl.readLock();</span><br><span class="line">  try &#123;</span><br><span class="line">    // 读入方法局部变量</span><br><span class="line">    .....</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    //释放悲观读锁</span><br><span class="line">    sl.unlockRead(stamp);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">//使用方法局部变量执行业务操作</span><br><span class="line">......</span><br></pre></td></tr></table></figure>



<h4><span id="写模版">写模版</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">long stamp = sl.writeLock();</span><br><span class="line">try &#123;</span><br><span class="line">  // 写共享变量</span><br><span class="line">  ......</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">  sl.unlockWrite(stamp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="线程协调">线程协调</span></h2><h3><span id="countdownlatch">CountDownLatch</span></h3><p>倒计数器锁</p>
<h4><span id="场景">场景</span></h4><p>主要用来解决一个线程等待多个线程的场景</p>
<p>可以类比旅游团团长要等待所有的游客到齐才能去下一个景点；</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* A synchronization aid that allows one or more threads to wait until</span><br><span class="line">* a set of operations being performed in other threads completes.</span><br></pre></td></tr></table></figure>



<p>一个线程要等待n个线程执行完之后，才继续执行；</p>
<p>初始化设置计数器的值为n，</p>
<p>对应执行的每个线程调用countDown()方法，计数器的值-1，</p>
<p>对应等待的线程调用await()，当计数器的值&#x3D;0，等待线程继续执行后续的代码。</p>
<p>类似生活中的例子：旅游团团长要等待所有的游客到齐才能去下一个景点</p>
<h4><span id="example">Example</span></h4><p>前几天老板突然匆匆忙忙过来，说对账系统最近越来越慢了，能不能快速优化一下。我了解了对账系统的业务后，发现还是挺简单的，用户通过在线商城下单，会生成电子订单，保存在订单库；之后物流会生成派送单给用户发货，派送单保存在派送单库。为了防止漏派送或者重复派送，对账系统每天还会校验是否存在异常订单。</p>
<p>对账系统的处理逻辑很简单，你可以参考下面的对账系统流程图。目前对账系统的处理逻辑是首先查询订单，然后查询派送单，之后对比订单和派送单，将差异写入差异库。</p>
<img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/19_1屏幕快照 2021-12-11 下午12.19.30.png">



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">while(存在未对账订单)&#123;</span><br><span class="line">  // 查询未对账订单</span><br><span class="line">  pos = getPOrders();</span><br><span class="line">  // 查询派送单</span><br><span class="line">  dos = getDOrders();</span><br><span class="line">  // 执行对账操作</span><br><span class="line">  diff = check(pos, dos);</span><br><span class="line">  // 差异写入差异库</span><br><span class="line">  save(diff);</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>



<p><strong>并行化提高性能</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">while(存在未对账订单)&#123;</span><br><span class="line">  // 查询未对账订单</span><br><span class="line">  Thread T1 = new Thread(()-&gt;&#123;</span><br><span class="line">    pos = getPOrders();</span><br><span class="line">  &#125;);</span><br><span class="line">  T1.start();</span><br><span class="line">  // 查询派送单</span><br><span class="line">  Thread T2 = new Thread(()-&gt;&#123;</span><br><span class="line">    dos = getDOrders();</span><br><span class="line">  &#125;);</span><br><span class="line">  T2.start();</span><br><span class="line">  // 等待T1、T2结束</span><br><span class="line">  T1.join();</span><br><span class="line">  T2.join();</span><br><span class="line">  // 执行对账操作</span><br><span class="line">  diff = check(pos, dos);</span><br><span class="line">  // 差异写入差异库</span><br><span class="line">  save(diff);</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>





<p>while 循环里面每次都会创建新的线程，而创建线程可是个耗时的操作。所以最好是创建出来的线程能够循环利用，估计这时你已经想到线程池了，是的，线程池就能解决这个问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 创建2个线程的线程池</span><br><span class="line">Executor executor = </span><br><span class="line">  Executors.newFixedThreadPool(2);</span><br><span class="line">while(存在未对账订单)&#123;</span><br><span class="line">  // 查询未对账订单</span><br><span class="line">  executor.execute(()-&gt; &#123;</span><br><span class="line">    pos = getPOrders();</span><br><span class="line">  &#125;);</span><br><span class="line">  // 查询派送单</span><br><span class="line">  executor.execute(()-&gt; &#123;</span><br><span class="line">    dos = getDOrders();</span><br><span class="line">  &#125;);</span><br><span class="line">  </span><br><span class="line">  /* ？？如何实现等待？？*/</span><br><span class="line">  </span><br><span class="line">  // 执行对账操作</span><br><span class="line">  diff = check(pos, dos);</span><br><span class="line">  // 差异写入差异库</span><br><span class="line">  save(diff);</span><br><span class="line">&#125;   </span><br></pre></td></tr></table></figure>



<p>CountDownLatch 设置主线程等待</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 创建2个线程的线程池</span><br><span class="line">Executor executor = </span><br><span class="line">  Executors.newFixedThreadPool(2);</span><br><span class="line">while(存在未对账订单)&#123;</span><br><span class="line">  // 计数器初始化为2</span><br><span class="line">  CountDownLatch latch = </span><br><span class="line">    new CountDownLatch(2);</span><br><span class="line">  // 查询未对账订单</span><br><span class="line">  executor.execute(()-&gt; &#123;</span><br><span class="line">    pos = getPOrders();</span><br><span class="line">    latch.countDown();</span><br><span class="line">  &#125;);</span><br><span class="line">  // 查询派送单</span><br><span class="line">  executor.execute(()-&gt; &#123;</span><br><span class="line">    dos = getDOrders();</span><br><span class="line">    latch.countDown();</span><br><span class="line">  &#125;);</span><br><span class="line">  </span><br><span class="line">  // 等待两个查询操作结束</span><br><span class="line">  latch.await();</span><br><span class="line">  </span><br><span class="line">  // 执行对账操作</span><br><span class="line">  diff = check(pos, dos);</span><br><span class="line">  // 差异写入差异库</span><br><span class="line">  save(diff);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3><span id="cyclicbarrier">CyclicBarrier</span></h3><h4><span id="场景">场景</span></h4><p>是一组线程之间互相等待，更像是几个驴友之间不离不弃。</p>
<p>CyclicBarrier 的计数器是可以循环利用的，而且具备自动重置的功能，一旦计数器减到 0 会自动重置到你设置的初始值。除此之外，CyclicBarrier 还可以设置回调函数</p>
<p>循环栅栏</p>
<p>n个线程之间互相等待，每个线程调用await()方法之后，计数器值-1，进入阻塞(休眠状态)，直到最后一个线程调用await()，计数器的值为0，所有线程结束等待，之后计数器的值被重置为n；</p>
<p>此外，如何构造方法里含有回调函数，在一个 回合(如果是循环执行) 里最后执行await()的线程上 会继续执行回调函数，执行完回调函数，所有线程不再阻塞等待，之后计数器的值被重置为n；</p>
<h4><span id="example">Example</span></h4><p>CyclicBarrier 进一步并行化，实现 查询未对账订单|查询派送单  和对账操作并行化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 订单队列</span><br><span class="line">Vector&lt;P&gt; pos;</span><br><span class="line">// 派送单队列</span><br><span class="line">Vector&lt;D&gt; dos;</span><br><span class="line">// 执行回调的线程池 </span><br><span class="line">Executor executor = </span><br><span class="line">  Executors.newFixedThreadPool(1);</span><br><span class="line">  </span><br><span class="line">  //注意回调函数是异步的，不需要真实的线程执行</span><br><span class="line">final CyclicBarrier barrier =</span><br><span class="line">  new CyclicBarrier(2, ()-&gt;&#123;</span><br><span class="line">    executor.execute(()-&gt;check());</span><br><span class="line">  &#125;);</span><br><span class="line">  </span><br><span class="line">void check()&#123;</span><br><span class="line">  P p = pos.remove(0);</span><br><span class="line">  D d = dos.remove(0);</span><br><span class="line">  // 执行对账操作</span><br><span class="line">  diff = check(p, d);</span><br><span class="line">  // 差异写入差异库</span><br><span class="line">  save(diff);</span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line">void checkAll()&#123;</span><br><span class="line">  // 循环查询订单库</span><br><span class="line">  Thread T1 = new Thread(()-&gt;&#123;</span><br><span class="line">    while(存在未对账订单)&#123;</span><br><span class="line">      // 查询订单库</span><br><span class="line">      pos.add(getPOrders());</span><br><span class="line">      // 等待</span><br><span class="line">      barrier.await();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  T1.start();  </span><br><span class="line">  // 循环查询运单库</span><br><span class="line">  Thread T2 = new Thread(()-&gt;&#123;</span><br><span class="line">    while(存在未对账订单)&#123;</span><br><span class="line">      // 查询运单库</span><br><span class="line">      dos.add(getDOrders());</span><br><span class="line">      // 等待</span><br><span class="line">      barrier.await();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  T2.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ol>
<li>回调函数使用线程池是为了异步操作，否则回掉函数是同步调用的，也就是本次对账操作执行完才能进行下一轮的检查。</li>
<li>线程数量固定为1，防止了多线程并发导致的数据不一致，因为订单和派送单是两个队列，只有单线程去两个队列中取消息才不会出现消息不匹配的问题。</li>
</ol>
<p>进一步简化：</p>
<h5><span id="使用completablefuture-异步执行">使用CompletableFuture 异步执行</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CompletableFuture&lt;List&gt; pOrderFuture = CompletableFuture.supplyAsync(this::getPOrders);</span><br><span class="line">CompletableFuture&lt;List&gt; dOrderFuture = CompletableFuture.supplyAsync(this::getDOrders);</span><br><span class="line">pOrderFuture.thenCombine(dOrderFuture, this::check)</span><br><span class="line">            .thenAccept(this::save);</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="容器">容器</span></h2><h3><span id="同步容器">同步容器</span></h3><p>ArrayList、HashSet、HashMap 都不是线程安全的</p>
<p>基于synchronized 同步关键字实现，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">List list = Collections.</span><br><span class="line">  synchronizedList(new ArrayList());</span><br><span class="line">Set set = Collections.</span><br><span class="line">  synchronizedSet(new HashSet());</span><br><span class="line">Map map = Collections.</span><br><span class="line">  synchronizedMap(new HashMap());</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其他基于 synchronized 实现的容器：</p>
<p>Vector、Stack、HashTable</p>
<h4><span id="迭代器">迭代器</span></h4><p>迭代器遍历容器存在线程安全问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">List list = Collections.</span><br><span class="line">  synchronizedList(new ArrayList());</span><br><span class="line">Iterator i = list.iterator(); </span><br><span class="line">while (i.hasNext())</span><br><span class="line">  foo(i.next());</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  =》 正确的做法</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">List list = Collections.</span><br><span class="line">  synchronizedList(new ArrayList());</span><br><span class="line">synchronized (list) &#123;  </span><br><span class="line">  Iterator i = list.iterator(); </span><br><span class="line">  while (i.hasNext())</span><br><span class="line">    foo(i.next());</span><br><span class="line">&#125;    </span><br><span class="line">  </span><br></pre></td></tr></table></figure>





<p>Java 在 1.5 版本之前所谓的线程安全的容器，主要指的就是同步容器；</p>
<p>不过同步容器有个最大的问题，那就是性能差，所有方法都用 synchronized 来保证互斥，串行度太高了。因此 Java 在 1.5 及之后版本提供了性能更高的容器，我们一般称为并发容器。</p>
<h3><span id="并发容器">并发容器</span></h3><h4><span id="copyonwritearraylist">CopyOnWriteArrayList</span></h4><p>每次写入新的元素都会copy 原数组，读写可以并行，</p>
<p>遍历操作一直都是基于原 array 执行，而写操作则是基于新 array 进行。</p>
<h5><span id="注意">注意</span></h5><p>使用 CopyOnWriteArrayList 需要注意的“坑”主要有两个方面。</p>
<p>一个是应用场景，CopyOnWriteArrayList 仅适用于写操作非常少的场景，而且能够容忍读写的短暂不一致。例如上面的例子中，写入的新元素并不能立刻被遍历到。</p>
<p>另一个需要注意的是，CopyOnWriteArrayList 迭代器是只读的，不支持增删改。因为迭代器遍历的仅仅是一个快照，而对快照进行增删改是没有意义的。</p>
<h4><span id="set">Set</span></h4><p>CopyOnWriteArraySet 和 ConcurrentSkipListSet</p>
<h4><span id="map">Map</span></h4><h5><span id="hashmap">HashMap</span></h5><p>线程不安全，1.8版本的hashmap 底层基于数组 + 链表 + 红黑树 的实现方式，当相同hashcode 的key对应链表很大时，链表自动转为红黑树，相对1.7版本，性能有了一定提高；</p>
<h5><span id="treemap">TreeMap</span></h5><p>不安全，底层基于红黑树，实现了SortedMap。是一个有序map, 元素插入map时，会按照key的字典序 或者 用户定义的key 比较器Comparator 进行排序。</p>
<h5><span id="linkedhashmap">LinkedHashMap</span></h5><p>不安全，有序。 在hashmap 基础上，维护了一个双向链表， 保持插入元素的顺序性。</p>
<p>链表维护了元素插入map的顺序，也就是说，</p>
<p><strong>该map是有序的，其顺序等于元素插入map的顺序</strong>，通常可以用来实现一个LRU(Least Recently Used)缓存。</p>
<p>LRU缓存：当缓存里存放的数据个数超过规定个数后，就把最不常用的移除掉. [LRU缓存的实现](</p>
<h5><span id="hashtable">HashTable</span></h5><p>安全</p>
<p>Hashtable在hashMap基础上，每个method前面都加上了synchronized方法，因此是线程安全的，但是进行多线程读写时，所有读写操作串行化，因此性能不高；</p>
<h5><span id="concurrenthashmap">ConcurrentHashMap</span></h5><p>安全</p>
<p>1.7版本通过分段锁segment Reentrantlock，将数据划分为多个segment，每个segment对应一个Reentrantlock，这样就实现并发地读写操作；1.8版本不再使用分段锁，通过使用CAS和Synchronized同步锁，实现并发操作。</p>
<h5><span id="concurrentskiplistmap">ConcurrentSkipListMap</span></h5><p>安全</p>
<p>基于skiplist 跳表实现，</p>
<p>跳表：SkipList 本身就是一种数据结构，插入、删除、查询操作平均的时间复杂度是 O(log n)，理论上和并发线程数没有关系，所以在并发度非常高的情况下，若你对 ConcurrentHashMap 的性能还不满意，可以尝试一下 ConcurrentSkipListMap。</p>
<h4><span id="queue">Queue</span></h4><p>Java 并发包里面 Queue 这类并发容器是最复杂的，你可以从以下两个维度来分类。一个维度是阻塞与非阻塞，所谓阻塞指的是当队列已满时，入队操作阻塞；当队列已空时，出队操作阻塞。另一个维度是单端与双端，单端指的是只能队尾入队，队首出队；而双端指的是队首队尾皆可入队出队。Java 并发包里阻塞队列都用 Blocking 关键字标识，单端队列使用 Queue 标识，双端队列使用 Deque 标识。</p>
<p>单端阻塞队列: ArrayBlockingQueue、LinkedBlockingQueue</p>
<p>双端阻塞队列: LinkedBlockingDeque</p>
<p>单端非阻塞队列: ConcurrentLinkedQueue</p>
<p>双端非阻塞队列: ConcurrentLinkedDeque</p>
<h5><span id="有界队列">有界队列</span></h5><p>使用队列时，需要格外注意队列是否支持有界（所谓有界指的是内部的队列是否有容量限制）。实际工作中，一般都不建议使用无界的队列，因为数据量大了之后很容易导致 OOM。上面我们提到的这些 Queue 中，只有 ArrayBlockingQueue 和 LinkedBlockingQueue 是支持有界的，所以在使用其他无界队列时，一定要充分考虑是否存在导致 OOM 的隐患。</p>
<h2><span id="cas">CAS</span></h2><p>无锁，使用原子类</p>
<p>java.util.concurrent.atomic 包</p>
<h3><span id="usage">Usage</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">public class Test &#123;</span><br><span class="line">  AtomicLong count = </span><br><span class="line">    new AtomicLong(0);</span><br><span class="line">  void add10K() &#123;</span><br><span class="line">    int idx = 0;</span><br><span class="line">    while(idx++ &lt; 10000) &#123;</span><br><span class="line">      count.getAndIncrement();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="线程池">线程池</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ThreadPoolExecutor(</span><br><span class="line">  int corePoolSize,</span><br><span class="line">  int maximumPoolSize,</span><br><span class="line">  long keepAliveTime,</span><br><span class="line">  TimeUnit unit,</span><br><span class="line">  BlockingQueue&lt;Runnable&gt; workQueue,</span><br><span class="line">  ThreadFactory threadFactory,</span><br><span class="line">  RejectedExecutionHandler handler) </span><br></pre></td></tr></table></figure>

<p>参数解析： 7个参数</p>
<p>corePoolSize ：核心线程池数量，表示线程池保有的最小线程数</p>
<p>maximumPoolSize： 表示线程池创建的最大线程数</p>
<p>keepAliveTime + unit:   空闲状态的线程允许存活的时间</p>
<p>workQueue：工作队列，用户提交的任务会先放置在该队列中</p>
<p>threadFactory：通过这个参数你可以自定义如何创建线程，例如可以给线程指定一个有意义的名字。</p>
<p>handler：自定义任务的拒绝策略。如果线程池中所有的线程都在忙碌，并且工作队列也满了（前提是工作队列是有界队列），那么此时提交任务，线程池就会拒绝接收。</p>
<p><strong>ThreadPoolExecutor 已经提供了以下 4 种策略</strong>：</p>
<p>CallerRunsPolicy：提交任务的线程自己去执行该任务。</p>
<p>AbortPolicy：默认的拒绝策略，会 throws RejectedExecutionException。</p>
<p>DiscardPolicy：直接丢弃任务，没有任何异常抛出。</p>
<p>DiscardOldestPolicy：丢弃最老的任务，其实就是把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列。</p>
<h3><span id="调用">调用</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// 无返回结果</span><br><span class="line">void execute(Runnable command) </span><br><span class="line"></span><br><span class="line">// 提交Runnable任务</span><br><span class="line">Future&lt;?&gt; </span><br><span class="line">  submit(Runnable task);</span><br><span class="line">  </span><br><span class="line">// 提交Callable任务</span><br><span class="line">&lt;T&gt; Future&lt;T&gt; </span><br><span class="line">  submit(Callable&lt;T&gt; task);</span><br><span class="line">  </span><br><span class="line">// 提交Runnable任务及结果引用  </span><br><span class="line">&lt;T&gt; Future&lt;T&gt; </span><br><span class="line">  submit(Runnable task, T result);</span><br></pre></td></tr></table></figure>

<p><strong>submit 三个方法的区别：</strong></p>
<p>1、提交 Runnable 任务 submit(Runnable task) ：这个方法的参数是一个 Runnable 接口，Runnable 接口的 run() 方法是没有返回值的，所以 submit(Runnable task) 这个方法返回的 Future 仅可以用来断言任务已经结束了，类似于 Thread.join()。</p>
<p>2、提交 Callable 任务 submit(Callable task)：这个方法的参数是一个 Callable 接口，它只有一个 call() 方法，并且这个方法是有返回值的，所以这个方法返回的 Future 对象可以通过调用其 get() 方法来获取任务的执行结果。</p>
<p>3、提交 Runnable 任务及结果引用 submit(Runnable task, T result)：这个方法很有意思，假设这个方法返回的 Future 对象是 f，f.get() 的返回值就是传给 submit() 方法的参数 result。 </p>
<p>（在实现Runnable 接口的类对应的构造函数里 传入变量T，在run方法中对变量进行操作）</p>
<p>需要你注意的是 Runnable 接口的实现类 Task 声明了一个有参构造函数 Task(Result r) ，创建 Task 对象的时候传入了 result 对象，这样就能在类 Task 的 run() 方法中对 result 进行各种操作了。result 相当于主线程和子线程之间的桥梁，通过它主子线程可以共享数据。</p>
<p>方法三使用测试：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">import java.util.concurrent.ExecutorService;</span><br><span class="line">import java.util.concurrent.Executors;</span><br><span class="line">import java.util.concurrent.Future;</span><br><span class="line"></span><br><span class="line">public class ThreadPoolTestReturn &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        ExecutorService executor</span><br><span class="line">                = Executors.newFixedThreadPool(1);</span><br><span class="line">// 创建Result对象r</span><br><span class="line">        Result r = new Result();</span><br><span class="line">        r.setAaa(&quot;aaa&quot;);</span><br><span class="line"></span><br><span class="line">        System.out.println(r.getXxx());</span><br><span class="line"></span><br><span class="line">// 提交任务</span><br><span class="line">        Future&lt;Result&gt; future =</span><br><span class="line">                executor.submit(new Task(r), r);</span><br><span class="line">        Result fr = future.get();</span><br><span class="line"></span><br><span class="line">        System.out.println(fr.getXxx());</span><br><span class="line"></span><br><span class="line">        executor.shutdown();</span><br><span class="line"></span><br><span class="line">// 下面等式成立</span><br><span class="line">//        fr === r;</span><br><span class="line">//        fr.getAAA() === a;</span><br><span class="line">//        fr.getXXX() === x</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Task implements Runnable&#123;</span><br><span class="line">    Result r;</span><br><span class="line">    //通过构造函数传入result</span><br><span class="line">    Task(Result r)&#123;</span><br><span class="line">        this.r = r;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">        String a = r.getAaa();</span><br><span class="line">        r.setXxx(a);</span><br><span class="line">    &#125;</span><br><span class="line">//    public void run() &#123;</span><br><span class="line">//        //可以操作result</span><br><span class="line">//        String a = r.getAaa();</span><br><span class="line">//        r.setXxx(a);</span><br><span class="line">//    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class Result &#123;</span><br><span class="line">    private String aaa;</span><br><span class="line">    private String xxx;</span><br><span class="line"></span><br><span class="line">    public void setAaa(String aaa) &#123;</span><br><span class="line">        this.aaa =aaa;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setXxx(String xxx) &#123;</span><br><span class="line">        this.xxx =xxx;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getAaa() &#123;</span><br><span class="line">        return aaa;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getXxx() &#123;</span><br><span class="line">        return xxx;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="futuretask">FutureTask</span></h2><h3><span id="usage">Usage</span></h3><p>FutureTask 实现了 Runnable 和 Future 接口，由于实现了 Runnable 接口，</p>
<p>所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行；</p>
<p>又因为实现了 Future 接口，所以也能用来获得任务的执行结果。</p>
<p>（1）示例代码是将 FutureTask 对象提交给 ThreadPoolExecutor 去执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 创建FutureTask</span><br><span class="line">FutureTask&lt;Integer&gt; futureTask</span><br><span class="line">  = new FutureTask&lt;&gt;(()-&gt; 1+2);</span><br><span class="line">// 创建线程池</span><br><span class="line">ExecutorService es = </span><br><span class="line">  Executors.newCachedThreadPool();</span><br><span class="line">// 提交FutureTask </span><br><span class="line">es.submit(futureTask);</span><br><span class="line">// 获取计算结果</span><br><span class="line">Integer result = futureTask.get();</span><br></pre></td></tr></table></figure>

<p>（2）FutureTask 对象直接被 Thread 执行的示例代码，利用 FutureTask 对象可以很容易获取子线程的执行结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 创建FutureTask</span><br><span class="line">FutureTask&lt;Integer&gt; futureTask</span><br><span class="line">  = new FutureTask&lt;&gt;(()-&gt; 1+2);</span><br><span class="line">// 创建并启动线程</span><br><span class="line">Thread T1 = new Thread(futureTask);</span><br><span class="line">T1.start();</span><br><span class="line">// 获取计算结果</span><br><span class="line">Integer result = futureTask.get();</span><br></pre></td></tr></table></figure>



<h3><span id="example">Example</span></h3><img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/23_2屏幕快照 2021-12-11 下午8.31.30.png">



<p>我们创建了两个 FutureTask——ft1 和 ft2，ft1 完成洗水壶、烧开水、泡茶的任务，ft2 完成洗茶壶、洗茶杯、拿茶叶的任务；这里需要注意的是 ft1 这个任务在执行泡茶任务前，需要等待 ft2 把茶叶拿来，所以 ft1 内部需要引用 ft2，并在执行泡茶之前，调用 ft2 的 get() 方法实现等待。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 创建任务T2的FutureTask</span><br><span class="line">FutureTask&lt;String&gt; ft2</span><br><span class="line">  = new FutureTask&lt;&gt;(new T2Task());</span><br><span class="line">// 创建任务T1的FutureTask</span><br><span class="line">FutureTask&lt;String&gt; ft1</span><br><span class="line">  = new FutureTask&lt;&gt;(new T1Task(ft2));</span><br><span class="line">// 线程T1执行任务ft1</span><br><span class="line">Thread T1 = new Thread(ft1);</span><br><span class="line">T1.start();</span><br><span class="line">// 线程T2执行任务ft2</span><br><span class="line">Thread T2 = new Thread(ft2);</span><br><span class="line">T2.start();</span><br><span class="line">// 等待线程T1执行结果</span><br><span class="line">System.out.println(ft1.get());</span><br><span class="line"></span><br><span class="line">// T1Task需要执行的任务：</span><br><span class="line">// 洗水壶、烧开水、泡茶</span><br><span class="line">class T1Task implements Callable&lt;String&gt;&#123;</span><br><span class="line">  FutureTask&lt;String&gt; ft2;</span><br><span class="line">  // T1任务需要T2任务的FutureTask</span><br><span class="line">  T1Task(FutureTask&lt;String&gt; ft2)&#123;</span><br><span class="line">    this.ft2 = ft2;</span><br><span class="line">  &#125;</span><br><span class="line">  @Override</span><br><span class="line">  String call() throws Exception &#123;</span><br><span class="line">    System.out.println(&quot;T1:洗水壶...&quot;);</span><br><span class="line">    TimeUnit.SECONDS.sleep(1);</span><br><span class="line">    </span><br><span class="line">    System.out.println(&quot;T1:烧开水...&quot;);</span><br><span class="line">    TimeUnit.SECONDS.sleep(15);</span><br><span class="line">    // 获取T2线程的茶叶  </span><br><span class="line">    String tf = ft2.get();</span><br><span class="line">    System.out.println(&quot;T1:拿到茶叶:&quot;+tf);</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;T1:泡茶...&quot;);</span><br><span class="line">    return &quot;上茶:&quot; + tf;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">// T2Task需要执行的任务:</span><br><span class="line">// 洗茶壶、洗茶杯、拿茶叶</span><br><span class="line">class T2Task implements Callable&lt;String&gt; &#123;</span><br><span class="line">  @Override</span><br><span class="line">  String call() throws Exception &#123;</span><br><span class="line">    System.out.println(&quot;T2:洗茶壶...&quot;);</span><br><span class="line">    TimeUnit.SECONDS.sleep(1);</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;T2:洗茶杯...&quot;);</span><br><span class="line">    TimeUnit.SECONDS.sleep(2);</span><br><span class="line"></span><br><span class="line">    System.out.println(&quot;T2:拿茶叶...&quot;);</span><br><span class="line">    TimeUnit.SECONDS.sleep(1);</span><br><span class="line">    return &quot;龙井&quot;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">// 一次执行结果：</span><br><span class="line">T1:洗水壶...</span><br><span class="line">T2:洗茶壶...</span><br><span class="line">T1:烧开水...</span><br><span class="line">T2:洗茶杯...</span><br><span class="line">T2:拿茶叶...</span><br><span class="line">T1:拿到茶叶:龙井</span><br><span class="line">T1:泡茶...</span><br><span class="line">上茶:龙井</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="completablefuture">CompletableFuture</span></h2><p>简化异步编程：直接提交任务到线程池，异步执行；</p>
<h3><span id="example">Example</span></h3><img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/24_1屏幕快照 2021-12-12 下午9.24.02.png">

<p>thenCombine  T3 等待T1 和T2完成后继续执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">//任务1：洗水壶-&gt;烧开水</span><br><span class="line">CompletableFuture&lt;Void&gt; f1 = </span><br><span class="line">  CompletableFuture.runAsync(()-&gt;&#123;</span><br><span class="line">  System.out.println(&quot;T1:洗水壶...&quot;);</span><br><span class="line">  sleep(1, TimeUnit.SECONDS);</span><br><span class="line"></span><br><span class="line">  System.out.println(&quot;T1:烧开水...&quot;);</span><br><span class="line">  sleep(15, TimeUnit.SECONDS);</span><br><span class="line">&#125;);</span><br><span class="line">//任务2：洗茶壶-&gt;洗茶杯-&gt;拿茶叶</span><br><span class="line">CompletableFuture&lt;String&gt; f2 = </span><br><span class="line">  CompletableFuture.supplyAsync(()-&gt;&#123;</span><br><span class="line">  System.out.println(&quot;T2:洗茶壶...&quot;);</span><br><span class="line">  sleep(1, TimeUnit.SECONDS);</span><br><span class="line"></span><br><span class="line">  System.out.println(&quot;T2:洗茶杯...&quot;);</span><br><span class="line">  sleep(2, TimeUnit.SECONDS);</span><br><span class="line"></span><br><span class="line">  System.out.println(&quot;T2:拿茶叶...&quot;);</span><br><span class="line">  sleep(1, TimeUnit.SECONDS);</span><br><span class="line">  return &quot;龙井&quot;;</span><br><span class="line">&#125;);</span><br><span class="line">//任务3：任务1和任务2完成后执行：泡茶</span><br><span class="line">CompletableFuture&lt;String&gt; f3 = </span><br><span class="line">  f1.thenCombine(f2, (__, tf)-&gt;&#123;</span><br><span class="line">    System.out.println(&quot;T1:拿到茶叶:&quot; + tf);</span><br><span class="line">    System.out.println(&quot;T1:泡茶...&quot;);</span><br><span class="line">    return &quot;上茶:&quot; + tf;</span><br><span class="line">  &#125;);</span><br><span class="line">//等待任务3执行结果</span><br><span class="line">System.out.println(f3.join());</span><br><span class="line"></span><br><span class="line">void sleep(int t, TimeUnit u) &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    u.sleep(t);</span><br><span class="line">  &#125;catch(InterruptedException e)&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">// 一次执行结果：</span><br><span class="line">T1:洗水壶...</span><br><span class="line">T2:洗茶壶...</span><br><span class="line">T1:烧开水...</span><br><span class="line">T2:洗茶杯...</span><br><span class="line">T2:拿茶叶...</span><br><span class="line">T1:拿到茶叶:龙井</span><br><span class="line">T1:泡茶...</span><br><span class="line">上茶:龙井</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="usage">Usage</span></h3><h4><span id="创建">创建</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">//使用默认线程池</span><br><span class="line">static CompletableFuture&lt;Void&gt; </span><br><span class="line">  runAsync(Runnable runnable)</span><br><span class="line">static &lt;U&gt; CompletableFuture&lt;U&gt; </span><br><span class="line">  supplyAsync(Supplier&lt;U&gt; supplier)</span><br><span class="line">  </span><br><span class="line">//可以指定线程池  </span><br><span class="line">static CompletableFuture&lt;Void&gt; </span><br><span class="line">  runAsync(Runnable runnable, Executor executor)</span><br><span class="line">static &lt;U&gt; CompletableFuture&lt;U&gt; </span><br><span class="line">  supplyAsync(Supplier&lt;U&gt; supplier, Executor executor)  </span><br><span class="line">  </span><br><span class="line">  </span><br></pre></td></tr></table></figure>



<p>4个静态方法：</p>
<p>runAsync(Runnable runnable)和supplyAsync(Supplier supplier)，</p>
<p>它们之间的区别是：Runnable 接口的 run() 方法没有返回值，而 Supplier 接口的 get() 方法是有返回值的。</p>
<p>前两个方法和后两个方法的区别在于：后两个方法可以指定线程池参数。</p>
<h4><span id="设置独立线程池">设置独立线程池</span></h4><p>默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数（也可以通过 JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism 来设置 ForkJoinPool 线程池的线程数）。</p>
<p>如果所有 CompletableFuture 共享一个线程池，那么一旦有任务执行一些很慢的 I&#x2F;O 操作，就会导致线程池中所有线程都阻塞在 I&#x2F;O 操作上，从而造成线程饥饿，进而影响整个系统的性能。所以，强烈建议你要根据不同的业务类型创建不同的线程池，以避免互相干扰。</p>
<h4><span id="通过future获取执行结果">通过future获取执行结果</span></h4><p>创建完 CompletableFuture 对象之后，会自动地异步执行 runnable.run() 方法或者 supplier.get() 方法，对于一个异步操作，你需要关注两个问题：一个是异步操作什么时候结束，另一个是如何获取异步操作的执行结果。因为 CompletableFuture 类实现了 Future 接口，所以这两个问题你都可以通过 Future 接口来解决。</p>
<h4><span id="completionstage-接口">CompletionStage 接口</span></h4><p>CompletableFuture 类还实现了 CompletionStage 接口，接口里面主要定义了线程间的协作关系（类似A等待B、C等待A和B执行完后继续执行）</p>
<p>模拟现实世界，主要分为几类方法：串行、并行、汇聚</p>
<img src="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/24_2屏幕快照 2021-12-12 下午9.43.51.png">





<h5><span id="1描述串行关系">（1）描述串行关系</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CompletionStage&lt;R&gt; thenApply(fn);</span><br><span class="line">CompletionStage&lt;R&gt; thenApplyAsync(fn);</span><br><span class="line">CompletionStage&lt;Void&gt; thenAccept(consumer);</span><br><span class="line">CompletionStage&lt;Void&gt; thenAcceptAsync(consumer);</span><br><span class="line">CompletionStage&lt;Void&gt; thenRun(action);</span><br><span class="line">CompletionStage&lt;Void&gt; thenRunAsync(action);</span><br><span class="line">CompletionStage&lt;R&gt; thenCompose(fn);</span><br><span class="line">CompletionStage&lt;R&gt; thenComposeAsync(fn);</span><br></pre></td></tr></table></figure>



<p>CompletionStage 接口里面描述串行关系，主要是 thenApply、thenAccept、thenRun 和 thenCompose 这四个系列的接口。</p>
<p>thenApply 系列函数里参数 fn 的类型是接口 Function，这个接口里与 CompletionStage 相关的方法是 R apply(T t)，这个方法既能接收参数也支持返回值，</p>
<p>而 thenAccept 系列方法里参数 consumer 的类型是接口Consumer，这个接口里与 CompletionStage 相关的方法是 void accept(T t)，这个方法虽然支持参数，但却不支持回值</p>
<p>thenRun 系列方法里 action 的参数是 Runnable，所以 action 既不能接收参数也不支持返回值，</p>
<p>这些方法里面 Async 代表的是异步执行 fn、consumer 或者 action。其中，需要你注意的是 thenCompose 系列方法，这个系列的方法会新创建出一个子流程，最终结果和 thenApply 系列是相同的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CompletableFuture&lt;String&gt; f0 = </span><br><span class="line">  CompletableFuture.supplyAsync(</span><br><span class="line">    () -&gt; &quot;Hello World&quot;)      //①</span><br><span class="line">  .thenApply(s -&gt; s + &quot; QQ&quot;)  //②</span><br><span class="line">  .thenApply(String::toUpperCase);//③</span><br><span class="line"></span><br><span class="line">System.out.println(f0.join());</span><br><span class="line">//串行执行 输出结果 HELLO WORLD QQ</span><br></pre></td></tr></table></figure>



<h5><span id="2描述and聚合关系">（2）描述And聚合关系</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CompletionStage&lt;R&gt; thenCombine(other, fn);</span><br><span class="line">CompletionStage&lt;R&gt; thenCombineAsync(other, fn);</span><br><span class="line">CompletionStage&lt;Void&gt; thenAcceptBoth(other, consumer);</span><br><span class="line">CompletionStage&lt;Void&gt; thenAcceptBothAsync(other, consumer);</span><br><span class="line">CompletionStage&lt;Void&gt; runAfterBoth(other, action);</span><br><span class="line">CompletionStage&lt;Void&gt; runAfterBothAsync(other, action);</span><br></pre></td></tr></table></figure>

<p>CompletionStage 接口里面描述 AND 汇聚关系，主要是 thenCombine、thenAcceptBoth 和 runAfterBoth 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同</p>
<h5><span id="3描述or聚合关系">（3）描述or聚合关系</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CompletionStage applyToEither(other, fn);</span><br><span class="line">CompletionStage applyToEitherAsync(other, fn);</span><br><span class="line">CompletionStage acceptEither(other, consumer);</span><br><span class="line">CompletionStage acceptEitherAsync(other, consumer);</span><br><span class="line">CompletionStage runAfterEither(other, action);</span><br><span class="line">CompletionStage runAfterEitherAsync(other, action);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>CompletionStage 接口里面描述 OR 汇聚关系，主要是 applyToEither、acceptEither 和 runAfterEither 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同。</p>
<p>下面的示例代码展示了如何使用 applyToEither() 方法来描述一个 OR 汇聚关系</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CompletableFuture&lt;String&gt; f1 = </span><br><span class="line">  CompletableFuture.supplyAsync(()-&gt;&#123;</span><br><span class="line">    int t = getRandom(5, 10);</span><br><span class="line">    sleep(t, TimeUnit.SECONDS);</span><br><span class="line">    return String.valueOf(t);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">CompletableFuture&lt;String&gt; f2 = </span><br><span class="line">  CompletableFuture.supplyAsync(()-&gt;&#123;</span><br><span class="line">    int t = getRandom(5, 10);</span><br><span class="line">    sleep(t, TimeUnit.SECONDS);</span><br><span class="line">    return String.valueOf(t);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">CompletableFuture&lt;String&gt; f3 = </span><br><span class="line">  f1.applyToEither(f2,s -&gt; s);</span><br><span class="line"></span><br><span class="line">System.out.println(f3.join());</span><br></pre></td></tr></table></figure>



<h5><span id="4处理异常">（4）处理异常</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CompletionStage exceptionally(fn);</span><br><span class="line">CompletionStage&lt;R&gt; whenComplete(consumer);</span><br><span class="line">CompletionStage&lt;R&gt; whenCompleteAsync(consumer);</span><br><span class="line">CompletionStage&lt;R&gt; handle(fn);</span><br><span class="line">CompletionStage&lt;R&gt; handleAsync(fn);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>exceptionally() 的使用非常类似于 try{}catch{}中的 catch{}，但是由于支持链式编程方式，所以相对更简单。既然有 try{}catch{}，那就一定还有 try{}finally{}，whenComplete() 和 handle() 系列方法就类似于 try{}finally{}中的 finally{}，无论是否发生异常都会执行 whenComplete() 中的回调函数 consumer 和 handle() 中的回调函数 fn。whenComplete() 和 handle() 的区别在于 whenComplete() 不支持返回结果，而 handle() 是支持返回结果的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CompletableFuture&lt;Integer&gt; </span><br><span class="line">  f0 = CompletableFuture</span><br><span class="line">    .supplyAsync(()-&gt;(7/0))</span><br><span class="line">    .thenApply(r-&gt;r*10)</span><br><span class="line">    .exceptionally(e-&gt;0);</span><br><span class="line">System.out.println(f0.join());</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="completionservice">CompletionService</span></h2><p>批量提交多个任务异步执行.</p>
<p>CompletionService 的实现原理是内部维护了一个阻塞队列，当任务执行结束就把任务的执行结果加入到阻塞队列中</p>
<h3><span id="example">Example</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 创建线程池</span><br><span class="line">ExecutorService executor = </span><br><span class="line">  Executors.newFixedThreadPool(3);</span><br><span class="line">// 创建CompletionService</span><br><span class="line">CompletionService&lt;Integer&gt; cs = new </span><br><span class="line">  ExecutorCompletionService&lt;&gt;(executor);</span><br><span class="line">// 异步向电商S1询价</span><br><span class="line">cs.submit(()-&gt;getPriceByS1());</span><br><span class="line">// 异步向电商S2询价</span><br><span class="line">cs.submit(()-&gt;getPriceByS2());</span><br><span class="line">// 异步向电商S3询价</span><br><span class="line">cs.submit(()-&gt;getPriceByS3());</span><br><span class="line">// 将询价结果异步保存到数据库</span><br><span class="line">for (int i=0; i&lt;3; i++) &#123;</span><br><span class="line">  //阻塞执行</span><br><span class="line">  Integer r = cs.take().get();</span><br><span class="line">  executor.execute(()-&gt;save(r));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="usage">Usage</span></h3><h4><span id="构造方法">构造方法</span></h4><p>CompletionService 接口的实现类是 ExecutorCompletionService，</p>
<p>这个实现类的构造方法有两个，分别是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ExecutorCompletionService(Executor executor)；</span><br><span class="line">ExecutorCompletionService(Executor executor, BlockingQueue&gt; completionQueue)。</span><br></pre></td></tr></table></figure>

<p>这两个构造方法都需要传入一个线程池，如果不指定 completionQueue，那么默认会使用无界的 LinkedBlockingQueue。任务执行结果的 Future 对象就是加入到 completionQueue 中。</p>
<h4><span id="method">method</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Future&lt;V&gt; submit(Callable&lt;V&gt; task);</span><br><span class="line">Future&lt;V&gt; submit(Runnable task, V result);</span><br><span class="line">Future&lt;V&gt; take() </span><br><span class="line">  throws InterruptedException;</span><br><span class="line">Future&lt;V&gt; poll();</span><br><span class="line">Future&lt;V&gt; poll(long timeout, TimeUnit unit) </span><br><span class="line">  throws InterruptedException;</span><br></pre></td></tr></table></figure>

<p>take()、poll() 都是从阻塞队列中获取并移除一个元素；它们的区别在于如果阻塞队列是空的，那么调用 take() 方法的线程会被阻塞，而 poll() 方法会返回 null 值。 poll(long timeout, TimeUnit unit) 方法支持以超时的方式获取并移除阻塞队列头部的一个元素，如果等待了 timeout unit 时间，阻塞队列还是空的，那么该方法会返回 null 值。</p>
<h4><span id="others">Others</span></h4><p>Dubbo 中有一种叫做 Forking 的集群模式，这种集群模式下，支持并行地调用多个查询服务，只要有一个成功返回结果，整个服务就可以返回了。例如你需要提供一个地址转坐标的服务，为了保证该服务的高可用和性能，你可以并行地调用 3 个地图服务商的 API，然后只要有 1 个正确返回了结果 r，那么地址转坐标这个服务就可以直接返回 r 了。这种集群模式可以容忍 2 个地图服务商服务异常，但缺点是消耗的资源偏多。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">geocoder(addr) &#123;</span><br><span class="line">  //并行执行以下3个查询服务， </span><br><span class="line">  r1=geocoderByS1(addr);</span><br><span class="line">  r2=geocoderByS2(addr);</span><br><span class="line">  r3=geocoderByS3(addr);</span><br><span class="line">  //只要r1,r2,r3有一个返回</span><br><span class="line">  //则返回</span><br><span class="line">  return r1|r2|r3;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 创建线程池</span><br><span class="line">ExecutorService executor =</span><br><span class="line">  Executors.newFixedThreadPool(3);</span><br><span class="line">// 创建CompletionService</span><br><span class="line">CompletionService&lt;Integer&gt; cs =</span><br><span class="line">  new ExecutorCompletionService&lt;&gt;(executor);</span><br><span class="line">// 用于保存Future对象</span><br><span class="line">List&lt;Future&lt;Integer&gt;&gt; futures =</span><br><span class="line">  new ArrayList&lt;&gt;(3);</span><br><span class="line">//提交异步任务，并保存future到futures </span><br><span class="line">futures.add(</span><br><span class="line">  cs.submit(()-&gt;geocoderByS1()));</span><br><span class="line">futures.add(</span><br><span class="line">  cs.submit(()-&gt;geocoderByS2()));</span><br><span class="line">futures.add(</span><br><span class="line">  cs.submit(()-&gt;geocoderByS3()));</span><br><span class="line">// 获取最快返回的任务执行结果</span><br><span class="line">Integer r = 0;</span><br><span class="line">try &#123;</span><br><span class="line">  // 只要有一个成功返回，则break</span><br><span class="line">  for (int i = 0; i &lt; 3; ++i) &#123;</span><br><span class="line">    r = cs.take().get();</span><br><span class="line">    //简单地通过判空来检查是否成功返回</span><br><span class="line">    if (r != null) &#123;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">  //取消所有任务</span><br><span class="line">  for(Future&lt;Integer&gt; f : futures)</span><br><span class="line">    f.cancel(true);</span><br><span class="line">&#125;</span><br><span class="line">// 返回结果</span><br><span class="line">return r;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="总结">总结</span></h3><p>当需要批量提交异步任务的时候建议你使用 CompletionService。CompletionService 将线程池 Executor 和阻塞队列 BlockingQueue 的功能融合在了一起，能够让批量异步任务的管理更简单。除此之外，CompletionService <strong>能够让异步任务的执行结果有序化，先执行完的先进入阻塞队列，利用这个特性，你可以轻松实现后续处理的有序性，避免无谓的等待</strong>，同时还可以快速实现诸如 Forking Cluster 这样的需求。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/" data-id="clkag9wyn00004v9a2apq02d6" data-title="jiketime_java并发" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/java/" rel="tag">java</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/sql/doc_sql" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/17/bigdata/sql/doc_sql/" class="article-date">
  <time class="dt-published" datetime="2023-07-17T02:40:15.000Z" itemprop="datePublished">2023-07-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-sql/">bigdata/sql</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/17/bigdata/sql/doc_sql/">doc_sql</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#spark">Spark</a></li>
<li><a href="#flink">Flink</a></li>
<li><a href="#hive">Hive</a></li>
</ul>
<!-- tocstop -->

<h2><span id="spark">Spark</span></h2><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref.html">Spark sql syntax</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/sql/index.html#struct">Spark sql function</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-datatypes.html">Spark data type</a></p>
<h2><span id="flink">Flink</span></h2><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/overview/">Flink sql syntax</a></p>
<p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/overview/">Flink sql function</a></p>
<p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/types/">Flink data type</a></p>
<h2><span id="hive">Hive</span></h2><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/hive/languagemanual">Hive Syntax</a></p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types">Hive data type</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/17/bigdata/sql/doc_sql/" data-id="clk6a22qi0000ug9a8i9hbffy" data-title="doc_sql" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sql/" rel="tag">sql</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/flink/flink_doc/flink_sql_20230713" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/" class="article-date">
  <time class="dt-published" datetime="2023-07-13T08:39:40.000Z" itemprop="datePublished">2023-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-flink-flink-doc/">bigdata/flink/flink_doc</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/">flink_sql_20230713</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#env">Env</a></li>
<li><a href="#arch">Arch</a><ul>
<li><a href="#basic">Basic</a><ul>
<li><a href="#jobmanager"><strong>JobManager</strong></a></li>
<li><a href="#taskmanager"><strong>TaskManager</strong></a></li>
<li><a href="#client"><strong>Client</strong></a></li>
<li><a href="#%E4%BB%BB%E5%8A%A1%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F"><strong>任务部署模式</strong></a></li>
</ul>
</li>
<li><a href="#%E4%BB%BB%E5%8A%A1%E9%83%A8%E7%BD%B2">任务部署</a><ul>
<li><a href="#%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%8B%E8%AF%95">任务提交测试</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sql">Sql</a><ul>
<li><a href="#example">Example</a></li>
<li><a href="#concepts">Concepts</a><ul>
<li><a href="#dynamic-table">dynamic table</a></li>
<li><a href="#versioned-table">Versioned table</a><ul>
<li><a href="#versioned-table-source">Versioned table source</a></li>
<li><a href="#versioned-table-views">Versioned table views</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#join">Join</a><ul>
<li><a href="#regular-join">Regular Join</a></li>
<li><a href="#interval-join">Interval Join</a></li>
<li><a href="#temporal-join">Temporal Join</a><ul>
<li><a href="#code-test">Code Test</a></li>
</ul>
</li>
<li><a href="#lookup-join">Lookup Join</a></li>
<li><a href="#window-join">Window Join</a></li>
</ul>
</li>
<li><a href="#connector">Connector</a><ul>
<li><a href="#datagen">DataGen</a></li>
<li><a href="#filesystem">FileSystem</a></li>
<li><a href="#blackhole">BlackHole</a></li>
<li><a href="#print">Print</a></li>
<li><a href="#jdbc">JDBC</a></li>
<li><a href="#kafka">Kafka</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<!-- tocstop -->

<h2><span id="env">Env</span></h2><p>local: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /Users/zhouqingfeng/Desktop/software/flink/flink-1.17.1</span><br><span class="line"></span><br><span class="line">./bin/start-cluster.sh</span><br><span class="line"></span><br><span class="line">http://localhost:8081/</span><br></pre></td></tr></table></figure>



<h2><span id="arch">Arch</span></h2><h3><span id="basic">Basic</span></h3><h4><span id="jobmanager"><strong>JobManager</strong></span></h4><p>resourceManager （资源管理(包括task slot) ）</p>
<p>任务分发(dispatcher) </p>
<p>任务管理 (JobMaster管理每个JobGraph)</p>
<h4><span id="taskmanager"><strong>TaskManager</strong></span></h4><p>执行job Task</p>
<h4><span id="client"><strong>Client</strong></span></h4><p>核心是执行每个Flink Application main( )方法，类似spark driver，</p>
<p>下载application 依赖到本地、执行main方法生成JobGraph、上传依赖和JobGraph到集群。</p>
<p>Application mode出现之前client 都运行在本地。</p>
<h4><span id="任务部署模式"><strong>任务部署模式</strong></span></h4><ul>
<li><p>in Application Mode     </p>
<p>main() 方法运行在 Jobmaster，类似spark 的yarn-cluster，driver单独运行，client开销非常小</p>
</li>
<li><p>in a Per-Job Mode (deprecated)</p>
<p>main() 方法运行在client，开销比较大 </p>
</li>
<li><p>in Session Mode</p>
<p>main() 方法运行在client，开销比较大</p>
</li>
</ul>
<h3><span id="任务部署">任务部署</span></h3><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/resource-providers/yarn/#deployment-modes-supported-by-flink-on-yarn">flink任务部署到集群</a></p>
<h4><span id="任务提交测试">任务提交测试</span></h4><p>Flink 1.12.*提交到hadoop yarn</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; </span><br><span class="line"></span><br><span class="line">  Options for yarn-cluster mode:</span><br><span class="line">     -d,--detached                        If present, runs the job in detached</span><br><span class="line">                                          mode</span><br><span class="line">     -m,--jobmanager &lt;arg&gt;                Set to yarn-cluster to use YARN</span><br><span class="line">                                          execution mode.</span><br><span class="line">     -yat,--yarnapplicationType &lt;arg&gt;     Set a custom application type for the</span><br><span class="line">                                          application on YARN</span><br><span class="line">     -yD &lt;property=value&gt;                 use value for given property</span><br><span class="line">     -yd,--yarndetached                   If present, runs the job in detached</span><br><span class="line">                                          mode (deprecated; use non-YARN</span><br><span class="line">                                          specific option instead)</span><br><span class="line">     -yh,--yarnhelp                       Help for the Yarn session CLI.</span><br><span class="line">     -yid,--yarnapplicationId &lt;arg&gt;       Attach to running YARN session</span><br><span class="line">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class="line">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container with</span><br><span class="line">                                          optional unit (default: MB)</span><br><span class="line">     -ynl,--yarnnodeLabel &lt;arg&gt;           Specify YARN node label for the YARN</span><br><span class="line">                                          application</span><br><span class="line">     -ynm,--yarnname &lt;arg&gt;                Set a custom name for the application</span><br><span class="line">                                          on YARN</span><br><span class="line">     -yq,--yarnquery                      Display available YARN resources</span><br><span class="line">                                          (memory, cores)</span><br><span class="line">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">     -ys,--yarnslots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class="line">                                          (t for transfer)</span><br><span class="line">     -ytm,--yarntaskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with</span><br><span class="line">                                          optional unit (default: MB)</span><br><span class="line">     -yz,--yarnzookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper</span><br><span class="line">                                          sub-paths for high availability mode</span><br><span class="line">     -z,--zookeeperNamespace &lt;arg&gt;        Namespace to create the Zookeeper</span><br><span class="line">                                          sub-paths for high availability mode</span><br></pre></td></tr></table></figure>



<p><strong>简单测试</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1  examples/batch/WordCount.jar  \</span><br><span class="line">--input   hdfs://localhost:9000/tmp/test/flink/ \</span><br><span class="line">--output  hdfs://localhost:9000/tmp/test/fout</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#file local</span><br><span class="line"></span><br><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1  examples/batch/WordCount.jar  --input    file:///Users/zhouqingfeng/Desktop/target/datalake/flink/flink-1.12.3/README.txt    --output  file:///Users/zhouqingfeng/Desktop/target/datalake/flink/flink-1.12.3/out.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1   -ynm  flinkTest -c com.test1.flink.connector.KafkaStreamTableTest1 \</span><br><span class="line">/Users/zhouqingfeng/Desktop/target/datalake/flink/projects/test/flink-playgrounds-master/table-walkthrough/target/spend-report-1.0.0.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1   -ynm  flinkTest -c com.test1.flink.connector.HiveTest2  /Users/zhouqingfeng/Desktop/target/datalake/flink/projects/test/flink-playgrounds-master/table-walkthrough/target/spend-report-1.0.0.jar</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="sql">Sql</span></h2><h3><span id="example">Example</span></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/sql-client.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt;&gt; 12.csv</span></span><br><span class="line"><span class="string">1,lisa,fina</span></span><br><span class="line"><span class="string">2,john,sales</span></span><br><span class="line"><span class="string">3,jimmy,sales</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE employee_info (</span><br><span class="line">    emp_id INT,</span><br><span class="line">    name VARCHAR,</span><br><span class="line">    dept VARCHAR</span><br><span class="line">) WITH ( </span><br><span class="line">    &#x27;connector&#x27; = &#x27;filesystem&#x27;,</span><br><span class="line">    &#x27;path&#x27; = &#x27;file:/Users/zhouqingfeng/Desktop/software/flink/flink-1.17.1/test/12.csv&#x27;,</span><br><span class="line">    &#x27;format&#x27; = &#x27;csv&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">select * from employee_info;</span><br></pre></td></tr></table></figure>



<h3><span id="concepts">Concepts</span></h3><h4><span id="dynamic-table">dynamic table</span></h4><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/concepts/dynamic_tables/">流表转换</a></p>
<h4><span id="versioned-table">Versioned table</span></h4><p>key的值随时间不短变化，表记录key的历史值</p>
<p>Flink SQL operates over dynamic tables that evolve, which may either be append-only or updating.</p>
<p>Versioned tables represent a special type of updating table that remembers the past values for each key.</p>
<p>Flink SQL can define versioned tables over any dynamic table with a <code>PRIMARY KEY</code> constraint and time attribute.</p>
<p>Fink sql基于主键约束和时间属性 定义版本表。</p>
<h5><span id="versioned-table-source">Versioned table source</span></h5><h5><span id="versioned-table-views">Versioned table views</span></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.17.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> currency_rates (</span><br><span class="line">	currency      STRING,</span><br><span class="line">	rate          <span class="type">DECIMAL</span>(<span class="number">32</span>, <span class="number">10</span>),</span><br><span class="line">	`update_time` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,</span><br><span class="line">	WATERMARK <span class="keyword">FOR</span> update_time <span class="keyword">AS</span> update_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">	<span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;topic&#x27;</span>	    <span class="operator">=</span> <span class="string">&#x27;rates&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest-offset&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;format&#x27;</span>    <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">&#123;&quot;currency&quot;:&quot;Yen&quot;, &quot;rate&quot;:<span class="number">102</span>&#125;</span><br><span class="line">&#123;&quot;currency&quot;:&quot;Euro&quot;, &quot;rate&quot;:<span class="number">114</span>&#125;</span><br><span class="line">&#123;&quot;currency&quot;:&quot;USD&quot;, &quot;rate&quot;:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Define a versioned view</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> versioned_rates <span class="keyword">AS</span>              </span><br><span class="line"><span class="keyword">SELECT</span> currency, rate, update_time              <span class="comment">-- (1) `update_time` keeps the event time</span></span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">      <span class="keyword">SELECT</span> <span class="operator">*</span>,</span><br><span class="line">      <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> currency  <span class="comment">-- (2) the inferred unique key `currency` can be a primary key</span></span><br><span class="line">         <span class="keyword">ORDER</span> <span class="keyword">BY</span> update_time <span class="keyword">DESC</span>) <span class="keyword">AS</span> rownum </span><br><span class="line">      <span class="keyword">FROM</span> currency_rates)</span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">=</span> <span class="number">1</span>; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> print_table ( </span><br><span class="line">	currency      STRING,</span><br><span class="line">	rate          <span class="type">DECIMAL</span>(<span class="number">32</span>, <span class="number">10</span>),</span><br><span class="line">	`update_time` <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> ( </span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span> </span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> print_table  <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> versioned_rates;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3><span id="join">Join</span></h3><h4><span id="regular-join">Regular Join</span></h4><h4><span id="interval-join">Interval Join</span></h4><h4><span id="temporal-join">Temporal Join</span></h4><p>Temporal table 时态表，就是版本表，存储表的多个快照(历史版本)，这里就是版本表</p>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/679659?spm=a2c6h.12873639.0.0.27345d11AZJgKz#slide-10">Temporal Table JOIN内部原理</a>，内部类似拉链表，存储表的多个快照版本，</p>
<p>相当于flink在内存里存储了多个key的状态，每个状态对应不同的时间窗口(数据有效期)</p>
<p>其他资料：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/475660438">Flink temporal table join研究</a></p>
<h5><span id="code-test">Code Test</span></h5><h4><span id="lookup-join">Lookup Join</span></h4><p>A lookup join is typically used to enrich a table with data that is queried from an external system.</p>
<p>常规维表join，每次查询最新数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Customers is backed by the JDBC connector and can be used for lookup joins</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">TABLE</span> Customers (</span><br><span class="line">  id <span class="type">INT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  country STRING,</span><br><span class="line">  zip STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc:mysql://mysqlhost:3306/customerdb&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;table-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;customers&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- enrich each order with customer information</span></span><br><span class="line"><span class="keyword">SELECT</span> o.order_id, o.total, c.country, c.zip</span><br><span class="line"><span class="keyword">FROM</span> Orders <span class="keyword">AS</span> o</span><br><span class="line">  <span class="keyword">JOIN</span> Customers <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> o.proc_time <span class="keyword">AS</span> c</span><br><span class="line">    <span class="keyword">ON</span> o.customer_id <span class="operator">=</span> c.id;</span><br></pre></td></tr></table></figure>



<h4><span id="window-join">Window Join</span></h4><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/queries/window-join/">基于window 进行join</a></p>
<h3><span id="connector">Connector</span></h3><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/</a></p>
<h4><span id="datagen">DataGen</span></h4><p>The DataGen connector allows for creating tables based on in-memory data generation. </p>
<h4><span id="filesystem">FileSystem</span></h4><p>在mac本地测试 指定目录下 生成的json文件是隐藏的，以.part- 开头</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">--source</span><br><span class="line">CREATE TABLE orders (   </span><br><span class="line">                   order_id BIGINT,   </span><br><span class="line">                   product_id BIGINT,   </span><br><span class="line">                   order_time TIMESTAMP(3),   </span><br><span class="line">                   order_amount DECIMAL(10, 2)   </span><br><span class="line">                 ) WITH (   </span><br><span class="line">                   &#x27;connector&#x27; = &#x27;kafka&#x27;,   </span><br><span class="line">                   &#x27;topic&#x27; = &#x27;testzq1&#x27;,   </span><br><span class="line">                   &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,   </span><br><span class="line">                   &#x27;properties.group.id&#x27; = &#x27;orders-consumer&#x27;,   </span><br><span class="line">                   &#x27;scan.startup.mode&#x27; = &#x27;latest-offset&#x27;,   </span><br><span class="line">                   &#x27;format&#x27; = &#x27;json&#x27;   </span><br><span class="line">                 )</span><br><span class="line"></span><br><span class="line">--dest</span><br><span class="line">CREATE TABLE orders_history (  </span><br><span class="line">                   order_id BIGINT,  </span><br><span class="line">                   product_id BIGINT,  </span><br><span class="line">                   order_time TIMESTAMP(3),  </span><br><span class="line">                   order_amount DECIMAL(10, 2)  </span><br><span class="line">                 ) WITH (  </span><br><span class="line">                   &#x27;connector&#x27; = &#x27;filesystem&#x27;,  </span><br><span class="line">                   &#x27;path&#x27; = &#x27;file:///Users/zhouqingfeng/Desktop/mydirect/tmp/flink/v1&#x27;,  </span><br><span class="line">                   &#x27;format&#x27; = &#x27;json&#x27;  </span><br><span class="line">                 ) </span><br><span class="line"></span><br><span class="line">--insert source -&gt; dest</span><br><span class="line">INSERT INTO orders_history </span><br><span class="line">                SELECT * FROM orders</span><br></pre></td></tr></table></figure>



<h4><span id="blackhole">BlackHole</span></h4><p>The BlackHole connector allows for swallowing all input records. It is designed for:</p>
<ul>
<li>high performance testing.</li>
<li>UDF to output, not substantive sink.</li>
</ul>
<p>Just like &#x2F;dev&#x2F;null device on Unix-like operating systems.</p>
<p>The BlackHole connector is built-in.</p>
<h4><span id="print">Print</span></h4><p>The Print connector allows for writing every row to the standard output or standard error stream.</p>
<p>It is designed for:</p>
<ul>
<li>Easy test for streaming job.</li>
<li>Very useful in production debugging.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String outputSql = &quot;CREATE TABLE print_table (\n&quot; +</span><br><span class="line">        &quot;    id  int,\n&quot; +</span><br><span class="line">        &quot;    name      string \n&quot; +</span><br><span class="line">        &quot;        ) WITH (\n&quot; +</span><br><span class="line">        &quot;                &#x27;connector&#x27; = &#x27;print&#x27;\n&quot; +</span><br><span class="line">        &quot;        )&quot;;</span><br></pre></td></tr></table></figure>



<h4><span id="jdbc">JDBC</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">String inputSql= &quot;CREATE TABLE myUser2 (\n&quot; +</span><br><span class="line">        &quot;    id  int,\n&quot; +</span><br><span class="line">        &quot;    name      string \n&quot; +</span><br><span class="line">        &quot;) WITH (\n&quot; +</span><br><span class="line">        &quot;  &#x27;connector&#x27;  = &#x27;jdbc&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;url&#x27;        = &#x27;jdbc:mysql://localhost:3306/test_zhou&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;table-name&#x27; = &#x27;users2&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;driver&#x27;     = &#x27;com.mysql.cj.jdbc.Driver&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;username&#x27;   = &#x27;root&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;password&#x27;   = &#x27;Zoom@123&#x27;\n&quot; +</span><br><span class="line">        &quot;)&quot;;</span><br></pre></td></tr></table></figure>



<h4><span id="kafka">Kafka</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE orders (   </span><br><span class="line">                   order_id BIGINT,   </span><br><span class="line">                   product_id BIGINT,   </span><br><span class="line">                   order_time TIMESTAMP(3),   </span><br><span class="line">                   order_amount DECIMAL(10, 2)   </span><br><span class="line">                 ) WITH (   </span><br><span class="line">                   &#x27;connector&#x27; = &#x27;kafka&#x27;,   </span><br><span class="line">                   &#x27;topic&#x27; = &#x27;testzq1&#x27;,   </span><br><span class="line">                   &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,   </span><br><span class="line">                   &#x27;properties.group.id&#x27; = &#x27;orders-consumer&#x27;,   </span><br><span class="line">                   &#x27;scan.startup.mode&#x27; = &#x27;latest-offset&#x27;,   </span><br><span class="line">                   &#x27;format&#x27; = &#x27;json&#x27;   </span><br><span class="line">                 )</span><br></pre></td></tr></table></figure>





<h2><span id="references">References</span></h2><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/overview/">Flink1.17 sql</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/" data-id="clk0zo6b70000cu9a9fpjej36" data-title="flink_sql_20230713" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flink/" rel="tag">flink</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_env/spark_terminal_tips" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/13/bigdata/spark/spark_env/spark_terminal_tips/" class="article-date">
  <time class="dt-published" datetime="2023-07-13T07:35:03.000Z" itemprop="datePublished">2023-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark-env/">bigdata/spark/spark_env</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/13/bigdata/spark/spark_env/spark_terminal_tips/">spark_terminal_tips</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3><span id="spark-shell">Spark-shell</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exit: :quit</span><br><span class="line">多行输入: :paste</span><br><span class="line">dataset 截断设置：dataset.show(false)</span><br></pre></td></tr></table></figure>



<h3><span id="spark-sql">Spark-sql</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sparksql 输出table head: spark.hadoop.hive.cli.print.header=true</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/13/bigdata/spark/spark_env/spark_terminal_tips/" data-id="clk0u7fk900009p9a7x29dzo0" data-title="spark_terminal_tips" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230712_spark_delta_lake" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/13/bigdata/spark/spark3/20230712_spark_delta_lake/" class="article-date">
  <time class="dt-published" datetime="2023-07-13T07:27:47.000Z" itemprop="datePublished">2023-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/13/bigdata/spark/spark3/20230712_spark_delta_lake/">20230713_delta_lake</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#env">Env</a></li>
<li><a href="#examples-test">Examples Test</a></li>
<li><a href="#batch">Batch</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E8%B5%84%E6%96%99">资料</a></li>
</ul>
<!-- tocstop -->

<h2><span id="env">Env</span></h2><p>Spark 3.4.1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;io.delta&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;delta-core_2.12&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-sql --packages io.delta:delta-core_2.12:2.4.0 \</span><br><span class="line">--conf &quot;spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension&quot; \</span><br><span class="line">--conf &quot;spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog&quot; \</span><br><span class="line">--conf spark.hadoop.hive.cli.print.header=true</span><br><span class="line"></span><br><span class="line">bin/spark-shell --packages io.delta:delta-core_2.12:2.4.0 \</span><br><span class="line">--conf &quot;spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension&quot; \</span><br><span class="line">--conf &quot;spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog&quot; \</span><br><span class="line">--conf spark.hadoop.hive.cli.print.header=true</span><br></pre></td></tr></table></figure>



<h2><span id="examples-test">Examples Test</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># create delta table, 这里是设定了表的路径, 未指定表名</span><br><span class="line">CREATE TABLE delta.`/tmp/delta-table` USING DELTA AS SELECT col1 as id FROM VALUES 0,1,2,3,4;</span><br><span class="line"></span><br><span class="line">#overwrite</span><br><span class="line">INSERT OVERWRITE delta.`/tmp/delta-table` SELECT col1 as id FROM VALUES 5,6,7,8,9;</span><br><span class="line"></span><br><span class="line">-- Update every even value by adding 100 to it</span><br><span class="line">UPDATE delta.`/tmp/delta-table` SET id = id + 100 WHERE id % 2 == 0;</span><br><span class="line"></span><br><span class="line">-- Delete very even value</span><br><span class="line">DELETE FROM delta.`/tmp/delta-table` WHERE id % 2 == 0;</span><br><span class="line"></span><br><span class="line">-- Upsert (merge) new data</span><br><span class="line">CREATE TEMP VIEW newData AS SELECT col1 AS id FROM VALUES 1,3,5,7,9,11,13,15,17,19;</span><br><span class="line"></span><br><span class="line">MERGE INTO delta.`/tmp/delta-table` AS oldData</span><br><span class="line">USING newData</span><br><span class="line">ON oldData.id = newData.id</span><br><span class="line">WHEN MATCHED</span><br><span class="line">  THEN UPDATE SET id = newData.id</span><br><span class="line">WHEN NOT MATCHED</span><br><span class="line">  THEN INSERT (id) VALUES (newData.id);</span><br><span class="line"></span><br><span class="line">SELECT * FROM delta.`/tmp/delta-table`;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- 查询快照：历史版本</span><br><span class="line">-- Read older versions of data using time travel</span><br><span class="line">SELECT * FROM delta.`/tmp/delta-table` VERSION AS OF 0;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--struct stream write to delta</span><br><span class="line">val streamingDf = spark.readStream.format(&quot;rate&quot;).load()</span><br><span class="line">val stream = streamingDf.select($&quot;value&quot; as &quot;id&quot;).writeStream.format(&quot;delta&quot;).option(&quot;checkpointLocation&quot;, &quot;/tmp/checkpoint&quot;).start(&quot;/tmp/delta-table2&quot;)</span><br><span class="line"></span><br><span class="line">--struct stream read from delta</span><br><span class="line"></span><br><span class="line">val stream2 = spark.readStream.format(&quot;delta&quot;).load(&quot;/tmp/delta-table2&quot;).writeStream.format(&quot;console&quot;).start()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="batch">Batch</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS default.people10m (</span><br><span class="line">  id INT,</span><br><span class="line">  firstName STRING,</span><br><span class="line">  middleName STRING,</span><br><span class="line">  lastName STRING,</span><br><span class="line">  gender STRING,</span><br><span class="line">  birthDate TIMESTAMP,</span><br><span class="line">  ssn STRING,</span><br><span class="line">  salary INT</span><br><span class="line">) USING DELTA</span><br><span class="line"></span><br><span class="line">insert into default.people10m values(1, &#x27;jimmy&#x27;, &#x27;dell&#x27;, &#x27;li&#x27;, &#x27;m&#x27;, &#x27;2001-11-12 12:21:21&#x27;, &#x27;ssn1&#x27;, 2600);</span><br><span class="line">insert into default.people10m values(2, &#x27;Jack&#x27;, &#x27;dell&#x27;, &#x27;li&#x27;, &#x27;m&#x27;, &#x27;2002-10-12 12:21:21&#x27;, &#x27;ssn1&#x27;, 2600);</span><br><span class="line">insert into default.people10m values(3, &#x27;lisa&#x27;, &#x27;dell&#x27;, &#x27;li&#x27;, &#x27;f&#x27;, &#x27;2001-09-16 12:21:21&#x27;, &#x27;ssn1&#x27;, 2600);</span><br><span class="line"></span><br><span class="line">DESCRIBE HISTORY default.people10m</span><br><span class="line">SELECT * FROM default.people10m TIMESTAMP AS OF &#x27;2018-10-18T22:15:12.013Z&#x27;</span><br><span class="line">SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="总结">总结</span></h2><p>Delta lake是数据湖的一种开源实现，</p>
<p>它是一种新型的存储格式(tableFormat)，可以看成是对传统数据仓库功能的扩展，弥补了传统数据仓库的不足</p>
<p>(1)  统一存储，相当于是大一统的储存，传统数仓主要是存储结构化数据，湖存储可以存储结构化、半结构化、非结构化数据。（表、音频、视频、文本等，底层是hdfs | oss对象存储，对象存储本身适合存储任意格式的数据）</p>
<p>(2)  支持事务，满足ACID，传统数仓不支持事务</p>
<p>(3)  流批统一。delta 表可以作为流计算的输入|输出，也可以作为批计算的输入|输出</p>
<p>(4)  支持time travel。可以存储数据的多个快照版本，查看历史版本数据。</p>
<p>(5)  支持upsert 和delete操作</p>
<p>(6) Scalable metadata handling: 支持管理超大规模表对应的元数据</p>
<h2><span id="资料">资料</span></h2><p><a target="_blank" rel="noopener" href="https://delta.io/">官网</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/13/bigdata/spark/spark3/20230712_spark_delta_lake/" data-id="clk0u7fkf00019p9aaj3hhad7" data-title="20230713_delta_lake" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230712_pyspark" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/12/bigdata/spark/spark3/20230712_pyspark/" class="article-date">
  <time class="dt-published" datetime="2023-07-12T14:54:38.000Z" itemprop="datePublished">2023-07-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/12/bigdata/spark/spark3/20230712_pyspark/">20230712_pyspark</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->



<!-- tocstop -->

<p>Pandas</p>
<p>用于数据分析、数据清洗</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/12/bigdata/spark/spark3/20230712_pyspark/" data-id="cljzvov5q0000pv9agj6hfomf" data-title="20230712_pyspark" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230711_structStreaming" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/" class="article-date">
  <time class="dt-published" datetime="2023-07-11T08:52:34.000Z" itemprop="datePublished">2023-07-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/">20230711_structStreaming</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E">执行引擎</a><ul>
<li><a href="#example">Example</a><ul>
<li><a href="#structstreaming">StructStreaming</a></li>
<li><a href="#batchprocessing">BatchProcessing</a></li>
</ul>
</li>
<li><a href="#basics">Basics</a><ul>
<li><a href="#outputmode"><strong>outputmode</strong></a></li>
<li><a href="#event-time">Event time</a></li>
<li><a href="#time-window">Time Window</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#fault-tolerance-semantics">Fault Tolerance Semantics</a></li>
<li><a href="#source">Source</a><ul>
<li><a href="#socket">Socket</a></li>
<li><a href="#file">File</a></li>
<li><a href="#rate">Rate</a></li>
</ul>
</li>
<li><a href="#%E5%BB%B6%E6%97%B6%E4%B9%B1%E5%BA%8F%E6%95%B0%E6%8D%AE">延时乱序数据</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%89">定义</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95">测试</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li>
</ul>
</li>
<li><a href="#%E7%8A%B6%E6%80%81%E5%AD%98%E5%82%A8">状态存储</a><ul>
<li><a href="#hdfs-state-store-provider">HDFS state store provider</a></li>
<li><a href="#rocksdb-state-store-implementation">RocksDB state store implementation</a></li>
</ul>
</li>
<li><a href="#sink">Sink</a></li>
<li><a href="#trigger">Trigger</a></li>
<li><a href="#checkpoint">checkpoint</a></li>
<li><a href="#kafka%E9%9B%86%E6%88%90">Kafka集成</a><ul>
<li><a href="#%E8%AF%BB%E5%8F%96kafka%E5%86%99%E5%85%A5console">读取kafka写入console</a></li>
<li><a href="#%E8%AF%BB%E5%8F%96kafka-%E6%99%AE%E9%80%9A%E8%81%9A%E5%90%88%E4%B8%8D%E5%90%AB%E7%AA%97%E5%8F%A3">读取kafka 普通聚合(不含窗口)</a></li>
<li><a href="#%E8%AF%BB%E5%8F%96kafka-%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%90%AB%E6%B0%B4%E5%8D%B0">读取kafka 窗口聚合(含水印)</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="执行引擎">执行引擎</span></h2><p>Spark SQL engine</p>
<p>两种处理模式：</p>
<p>默认 <em>micro-batch processing</em> engine,  最低延时 100ms，exactly-once fault-tolerance guarantees</p>
<p>可配置模式<strong>Continuous Processing</strong>，最低延时 1ms，at-least-once guarantees</p>
<p>Internally, by default, Structured Streaming queries are processed using a <em>micro-batch processing</em> engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called <strong>Continuous Processing</strong>, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset&#x2F;DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.</p>
<h3><span id="example">Example</span></h3><h4><span id="structstreaming">StructStreaming</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * start server: nc -lk 9999</span><br><span class="line"> * 可以看到这里 的api写法基本跟batch processing 批处理的api保持一致，</span><br><span class="line"> * 不同的地方是：</span><br><span class="line"> * (1) spark.readStream | wordCounts.writeStream，batch api的写法 spark.read | wordCounts.write</span><br><span class="line"> * (2) 流处理触发运行，调用方法wordCounts.writeStream.start()，而批处理调用方法 wordCounts.write.save()</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleStructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCount&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">    val lines = spark.readStream</span><br><span class="line">      .format(&quot;socket&quot;)</span><br><span class="line">      .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">      .option(&quot;port&quot;, 9999)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    // dataframe to dataset; then Split the lines into words</span><br><span class="line">    val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">    // Generate running word count</span><br><span class="line">    val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the running counts to the console</span><br><span class="line">    //console outputMode支持 update和complete, 有聚合计算的情况不支持append模式</span><br><span class="line">    val query = wordCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="batchprocessing">BatchProcessing</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetWordCountExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCount&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val lines = spark.read.format(&quot;text&quot;).load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/README.md&quot;)</span><br><span class="line"></span><br><span class="line">    // dataframe to dataset; then Split the lines into words</span><br><span class="line">    val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">    // Generate running word count</span><br><span class="line">    val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">    wordCounts.write.mode(&quot;overwrite&quot;).format(&quot;parquet&quot;).save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="basics">Basics</span></h3><h4><span id="outputmode"><strong>outputmode</strong></span></h4><p><em>Complete Mode</em> -   结果集 每次输出所有数据</p>
<p><em>Append Mode</em> -  结果集 每次只输出新增的数据（只包括新增，不包括变更的）</p>
<p><em>Update Mode</em> -  结果集 每次只输出更新的数据（包括新增 和 变更的  new|update）</p>
<h4><span id="event-time">Event time</span></h4><p>事件时间，只与事件本身有关</p>
<h4><span id="time-window">Time Window</span></h4><p>滚动窗口：Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals</p>
<p>滑动窗口：Sliding windows are similar to the tumbling windows from the point of being “fixed-sized”, but windows can overlap if the duration of slide is smaller than the duration of window, and in this case an input can be bound to the multiple windows.</p>
<p>会话窗口： 窗口在一定间隔内没有接收到新的数据，进行关闭，窗口大小不固定</p>
<p>a session window closes when there’s no input received within gap duration after receiving the latest input.</p>
<h2><span id="fault-tolerance-semantics">Fault Tolerance Semantics</span></h2><p>如何实现容错 和 exactly-once  端到端一致性保证：</p>
<p>Delivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming. To achieve that, we have designed the Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and&#x2F;or reprocessing. Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure <strong>end-to-end exactly-once semantics</strong> under any failure.</p>
<p>Source:  replayable </p>
<p>Sink: idempotent</p>
<p>Engine: checkpointing and write-ahead logs </p>
<h2><span id="source">Source</span></h2><h3><span id="socket">Socket</span></h3><p>对应example</p>
<h3><span id="file">File</span></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Read all the csv files written atomically in a directory</span></span><br><span class="line"><span class="keyword">val</span> userSchema = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;string&quot;</span>).add(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;integer&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> csvDF = spark</span><br><span class="line">  .readStream</span><br><span class="line">  .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">  .schema(userSchema)      <span class="comment">// Specify schema of the csv files</span></span><br><span class="line">  .csv(<span class="string">&quot;/path/to/directory&quot;</span>)    <span class="comment">// Equivalent to format(&quot;csv&quot;).load(&quot;/path/to/directory&quot;)</span></span><br></pre></td></tr></table></figure>



<h3><span id="rate">Rate</span></h3><p>Generates data at the specified number of rows per second, each output row contains a <code>timestamp</code> and <code>value</code>. Where <code>timestamp</code> is a <code>Timestamp</code> type containing the time of message dispatch, and <code>value</code> is of <code>Long</code> type containing the message count, starting from 0 as the first row. </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a streaming DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = spark.readStream</span><br><span class="line">  .format(<span class="string">&quot;rate&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;rowsPerSecond&quot;</span>, <span class="number">10</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> query = df.writeStream</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/out/rate/&quot;</span>)</span><br><span class="line">  .outputMode(<span class="string">&quot;update&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;numRows&quot;</span>, <span class="number">20</span>)</span><br><span class="line">  .option(<span class="string">&quot;truncate&quot;</span>, <span class="literal">false</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"> query.awaitTermination()</span><br></pre></td></tr></table></figure>



<h2><span id="延时乱序数据">延时乱序数据</span></h2><h3><span id="定义">定义</span></h3><p><strong>Watermark</strong>: 水位线</p>
<p>Watemark &#x3D; the current max  event time - late threshold</p>
<p>基于事件时间产生，本质是一个时间戳，和时间窗口结合，用于处理延时晚到的乱序时间。随着事件的不断流入(到计算逻辑flink或spark代码)，水位线不断上涨，当水位线 &gt;&#x3D; 窗口结束时间，流入到这个窗口的新的延时数据将不再被该窗口计算处理。</p>
<p>水位线作用和定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用来处理数据延迟，数据乱序。</span><br><span class="line"></span><br><span class="line">水印（更准确的说法是水位线，随着接收事件时间的不断增长而不断上涨）本质上是一个时间戳，代表事件时间比这个时间戳更小的数据已全部到达，（所有比这时间戳更早的数据视为延迟数据）当水印watermark&gt;=窗口对应的结束时间，对应窗口会触发计算并进行关闭</span><br></pre></td></tr></table></figure>



<p><strong>水印只与事件时间有关</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark 水印 和flink的水印 均是来自google的dataflow model,  其核心思想是在无边界、乱序的数据流中，在数据准确性、数据延迟性和计算性能开销之间 追求一个平衡。</span><br><span class="line">水位线本质是事件时间戳，仅仅只与 事件本身对应的时间属性event_time 和用户定义的允许数据延迟的时间（late threshold）有关，与系统本身的处理时间processing time 没有关系。 「之前一直理解不了，将processing time 和event time混淆到一块，误以为event time 和processing time时钟是同步的，其实两者是没有关系的，一个是事件本身决定的，一个是计算系统处理时间，我们通常讲窗口、水印是针对事件而言的，而现实中事件时间和处理时间可能中间间隔了好几个小时，水印本质解决的问题是有关在事件中多条数据乱序到达的问题，只能依赖与事件本身的时间去想办法解决，通过引入水位线的概念（由事件时间生成，随着数据的不断流入水位线不断上涨），当水位线&gt;=某个窗口结束时间（当然，这里窗口也是完全依据流入的数据事件时间划分的） 意味着在这之后流入到这个窗口的新的数据将不再被该窗口计算处理 」</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="测试">测试</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 测试水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> * 1、start server: nc -lk 9999</span><br><span class="line"> * 2、params: localhost 9999 10 (设置滚动窗口 10s间隔)</span><br><span class="line"> *</span><br><span class="line"> * 3、late threshold = 10s</span><br><span class="line"> */</span><br><span class="line">object ExampleWatermark_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if (args.length &lt; 3) &#123;</span><br><span class="line">      System.err.println(&quot;Usage: StructuredNetworkWordCountWindowed &lt;hostname&gt; &lt;port&gt;&quot; +</span><br><span class="line">        &quot; &lt;window duration in seconds&gt; [&lt;slide duration in seconds&gt;]&quot;)</span><br><span class="line">      System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val host = args(0)</span><br><span class="line">    val port = args(1).toInt</span><br><span class="line">    val windowSize = args(2).toInt</span><br><span class="line">    val slideSize = if (args.length == 3) windowSize else args(3).toInt</span><br><span class="line">    if (slideSize &gt; windowSize) &#123;</span><br><span class="line">      System.err.println(&quot;&lt;slide duration&gt; must be less than or equal to &lt;window duration&gt;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val windowDuration = s&quot;$windowSize seconds&quot;</span><br><span class="line">    val slideDuration = s&quot;$slideSize seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCountWindowed&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line">    val lines = spark.readStream</span><br><span class="line">      .format(&quot;socket&quot;)</span><br><span class="line">      .option(&quot;host&quot;, host)</span><br><span class="line">      .option(&quot;port&quot;, port)</span><br><span class="line">      //      .option(&quot;includeTimestamp&quot;, true)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    // Split the lines into words, retaining timestamps</span><br><span class="line">    //    val words = lines.as[(String, Timestamp)].flatMap(line =&gt;</span><br><span class="line">    //      line._1.split(&quot; &quot;).map(word =&gt; (word, line._2))</span><br><span class="line">    //    ).toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">    val query = windowedCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>注意</strong>：spark 水印只在append 模式和update模式下有作用，在complete模式下没有任何作用，所有数据都会输出。</p>
<p>在上述example中，</p>
<p>Terminal 开启端口本地测试 （nc -lk 9000 10），窗口为10s, 最大延迟时间也是10s,</p>
<p>依次每行输入数据，</p>
<p>spark,2020-09-04 10:02:41<br>spark,2020-09-04 10:02:42<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:48<br>spark,2020-09-04 10:02:21<br>spark,2020-09-04 10:02:31<br>spark,2020-09-04 10:02:32<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:51<br>spark,2020-09-04 10:02:59<br>spark,2020-09-04 10:02:53<br>spark,2020-09-04 10:03:06</p>
<p>spark,2020-09-04 10:02:45</p>
<p>spark,2020-09-04 10:03:07<br>spark,2020-09-04 10:02:53<br>spark,2020-09-04 10:03:08<br>spark,2020-09-04 10:03:13<br>spark,2020-09-04 10:03:13<br>spark,2020-09-04 10:03:33<br>spark,2020-09-04 10:03:36</p>
<p>前四条数据依次输入，</p>
<p>spark,2020-09-04 10:02:41<br>spark,2020-09-04 10:02:42<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:48</p>
<p>输出结果为：</p>
<p>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]|spark|4    |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（可以看到窗口是[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]，当时大概是11点30分测试的，这里划定的窗口完全是由事件时间决定的，与其他没有关系，这里水位线watermark=2020-09-04 10:02:49 - 10s = 2020-09-04 10:02:39， 意味着窗口结束时间T                                      2020-09-04 10:02:39 &gt;=T, 对应的流入该窗口内的新数据不再被处理，最接近水位线的窗口是                                                                     [2020-09-04 10:02:20.0,2020-09-04 10:02:30.0] ,  因此后面接着输入了一条位于该窗口内的新的数据，spark,2020-09-04 10:02:21，但是没有结果输出，因为水位线高过了窗口结束时间T = 2020-09-04 10:02:30.0）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:21，</p>
<p>没有结果输出</p>
<p>输入spark,2020-09-04 10:02:31，</p>
<p>输出结果为：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]|spark|1    |<br>+———————————————+—–+—–+</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 &gt;= 2020-09-04 10:02:40.0 不成立， 老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据继续被处理）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:32，</p>
<p>输出结果为：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]|spark|2    |<br>+———————————————+—–+—–+    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 &gt;= 2020-09-04 10:02:40.0 不成立，老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据继续被处理 ）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:49，</p>
<p>输出：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]|spark|5    |    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 ）</span><br></pre></td></tr></table></figure>

<p>输入 </p>
<p>spark,2020-09-04 10:02:51<br>spark,2020-09-04 10:02:59<br>spark,2020-09-04 10:02:53</p>
<p>输出</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:50.0,2020-09-04 10:03:00.0]|spark|3    |<br>+———————————————+—–+—–+       </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线上涨2020-09-04 10:02:49 &gt;= 2020-09-04 10:02:40.0 成立 ，有新窗口出现，老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据不再处理）</span><br></pre></td></tr></table></figure>

<p>输入 spark,2020-09-04 10:03:06 </p>
<p>输出 </p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:03:00.0,2020-09-04 10:03:10.0]|spark|1    |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线上涨 2020-09-04 10:02:56 &gt;= 2020-09-04 10:02:50.0 成立 ，有新窗口出现，老的窗口 [2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]  流入的新数据不再处理）</span><br></pre></td></tr></table></figure>

<p>输入 spark,2020-09-04 10:02:45 </p>
<p>输出为，没有结果输出</p>
<p>+——+—-+—–+<br>|window|word|count|<br>+——+—-+—–+<br>+——+—-+—–+</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:56 保持不变，老的窗口 [2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]  流入的新数据不再处理，因此流入 位于老窗口的 新数据spark,2020-09-04 10:02:45后，老窗口结果没更新）</span><br></pre></td></tr></table></figure>



<h3><span id="结论">结论</span></h3><p>Flink&#x2F;spark  用事件时间 + 窗口 + 水印来解决实际生产中的数据乱序问题，有如下的触发条件：</p>
<p>watermark 时间 &gt;&#x3D; window_end_time (后续到达的落入之前窗口的数据被丢弃掉)；</p>
<p>在 [window_start_time,window_end_time) 中有数据存在，这个窗口是左闭右开的。</p>
<p>此外，WaterMark 的生成是以对象的形式发送到下游，同样会消耗内存，因此水印的生成时间和频率都要进行严格控制，否则会影响我们的正常作业。</p>
<h2><span id="状态存储">状态存储</span></h2><h3><span id="hdfs-state-store-provider">HDFS state store provider</span></h3><p>The HDFS backend state store provider is the default implementation of [[StateStoreProvider]] and [[StateStore]] in which all the data is stored in memory map in the first stage, and then backed by files in an HDFS-compatible file system. 占用executor内存</p>
<h3><span id="rocksdb-state-store-implementation">RocksDB state store implementation</span></h3><p>(嵌入式key-value数据库，不需要额外安装）不消耗executor内存，适用于大状态存储</p>
<p>3.2 以及之后提供支持。</p>
<p>As of Spark 3.2, we add a new built-in state store implementation, RocksDB state store provider.</p>
<p>Rather than keeping the state in the JVM memory, this solution uses RocksDB to efficiently manage the state in the native memory and the local disk.</p>
<p>目前理解是：rocksdb存储状态，替代之前状态存储占用executor 内存，</p>
<p>checkpoint两者都需要配置，checkpoint是在任务故障恢复或重启时用到。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.config(&quot;spark.sql.streaming.stateStore.providerClass&quot;, &quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;)</span><br></pre></td></tr></table></figure>



<h2><span id="sink">Sink</span></h2><p>There are a few types of built-in output sinks.</p>
<p>官方目前只有file 支持exactly-one, kafka支持at-least once</p>
<ul>
<li><strong>File sink</strong> - Stores the output to a directory.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;parquet&quot;)        // can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span><br><span class="line">    .option(&quot;path&quot;, &quot;path/to/destination/dir&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;kafka&quot;)</span><br><span class="line">    .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">    .option(&quot;topic&quot;, &quot;updates&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. See later in the section for more details.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Console sink (for debugging)</strong> - Prints the output to the console&#x2F;stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;console&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;memory&quot;)</span><br><span class="line">    .queryName(&quot;tableName&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<p><strong>ForeachBatch</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: <span class="type">DataFrame</span>, batchId: <span class="type">Long</span>) =&gt;</span><br><span class="line">  batchDF.persist()</span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 1</span></span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 2</span></span><br><span class="line">  batchDF.unpersist()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="trigger">Trigger</span></h2><p>触发器，多长时间执行一次</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Default trigger (runs micro-batch as soon as it can)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ProcessingTime trigger with two-seconds micro-batch interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">&quot;2 seconds&quot;</span>))</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// One-time trigger (Deprecated, encouraged to use Available-now trigger)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Once</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Available-now trigger</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">AvailableNow</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Continuous trigger with one-second checkpointing interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Continuous</span>(<span class="string">&quot;1 second&quot;</span>))</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>



<h2><span id="checkpoint">checkpoint</span></h2><p>任务失败恢复、重启  （Recovering from Failures with Checkpointing）</p>
<p>In case of a failure or intentional shutdown, you can recover the previous progress and state of a previous query, and continue where it left off. This is done using checkpointing and write-ahead logs. You can configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/structured-streaming-programming-guide.html#quick-example">quick example</a>) to the checkpoint location. This checkpoint location has to be a path in an HDFS compatible file system, and can be set as an option in the DataStreamWriter when <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/structured-streaming-programming-guide.html#starting-streaming-queries">starting a query</a>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;path/to/HDFS/dir&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;memory&quot;</span>)</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>





<h2><span id="kafka集成">Kafka集成</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql-kafka-0-10_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka写入console">读取kafka写入console</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> * late threshold = 10s</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; console （watermark + window）</span><br><span class="line"> *</span><br><span class="line"> * 输入： spark,2020-09-04 10:02:49</span><br><span class="line"> * 输出：</span><br><span class="line"> * +------------------------------------------+-----+-----+</span><br><span class="line"> * |window                                    |word |count|</span><br><span class="line"> * +------------------------------------------+-----+-----+</span><br><span class="line"> * |&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">object ExampleKafka_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">    val query = windowedCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka-普通聚合不含窗口">读取kafka 普通聚合(不含窗口)</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; kafka （no window）</span><br><span class="line"> *</span><br><span class="line"> * 常规聚合计算   group by -&gt; count</span><br><span class="line"> * 累加统计 wordcount</span><br><span class="line"> *</span><br><span class="line"> * 输入： hadoop,2020-09-04 10:02:49</span><br><span class="line"> * 输出： hadoop,1</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleKafka3_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      //配置RocksDBStateStore, 默认HdfsStateStore</span><br><span class="line">      .config(&quot;spark.sql.streaming.stateStore.providerClass&quot;, &quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .groupBy(</span><br><span class="line">          $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    *</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|window                                    |word |count|</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line">    *</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">//    val query = windowedCounts</span><br><span class="line">//      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">//      .writeStream</span><br><span class="line">//      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">//      .format(&quot;console&quot;)</span><br><span class="line">//      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">//      .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Write key-value data from a DataFrame to a specific Kafka topic specified in an option</span><br><span class="line">    val query = windowedCounts</span><br><span class="line">      .selectExpr(&quot;concat_ws(&#x27;,&#x27;, word, cast(count as string)) as value&quot;)</span><br><span class="line">      .writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .option(&quot;checkpointLocation&quot;, &quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1/out1&quot;)</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;,  &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;topic&quot;, &quot;testzq2&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka-窗口聚合含水印">读取kafka 窗口聚合(含水印)</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> * late threshold = 10s</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; kafka （watermark + window）</span><br><span class="line"> *</span><br><span class="line"> * 输入： spark,2020-09-04 10:02:49</span><br><span class="line"> * 输出： &#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;,spark,1</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleKafka2_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    *</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|window                                    |word |count|</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line">    *</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">//    val query = windowedCounts</span><br><span class="line">//      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">//      .writeStream</span><br><span class="line">//      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">//      .format(&quot;console&quot;)</span><br><span class="line">//      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">//      .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Write key-value data from a DataFrame to a specific Kafka topic specified in an option</span><br><span class="line">    val query = windowedCounts</span><br><span class="line">      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">      .writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .option(&quot;checkpointLocation&quot;, &quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1/out&quot;)</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;,  &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;topic&quot;, &quot;testzq2&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/" data-id="cljy55ft700000n9a3c2vb7n4" data-title="20230711_structStreaming" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230707_sparksql" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/09/bigdata/spark/spark3/20230707_sparksql/" class="article-date">
  <time class="dt-published" datetime="2023-07-09T14:59:37.000Z" itemprop="datePublished">2023-07-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/09/bigdata/spark/spark3/20230707_sparksql/">20230707_sparksql</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#table-cache">Table Cache</a></li>
<li><a href="#config">Config</a></li>
<li><a href="#join-strategy-hint">Join Strategy Hint</a></li>
<li><a href="#coalesce-hints">Coalesce hints</a></li>
<li><a href="#operator">Operator</a></li>
<li><a href="#aqe">AQE</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C">数据倾斜</a></li>
<li><a href="#ddl">DDL</a><ul>
<li><a href="#cte">CTE</a></li>
<li><a href="#analyze">Analyze</a></li>
<li><a href="#cache-table">Cache table</a></li>
<li><a href="#refresh-table">REFRESH TABLE</a></li>
<li><a href="#msck-repair">msck repair</a></li>
<li><a href="#distribute-by">distribute by</a></li>
<li><a href="#cluster-by">cluster by</a></li>
<li><a href="#explain">explain</a></li>
<li><a href="#inline">Inline</a></li>
<li><a href="#file">File</a></li>
<li><a href="#join">Join</a><ul>
<li><a href="#semi-join"><strong>Semi Join</strong></a></li>
<li><a href="#anti-join"><strong>Anti Join</strong></a></li>
</ul>
</li>
<li><a href="#limit">limit</a></li>
<li><a href="#%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C">集合操作</a></li>
<li><a href="#tablesample">TABLESAMPLE</a></li>
<li><a href="#tvf">TVF</a></li>
<li><a href="#%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0">聚合函数</a><ul>
<li><a href="#ordered-set-aggregate-functions">Ordered-Set Aggregate Functions</a></li>
</ul>
</li>
<li><a href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0">窗口函数</a></li>
<li><a href="#pivot">PIVOT</a></li>
<li><a href="#unpivot">UNPIVOT</a></li>
<li><a href="#lateral">Lateral</a></li>
</ul>
</li>
<li><a href="#dml">DML</a><ul>
<li><a href="#insert-into">Insert into</a></li>
<li><a href="#insert-overwrite-directory">Insert overwrite directory</a><ul>
<li><a href="#spark-format">Spark format</a></li>
<li><a href="#hive-format">Hive format</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="table-cache">Table Cache</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.catalog.cacheTable(&quot;tableName&quot;)</span><br><span class="line">spark.catalog.uncacheTable(&quot;tableName&quot;)</span><br><span class="line"></span><br><span class="line">dataFrame.cache()</span><br><span class="line">dataFrame.unpersist()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="config">Config</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.autoBroadcastJoinThreshold  	10485760 (10m)</span><br><span class="line">Configures the maximum size in bytes for a table that will be broadcast</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="join-strategy-hint">Join Strategy Hint</span></h2><p>The join strategy hints, namely <code>BROADCAST</code>, <code>MERGE</code>, <code>SHUFFLE_HASH</code> and <code>SHUFFLE_REPLICATE_NL</code>, instruct Spark to use the hinted strategy.</p>
<p><code>BROADCAST</code> 细分为两种：broadcast hash join or broadcast nested loop join </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.table(<span class="string">&quot;src&quot;</span>).join(spark.table(<span class="string">&quot;records&quot;</span>).hint(<span class="string">&quot;broadcast&quot;</span>), <span class="string">&quot;key&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">-- <span class="type">We</span> accept <span class="type">BROADCAST</span>, <span class="type">BROADCASTJOIN</span> and <span class="type">MAPJOIN</span> <span class="keyword">for</span> broadcast hint</span><br><span class="line"><span class="type">SELECT</span> <span class="comment">/*+ BROADCAST(r) */</span> * <span class="type">FROM</span> records r <span class="type">JOIN</span> src s <span class="type">ON</span> r.key = s.key</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="coalesce-hints">Coalesce hints</span></h2><p>Coalesce hints allow Spark SQL users to control the number of output files just like <code>coalesce</code>, <code>repartition</code> and <code>repartitionByRange</code> in the Dataset API, they can be used for performance tuning and reducing the number of output files.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SELECT /*+ COALESCE(3) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION(3) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION(c) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION(3, c) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE(3) */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE(c) */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE(3, c) */ * FROM t;</span><br></pre></td></tr></table></figure>



<h2><span id="operator">Operator</span></h2><p>A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations.</p>
<p>支持 函数型算子 和  关系型算子 </p>
<p>Spark SQL 执行引擎：catalyst优化器和</p>
<p>tungsten优化(数据结构设计(基于unsaferow存储、内存页管理)和全阶段代码生成)</p>
<h2><span id="aqe">AQE</span></h2><p>AQE 是 Spark SQL 的一种动态优化机制，在运行时，每当 Shuffle Map 阶段执行完毕，AQE 都会结合这个阶段的统计信息，基于既定的规则动态地调整、修正尚未执行的逻辑计划和物理计划，来完成对原始查询语句的运行时优化。</p>
<p>Spark 3.2.0 以及之后的版本默认是开启状态。</p>
<p>Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0.  </p>
<p>Spark SQL can turn on and off AQE by <code>spark.sql.adaptive.enabled</code> as an umbrella configuration。</p>
<p>As of Spark 3.0, there are three major features in AQE: including coalescing post-shuffle partitions, converting sort-merge join to broadcast join, and skew join optimization.</p>
<p>（AQE 赖以优化的统计信息与 CBO 不同，这些统计信息并不是关于某张表或是哪个列，而是 Shuffle Map 阶段输出的中间文件。）</p>
<p>自动分区合并(拆分) : reduce  合并数据量小的分区，减少小文件数量</p>
<p>Join 策略调整：sort-merge join to broadcast join</p>
<p>Join倾斜优化：倾斜分区会被拆分成多个分区 (倾斜分区拆分为多个数据分区、对另外一张表对应的数据分区进行复制)</p>
<h2><span id="数据倾斜">数据倾斜</span></h2><p>单表引起的倾斜：</p>
<p>1、map端聚合</p>
<p>2、单次聚合改为分两次聚合完成 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct uid) from t1 group by day</span><br><span class="line">select count(1) from ( select day, uid from t1 group by day, uid ) tmp group by day</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>多表join引起的倾斜：</p>
<p>1、调整 spark.sql.autoBroadcastJoinThreshold,  尽量使用broadcastjoin</p>
<p>2、<code>spark.sql.adaptive.enabled</code>  开启AQE，实现Join倾斜自动优化，可能出现executor粒度的数据倾斜(倾斜分区划分的任务重复分配到一个executor)</p>
<p>3、使用两阶段shuffle，将倾斜key和非倾斜key分开处理，最后结果union。</p>
<p>对于非倾斜key，常规处理；</p>
<p>对于倾斜key，外表对应的key：加盐、添加随机后缀，内表对应的key：添加后缀、数据进行复制，内外表进行一阶段 join 聚合，第二阶段 将第一阶段聚合结果 的joinkey 去除后缀后，进行二次聚合。</p>
<h2><span id="ddl">DDL</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">--Create bucketed table through CTAS and CTE</span><br><span class="line">CREATE TABLE student_bucket</span><br><span class="line">    USING parquet</span><br><span class="line">    CLUSTERED BY (id) INTO 4 buckets (</span><br><span class="line">    WITH tmpTable AS (</span><br><span class="line">        SELECT * FROM student WHERE id &gt; 100</span><br><span class="line">    )</span><br><span class="line">    SELECT * FROM tmpTable</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CREATE TABLE student_parquet(id INT, name STRING, age INT) USING PARQUET</span><br><span class="line">    OPTIONS (</span><br><span class="line">      &#x27;parquet.bloom.filter.enabled&#x27;=&#x27;true&#x27;,</span><br><span class="line">      &#x27;parquet.bloom.filter.enabled#age&#x27;=&#x27;false&#x27;</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line"># 添加分区    </span><br><span class="line">ALTER TABLE table_identifier ADD [IF NOT EXISTS] </span><br><span class="line">    ( partition_spec [ partition_spec ... ] )</span><br><span class="line"></span><br><span class="line"># 刷新分区</span><br><span class="line">[MSCK] REPAIR TABLE table_identifier [&#123;ADD|DROP|SYNC&#125; PARTITIONS]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="cte">CTE</span></h3><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-cte.html">https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-cte.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CTE</span><br><span class="line"># Common Table Expression (CTE)</span><br><span class="line">A common table expression (CTE) defines a temporary result set that a user can reference possibly multiple times within the scope of a SQL statement. </span><br><span class="line">表示一个临时结果集，用户可以在sql语句中引用多次</span><br><span class="line">-- CTE with multiple column aliases</span><br><span class="line">WITH t(x, y) AS (SELECT 1, 2)</span><br><span class="line">SELECT * FROM t WHERE x = 1 AND y = 2;</span><br></pre></td></tr></table></figure>



<h3><span id="analyze">Analyze</span></h3><p>获取表的统计信息，包括表的大小、行数，也可以针对字段进行统计</p>
<p>The <code>ANALYZE TABLE</code> statement collects statistics about one specific table or all the tables in one specified database, that are to be used by the query optimizer to find a better query execution plan.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">analyze table emp compute statistics</span><br><span class="line"># noscan 使用noscan仅获取表大小, 不需要扫描整个表</span><br><span class="line"># Collects only the table’s size in bytes (which does not require scanning the entire table).</span><br><span class="line">analyze table emp compute statistics noscan;</span><br><span class="line">desc extended emp;</span><br></pre></td></tr></table></figure>



<h3><span id="cache-table">Cache table</span></h3><p><code>CACHE TABLE</code> statement caches contents of a table or output of a query with the given storage level. If a query is cached, then a temp view will be created for this query. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CACHE TABLE emp;</span><br><span class="line"></span><br><span class="line">CACHE TABLE emp OPTIONS (&#x27;storageLevel&#x27; &#x27;DISK_ONLY&#x27;) SELECT salary FROM emp</span><br><span class="line"></span><br><span class="line">CLEAR CACHE;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="refresh-table">REFRESH TABLE</span></h3><p>（Invalidates and refreshes all the cached data and metadata of the given table. For performance reasons, Spark SQL or the external data source library it uses might cache certain metadata about a table, such as the location of blocks (数据块的位置信息). When those change outside of Spark SQL, users should call this function to invalidate the cache.）</p>
<p>更新表的缓存信息，包括元数据等。主要用于表的数据修改、表中元数据未修改的场景。(无法更新分区，msck是更新分区信息)</p>
<p>Spark为了提高性能会缓存Parquet的元数据信息。当更新了Parquet表数据时，缓存的元数据信息未更新，导致Spark SQL查询不到新插入的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- The cached entries of the table will be refreshed  </span><br><span class="line">-- The table is resolved from the current database as the table name is unqualified.</span><br><span class="line"># 这个表的缓存信息 会更新</span><br><span class="line">REFRESH TABLE tbl1;</span><br><span class="line"></span><br><span class="line">##测试的情况，parquet表的数据目录新增数据后(手动增加)，spark sql查询不到新的数据，执行refresh，</span><br><span class="line">spark sql可以查询到新的数据</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>和msck区别：</p>
<p>msck repair table </p>
<p>作用是检查HDFS目录下存在（不存在）但表的metastore中不存在（存在）的元数据信息，更新metastore到hive。</p>
<p>每次执行msck repair这个命令，都会检查所有分区的目录是否在hive元数据中存在，如果是每次新增一个分区的任务（daily的),那么使用这个语句将会越来越耗费时间，建议使用ALTER TABLE ADD PARTITION 命令。MSCK适合一次导入很多分区，需要将这些分区都更新到元数据信息中。</p>
<h3><span id="msck-repair">msck repair</span></h3><p>只能作用与分区表，否则提示信息(because it is not a partitioned table.)</p>
<p>更新分区元数据信息到hive ,  （适用于分区数据已更新，但是分区元数据未更新的情况）</p>
<p><code>REPAIR TABLE</code> recovers all the partitions in the directory of a table and updates the Hive metastore. When creating a table using <code>PARTITIONED BY</code> clause, partitions are generated and registered in the Hive metastore. However, if the partitioned table is created from existing data, partitions are not registered automatically in the Hive metastore. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">-- create a partitioned table from existing data /tmp/namesAndAges.parquet</span><br><span class="line">CREATE TABLE tt1 (employee_name STRING, salary INT, department string) USING parquet PARTITIONED BY (department) location &#x27;/tmp/test/spark_ext&#x27;;</span><br><span class="line"></span><br><span class="line">-- SELECT * FROM tt1 does not return results</span><br><span class="line">SELECT * FROM tt1;</span><br><span class="line"></span><br><span class="line">refresh table tt1;</span><br><span class="line">-- SELECT * FROM tt1 does not return results</span><br><span class="line">SELECT * FROM tt1;</span><br><span class="line"></span><br><span class="line">-- run REPAIR TABLE to recovers all the partitions</span><br><span class="line">msck REPAIR TABLE tt1;</span><br><span class="line"></span><br><span class="line">-- SELECT * FROM tt1  returns results</span><br><span class="line">SELECT * FROM tt1;</span><br><span class="line">James	3000	Sales</span><br><span class="line">Robert	4100	Sales</span><br><span class="line">James	3000	Sales</span><br><span class="line">Michael	4600	Sales</span><br><span class="line">Maria	3000	Finance</span><br><span class="line">Kumar	2000	Marketing</span><br><span class="line">Scott	3300	Finance</span><br><span class="line">Jen	3900	Finance</span><br><span class="line">Jeff	3000	Marketing</span><br><span class="line">Saif	4100	Sales</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="distribute-by">distribute by</span></h3><p>数据重分区</p>
<h3><span id="cluster-by">cluster by</span></h3><p>Distribute  by A sort by A</p>
<p>数据重分区 + 分区内排序</p>
<h3><span id="explain">explain</span></h3><p>explain  extended  select * from emp</p>
<h3><span id="inline">Inline</span></h3><p>values 创建临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- three rows with a table alias</span><br><span class="line">SELECT * FROM VALUES (&quot;one&quot;, 1), (&quot;two&quot;, 2), (&quot;three&quot;, null) AS data(a, b);</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="file">File</span></h3><p>sql查询文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM parquet.`examples/src/main/resources/users.parquet`;</span><br><span class="line"></span><br><span class="line">SELECT * FROM orc.`examples/src/main/resources/users.orc`;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="join">Join</span></h3><ul>
<li><p><strong>join_type</strong></p>
<p>Specifies the join type.</p>
<p><strong>Syntax:</strong></p>
<p><code>[ INNER ] | CROSS | LEFT [ OUTER ] | [ LEFT ] SEMI | RIGHT [ OUTER ] | FULL [ OUTER ] | [ LEFT ] ANTI</code></p>
</li>
</ul>
<h4><span id="semi-join"><strong>Semi Join</strong></span></h4><p>A semi join returns values from the left side of the relation that has a match with the right. It is also referred to as a left semi join.</p>
<p>只返回左表中的值，这些值在右表存在</p>
<p><strong>Syntax:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">relation [ LEFT ] SEMI JOIN relation [ join_criteria ]</span><br></pre></td></tr></table></figure>

<h4><span id="anti-join"><strong>Anti Join</strong></span></h4><p>An anti join returns values from the left relation that has no match with the right. It is also referred to as a left anti join.</p>
<p>只返回左表中的值，这些值在右表不存在</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">relation [ LEFT ] ANTI JOIN relation [ join_criteria ]</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">-- Use employee and department tables to demonstrate different type of joins.</span><br><span class="line">SELECT * FROM employee;</span><br><span class="line">+---+-----+------+</span><br><span class="line">| id| name|deptno|</span><br><span class="line">+---+-----+------+</span><br><span class="line">|105|Chloe|     5|</span><br><span class="line">|103| Paul|     3|</span><br><span class="line">|101| John|     1|</span><br><span class="line">|102| Lisa|     2|</span><br><span class="line">|104| Evan|     4|</span><br><span class="line">|106|  Amy|     6|</span><br><span class="line">+---+-----+------+</span><br><span class="line"></span><br><span class="line">SELECT * FROM department;</span><br><span class="line">+------+-----------+</span><br><span class="line">|deptno|   deptname|</span><br><span class="line">+------+-----------+</span><br><span class="line">|     3|Engineering|</span><br><span class="line">|     2|      Sales|</span><br><span class="line">|     1|  Marketing|</span><br><span class="line">+------+-----------+</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate inner join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee INNER JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate left join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee LEFT JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|105|Chloe|     5|       NULL|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">|104| Evan|     4|       NULL|</span><br><span class="line">|106|  Amy|     6|       NULL|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate right join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee RIGHT JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate full join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee FULL JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|106|  Amy|     6|       NULL|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|105|Chloe|     5|       NULL|</span><br><span class="line">|104| Evan|     4|       NULL|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate cross join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname FROM employee CROSS JOIN department;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|105|Chloe|     5|Engineering|</span><br><span class="line">|105|Chloe|     5|  Marketing|</span><br><span class="line">|105|Chloe|     5|      Sales|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|103| Paul|     3|  Marketing|</span><br><span class="line">|103| Paul|     3|      Sales|</span><br><span class="line">|101| John|     1|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|101| John|     1|      Sales|</span><br><span class="line">|102| Lisa|     2|Engineering|</span><br><span class="line">|102| Lisa|     2|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">|104| Evan|     4|Engineering|</span><br><span class="line">|104| Evan|     4|  Marketing|</span><br><span class="line">|104| Evan|     4|      Sales|</span><br><span class="line">|106|  Amy|     4|Engineering|</span><br><span class="line">|106|  Amy|     4|  Marketing|</span><br><span class="line">|106|  Amy|     4|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate semi join.</span><br><span class="line">SELECT * FROM employee SEMI JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+</span><br><span class="line">| id| name|deptno|</span><br><span class="line">+---+-----+------+</span><br><span class="line">|103| Paul|     3|</span><br><span class="line">|101| John|     1|</span><br><span class="line">|102| Lisa|     2|</span><br><span class="line">+---+-----+------+</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate anti join.</span><br><span class="line">SELECT * FROM employee ANTI JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+</span><br><span class="line">| id| name|deptno|</span><br><span class="line">+---+-----+------+</span><br><span class="line">|105|Chloe|     5|</span><br><span class="line">|104| Evan|     4|</span><br><span class="line">|106|  Amy|     6|</span><br><span class="line">+---+-----+------+</span><br></pre></td></tr></table></figure>

<h3><span id="limit">limit</span></h3><p>The <code>OFFSET</code> clause is used to specify the number of rows to skip before beginning to return rows returned by the <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/sql-ref-syntax-qry-select.html">SELECT</a> statement. In general, this clause is used in conjunction with <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/sql-ref-syntax-qry-select-orderby.html">ORDER BY</a> to ensure that the results are deterministic.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-- Skip the first two rows.</span><br><span class="line">SELECT name, age FROM person ORDER BY name OFFSET 2;</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">| John A| 18|</span><br><span class="line">| Mike A| 25|</span><br><span class="line">|Shone S| 16|</span><br><span class="line">|Zen Hui| 25|</span><br></pre></td></tr></table></figure>



<h3><span id="集合操作">集合操作</span></h3><ul>
<li><code>EXCEPT</code> or <code>MINUS</code></li>
<li><code>INTERSECT</code></li>
<li><code>UNION</code></li>
</ul>
<p>union  | union all    前者结果集去重，后者不去重</p>
<h3><span id="tablesample">TABLESAMPLE</span></h3><p>表数据采样</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM test;</span><br><span class="line">+--+----+</span><br><span class="line">|id|name|</span><br><span class="line">+--+----+</span><br><span class="line">| 5|Alex|</span><br><span class="line">| 8|Lucy|</span><br><span class="line">| 2|Mary|</span><br><span class="line">| 4|Fred|</span><br><span class="line">| 1|Lisa|</span><br><span class="line">| 9|Eric|</span><br><span class="line">|10|Adam|</span><br><span class="line">| 6|Mark|</span><br><span class="line">| 7|Lily|</span><br><span class="line">| 3|Evan|</span><br><span class="line">+--+----+</span><br><span class="line"></span><br><span class="line">SELECT * FROM test TABLESAMPLE (50 PERCENT);</span><br><span class="line">+--+----+</span><br><span class="line">|id|name|</span><br><span class="line">+--+----+</span><br><span class="line">| 5|Alex|</span><br><span class="line">| 2|Mary|</span><br><span class="line">| 4|Fred|</span><br><span class="line">| 9|Eric|</span><br><span class="line">|10|Adam|</span><br><span class="line">| 3|Evan|</span><br><span class="line">+--+----+</span><br><span class="line"></span><br><span class="line">SELECT * FROM test TABLESAMPLE (5 ROWS);</span><br></pre></td></tr></table></figure>



<h3><span id="tvf">TVF</span></h3><p>表值函数</p>
<p>返回多行数据</p>
<p>A table-valued function (TVF) is a function that returns a relation or a set of rows. There are two types of TVFs in Spark SQL:</p>
<ol>
<li>a TVF that can be specified in a FROM clause, e.g. range;</li>
<li>a TVF that can be specified in SELECT&#x2F;LATERAL VIEW clauses, e.g. explode.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">-- range call with end</span><br><span class="line">SELECT * FROM range(6 + cos(3));</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  0|</span><br><span class="line">|  1|</span><br><span class="line">|  2|</span><br><span class="line">|  3|</span><br><span class="line">|  4|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">-- range call with start and end</span><br><span class="line">SELECT * FROM range(5, 10);</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  5|</span><br><span class="line">|  6|</span><br><span class="line">|  7|</span><br><span class="line">|  8|</span><br><span class="line">|  9|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">-- range call with numPartitions</span><br><span class="line">SELECT * FROM range(0, 10, 2, 200);</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  0|</span><br><span class="line">|  2|</span><br><span class="line">|  4|</span><br><span class="line">|  6|</span><br><span class="line">|  8|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">-- range call with a table alias</span><br><span class="line">SELECT * FROM range(5, 8) AS test;</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  5|</span><br><span class="line">|  6|</span><br><span class="line">|  7|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">SELECT explode(array(10, 20));</span><br><span class="line">+---+</span><br><span class="line">|col|</span><br><span class="line">+---+</span><br><span class="line">| 10|</span><br><span class="line">| 20|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">SELECT inline(array(struct(1, &#x27;a&#x27;), struct(2, &#x27;b&#x27;)));</span><br><span class="line">+----+----+</span><br><span class="line">|col1|col2|</span><br><span class="line">+----+----+</span><br><span class="line">|   1|   a|</span><br><span class="line">|   2|   b|</span><br><span class="line">+----+----+</span><br><span class="line"></span><br><span class="line">SELECT posexplode(array(10,20));</span><br><span class="line">+---+---+</span><br><span class="line">|pos|col|</span><br><span class="line">+---+---+</span><br><span class="line">|  0| 10|</span><br><span class="line">|  1| 20|</span><br><span class="line">+---+---+</span><br><span class="line"></span><br><span class="line">SELECT stack(2, 1, 2, 3);</span><br><span class="line">+----+----+</span><br><span class="line">|col0|col1|</span><br><span class="line">+----+----+</span><br><span class="line">|   1|   2|</span><br><span class="line">|   3|null|</span><br><span class="line">+----+----+</span><br><span class="line"></span><br><span class="line">SELECT json_tuple(&#x27;&#123;&quot;a&quot;:1, &quot;b&quot;:2&#125;&#x27;, &#x27;a&#x27;, &#x27;b&#x27;);</span><br><span class="line">+---+---+</span><br><span class="line">| c0| c1|</span><br><span class="line">+---+---+</span><br><span class="line">|  1|  2|</span><br><span class="line">+---+---+</span><br><span class="line"></span><br><span class="line">SELECT parse_url(&#x27;http://spark.apache.org/path?query=1&#x27;, &#x27;HOST&#x27;);</span><br><span class="line">+-----------------------------------------------------+</span><br><span class="line">|parse_url(http://spark.apache.org/path?query=1, HOST)|</span><br><span class="line">+-----------------------------------------------------+</span><br><span class="line">|                                     spark.apache.org|</span><br><span class="line">+-----------------------------------------------------+</span><br><span class="line"></span><br><span class="line">-- Use explode in a LATERAL VIEW clause</span><br><span class="line">CREATE TABLE test (c1 INT);</span><br><span class="line">INSERT INTO test VALUES (1);</span><br><span class="line">INSERT INTO test VALUES (2);</span><br><span class="line">SELECT * FROM test LATERAL VIEW explode (ARRAY(3,4)) AS c2;</span><br><span class="line">+--+--+</span><br><span class="line">|c1|c2|</span><br><span class="line">+--+--+</span><br><span class="line">| 1| 3|</span><br><span class="line">| 1| 4|</span><br><span class="line">| 2| 3|</span><br><span class="line">| 2| 4|</span><br><span class="line">+--+--+</span><br></pre></td></tr></table></figure>



<h3><span id="聚合函数">聚合函数</span></h3><h4><span id="ordered-set-aggregate-functions">Ordered-Set Aggregate Functions</span></h4><p>These aggregate Functions use different syntax than the other aggregate functions so that to specify an expression (typically a column name) by which to order the values.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">CREATE OR REPLACE TEMPORARY VIEW basic_pays AS SELECT * FROM VALUES</span><br><span class="line">(&#x27;Diane Murphy&#x27;,&#x27;Accounting&#x27;,8435),</span><br><span class="line">(&#x27;Mary Patterson&#x27;,&#x27;Accounting&#x27;,9998),</span><br><span class="line">(&#x27;Jeff Firrelli&#x27;,&#x27;Accounting&#x27;,8992),</span><br><span class="line">(&#x27;William Patterson&#x27;,&#x27;Accounting&#x27;,8870),</span><br><span class="line">(&#x27;Gerard Bondur&#x27;,&#x27;Accounting&#x27;,11472),</span><br><span class="line">(&#x27;Anthony Bow&#x27;,&#x27;Accounting&#x27;,6627),</span><br><span class="line">(&#x27;Leslie Jennings&#x27;,&#x27;IT&#x27;,8113),</span><br><span class="line">(&#x27;Leslie Thompson&#x27;,&#x27;IT&#x27;,5186),</span><br><span class="line">(&#x27;Julie Firrelli&#x27;,&#x27;Sales&#x27;,9181),</span><br><span class="line">(&#x27;Steve Patterson&#x27;,&#x27;Sales&#x27;,9441),</span><br><span class="line">(&#x27;Foon Yue Tseng&#x27;,&#x27;Sales&#x27;,6660),</span><br><span class="line">(&#x27;George Vanauf&#x27;,&#x27;Sales&#x27;,10563),</span><br><span class="line">(&#x27;Loui Bondur&#x27;,&#x27;SCM&#x27;,10449),</span><br><span class="line">(&#x27;Gerard Hernandez&#x27;,&#x27;SCM&#x27;,6949),</span><br><span class="line">(&#x27;Pamela Castillo&#x27;,&#x27;SCM&#x27;,11303),</span><br><span class="line">(&#x27;Larry Bott&#x27;,&#x27;SCM&#x27;,11798),</span><br><span class="line">(&#x27;Barry Jones&#x27;,&#x27;SCM&#x27;,10586)</span><br><span class="line">AS basic_pays(employee_name, department, salary);</span><br><span class="line"></span><br><span class="line">SELECT * FROM basic_pays;</span><br><span class="line">+-----------------+----------+------+</span><br><span class="line">|    employee_name|department|salary|</span><br><span class="line">+-----------------+----------+------+</span><br><span class="line">|      Anthony Bow|Accounting|	6627|</span><br><span class="line">|      Barry Jones|       SCM| 10586|</span><br><span class="line">|     Diane Murphy|Accounting|	8435|</span><br><span class="line">|   Foon Yue Tseng|     Sales|	6660|</span><br><span class="line">|    George Vanauf|     Sales| 10563|</span><br><span class="line">|    Gerard Bondur|Accounting| 11472|</span><br><span class="line">| Gerard Hernandez|       SCM|	6949|</span><br><span class="line">|    Jeff Firrelli|Accounting|	8992|</span><br><span class="line">|   Julie Firrelli|     Sales|	9181|</span><br><span class="line">|       Larry Bott|       SCM| 11798|</span><br><span class="line">|  Leslie Jennings|        IT|	8113|</span><br><span class="line">|  Leslie Thompson|        IT|	5186|</span><br><span class="line">|      Loui Bondur|       SCM| 10449|</span><br><span class="line">|   Mary Patterson|Accounting|	9998|</span><br><span class="line">|  Pamela Castillo|       SCM| 11303|</span><br><span class="line">|  Steve Patterson|     Sales|	9441|</span><br><span class="line">|William Patterson|Accounting|	8870|</span><br><span class="line">+-----------------+----------+------+</span><br><span class="line"></span><br><span class="line"># percentile_disc是离散值(discrete)，percentile_cont是连续值</span><br><span class="line">SELECT</span><br><span class="line">    department,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) AS pc1,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pc2,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) AS pc3,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pc4,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) AS pd1,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pd2,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) AS pd3,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pd4</span><br><span class="line">FROM basic_pays</span><br><span class="line">GROUP BY department</span><br><span class="line">ORDER BY department;</span><br><span class="line">+----------+-------+--------+-------+--------+-----+-----+-----+-----+</span><br><span class="line">|department|    pc1|     pc2|    pc3|     pc4|  pd1|  pd2|  pd3|  pd4|</span><br><span class="line">+----------+-------+--------+-------+--------+-----+-----+-----+-----+</span><br><span class="line">|Accounting|8543.75| 7838.25| 9746.5|10260.75| 8435| 6627| 9998|11472|</span><br><span class="line">|        IT|5917.75|    NULL|7381.25|    NULL| 5186| NULL| 8113| NULL|</span><br><span class="line">|     Sales|8550.75|    NULL| 9721.5|    NULL| 6660| NULL|10563| NULL|</span><br><span class="line">|       SCM|10449.0|10786.25|11303.0|11460.75|10449|10449|11303|11798|</span><br><span class="line">+----------+-------+--------+-------+--------+-----+-----+-----+-----+</span><br></pre></td></tr></table></figure>



<h3><span id="窗口函数">窗口函数</span></h3><p><strong>window_function</strong></p>
<ul>
<li><p>Ranking Functions</p>
<p><strong>Syntax:</strong> <code>RANK | DENSE_RANK | PERCENT_RANK | NTILE | ROW_NUMBER</code></p>
</li>
<li><p>Analytic Functions</p>
<p><strong>Syntax:</strong> <code>CUME_DIST | LAG | LEAD | NTH_VALUE | FIRST_VALUE | LAST_VALUE</code></p>
</li>
<li><p>Aggregate Functions</p>
<p><strong>Syntax:</strong> <code>MAX | MIN | COUNT | SUM | AVG | ...</code></p>
</li>
</ul>
<h3><span id="pivot">PIVOT</span></h3><p>数据透视</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PIVOT ( &#123; aggregate_expression [ AS aggregate_expression_alias ] &#125; [ , ... ]</span><br><span class="line">    FOR column_list IN ( expression_list ) )</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">CREATE table emp111 AS select * from values</span><br><span class="line">(&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)</span><br><span class="line"></span><br><span class="line">CREATE OR REPLACE TEMPORARY VIEW emp11(dept, sex, salary) AS select * from values</span><br><span class="line">(&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)</span><br><span class="line"></span><br><span class="line">select * from emp11;</span><br><span class="line">dept	sex	salary</span><br><span class="line">dev	m	115</span><br><span class="line">sale	f	95</span><br><span class="line">sale	m	85</span><br><span class="line">dev	f	117</span><br><span class="line">dev	m	117</span><br><span class="line"></span><br><span class="line">SELECT * FROM emp11</span><br><span class="line">    PIVOT (</span><br><span class="line">        SUM(salary) AS sal_sum</span><br><span class="line">        FOR sex IN (&#x27;m&#x27;, &#x27;f&#x27;)</span><br><span class="line">    );</span><br><span class="line">-------    </span><br><span class="line">dept	m	f</span><br><span class="line">dev	232	117</span><br><span class="line">sale	85	95   </span><br><span class="line"></span><br><span class="line">SELECT * FROM emp11</span><br><span class="line">    PIVOT (</span><br><span class="line">        SUM(salary) AS sal_sum, avg(salary) AS sal_avg</span><br><span class="line">        FOR sex IN (&#x27;m&#x27;, &#x27;f&#x27;)</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">--------</span><br><span class="line"></span><br><span class="line">dept	m_sal_sum	m_sal_avg	f_sal_sum	f_sal_avg</span><br><span class="line">dev	232	116.0	117	117.0</span><br><span class="line">sale	85	85.0	95	95.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="unpivot">UNPIVOT</span></h3><p>数据透视的逆向，有点类似把 透视表的一行转成多行输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE sales_quarterly (year INT, q1 INT, q2 INT, q3 INT, q4 INT);</span><br><span class="line">INSERT INTO sales_quarterly VALUES</span><br><span class="line">    (2020, null, 1000, 2000, 2500),</span><br><span class="line">    (2021, 2250, 3200, 4200, 5900),</span><br><span class="line">    (2022, 4200, 3100, null, null);</span><br><span class="line"></span><br><span class="line">-- column names are used as unpivot columns</span><br><span class="line">SELECT * FROM sales_quarterly</span><br><span class="line">    UNPIVOT (</span><br><span class="line">        sales FOR quarter IN (q1, q2, q3, q4)</span><br><span class="line">    );</span><br><span class="line">      </span><br><span class="line">+------+---------+-------+</span><br><span class="line">| year | quarter | sales |</span><br><span class="line">+------+---------+-------+</span><br><span class="line">| 2020 | q2      | 1000  |</span><br><span class="line">| 2020 | q3      | 2000  |</span><br><span class="line">| 2020 | q4      | 2500  |</span><br><span class="line">| 2021 | q1      | 2250  |</span><br><span class="line">| 2021 | q2      | 3200  |</span><br><span class="line">| 2021 | q3      | 4200  |</span><br><span class="line">| 2021 | q4      | 5900  |</span><br><span class="line">| 2022 | q1      | 4200  |</span><br><span class="line">| 2022 | q2      | 3100  |</span><br><span class="line">+------+---------+-------+</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="lateral">Lateral</span></h3><p>LATERAL VIEW 侧边视图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LATERAL VIEW [ OUTER ] generator_function ( expression [ , ... ] ) [ table_alias ] AS column_alias [ , ... ]</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>generator_function</strong></p>
<p>Specifies a generator function (EXPLODE, INLINE, etc.).</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE person (id INT, name STRING, age INT, class INT, address STRING);</span><br><span class="line">INSERT INTO person VALUES</span><br><span class="line">    (100, &#x27;John&#x27;, 30, 1, &#x27;Street 1&#x27;),</span><br><span class="line">    (200, &#x27;Mary&#x27;, NULL, 1, &#x27;Street 2&#x27;),</span><br><span class="line">    (300, &#x27;Mike&#x27;, 80, 3, &#x27;Street 3&#x27;),</span><br><span class="line">    (400, &#x27;Dan&#x27;, 50, 4, &#x27;Street 4&#x27;);</span><br><span class="line"></span><br><span class="line">SELECT * FROM person</span><br><span class="line">    LATERAL VIEW EXPLODE(ARRAY(30, 60)) tableName AS c_age</span><br><span class="line">    LATERAL VIEW EXPLODE(ARRAY(40, 80)) AS d_age;</span><br><span class="line">+------+-------+-------+--------+-----------+--------+--------+</span><br><span class="line">|  id  | name  |  age  | class  |  address  | c_age  | d_age  |</span><br><span class="line">+------+-------+-------+--------+-----------+--------+--------+</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 30     | 40     |</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 30     | 80     |</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 60     | 40     |</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 60     | 80     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 30     | 40     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 30     | 80     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 60     | 40     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 60     | 80     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 30     | 40     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 30     | 80     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 60     | 40     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 60     | 80     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 30     | 40     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 30     | 80     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 60     | 40     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 60     | 80     |</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="dml">DML</span></h2><h3><span id="insert-into">Insert into</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE students (name STRING, address  STRING) PARTITIONED BY (birthday DATE);</span><br><span class="line"></span><br><span class="line">INSERT INTO students PARTITION (birthday = date&#x27;2019-01-02&#x27;)</span><br><span class="line">    VALUES (&#x27;Amy Smith&#x27;, &#x27;123 Park Ave, San Jose&#x27;);</span><br><span class="line"></span><br><span class="line">SELECT * FROM students;</span><br><span class="line">+-------------+-------------------------+-----------+</span><br><span class="line">|         name|                  address|   birthday|</span><br><span class="line">+-------------+-------------------------+-----------+</span><br><span class="line">|    Amy Smith|   123 Park Ave, San Jose| 2019-01-02|</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE students PARTITION (student_id = 11215016) (address, name) VALUES</span><br><span class="line">    (&#x27;Hangzhou, China&#x27;, &#x27;Kent Yao Jr.&#x27;);</span><br></pre></td></tr></table></figure>





<h3><span id="insert-overwrite-directory">Insert overwrite directory</span></h3><h4><span id="spark-format">Spark format</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE DIRECTORY &#x27;/tmp/destination&#x27;</span><br><span class="line">    USING parquet</span><br><span class="line">    OPTIONS (col1 1, col2 2, col3 &#x27;test&#x27;)</span><br><span class="line">    SELECT * FROM test_table;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE DIRECTORY</span><br><span class="line">    USING parquet</span><br><span class="line">    OPTIONS (&#x27;path&#x27; &#x27;/tmp/destination&#x27;, col1 1, col2 2, col3 &#x27;test&#x27;)</span><br><span class="line">    SELECT * FROM test_table;</span><br></pre></td></tr></table></figure>

<h4><span id="hive-format">Hive format</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE LOCAL DIRECTORY &#x27;/tmp/destination&#x27;</span><br><span class="line">    STORED AS orc</span><br><span class="line">    SELECT * FROM test_table;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE LOCAL DIRECTORY &#x27;/tmp/destination&#x27;</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;</span><br><span class="line">    SELECT * FROM test_table;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/09/bigdata/spark/spark3/20230707_sparksql/" data-id="cljvl2jh80000619a7e0a2rjw" data-title="20230707_sparksql" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-flink-flink-doc/">bigdata/flink/flink_doc</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-env/">bigdata/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-sql/">bigdata/sql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-concurrent/">java/concurrent</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-jvm/">java/jvm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql/" rel="tag">sql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/flink/" style="font-size: 10px;">flink</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sql/" style="font-size: 10px;">sql</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/20/java/designpattern/designpattern_pactice1/">designpattern_pactice1</a>
          </li>
        
          <li>
            <a href="/2023/07/20/java/jvm/jvm_basic/">jvm_basic</a>
          </li>
        
          <li>
            <a href="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/">jiketime_java并发</a>
          </li>
        
          <li>
            <a href="/2023/07/17/bigdata/sql/doc_sql/">doc_sql</a>
          </li>
        
          <li>
            <a href="/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/">flink_sql_20230713</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>