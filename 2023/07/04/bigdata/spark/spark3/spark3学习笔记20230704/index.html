<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>spark3学习笔记20230704 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Example wordcount SparkLauncher   DataSet Dataset API Common Cube 数据立方     SparkSql Group by 分组 cube用法   org.apache.spark.sql.functions collect_list min percentile_approx kurtosis Product skewness">
<meta property="og:type" content="article">
<meta property="og:title" content="spark3学习笔记20230704">
<meta property="og:url" content="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Example wordcount SparkLauncher   DataSet Dataset API Common Cube 数据立方     SparkSql Group by 分组 cube用法   org.apache.spark.sql.functions collect_list min percentile_approx kurtosis Product skewness">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-04T03:08:56.000Z">
<meta property="article:modified_time" content="2023-07-07T03:01:57.567Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bigdata/spark/spark3/spark3学习笔记20230704" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T03:08:56.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      spark3学习笔记20230704
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#example">Example</a><ul>
<li><a href="#wordcount">wordcount</a></li>
<li><a href="#sparklauncher">SparkLauncher</a></li>
</ul>
</li>
<li><a href="#dataset">DataSet</a><ul>
<li><a href="#dataset-api">Dataset API</a><ul>
<li><a href="#common">Common</a></li>
<li><a href="#cube-%E6%95%B0%E6%8D%AE%E7%AB%8B%E6%96%B9">Cube 数据立方</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sparksql">SparkSql</a><ul>
<li><a href="#group-by-%E5%88%86%E7%BB%84">Group by 分组</a><ul>
<li><a href="#cube%E7%94%A8%E6%B3%95">cube用法</a></li>
</ul>
</li>
<li><a href="#orgapachesparksqlfunctions">org.apache.spark.sql.functions</a><ul>
<li><a href="#collect_list">collect_list</a></li>
<li><a href="#min">min</a></li>
<li><a href="#percentile_approx">percentile_approx</a></li>
<li><a href="#kurtosis">kurtosis</a></li>
<li><a href="#product">Product</a></li>
<li><a href="#skewness">skewness</a></li>
<li><a href="#stddev_pop">stddev_pop</a></li>
<li><a href="#stddev">stddev</a></li>
<li><a href="#var_pop">var_pop</a></li>
<li><a href="#var_samp">var_samp</a></li>
<li><a href="#variance">variance</a></li>
<li><a href="#collection">Collection</a></li>
<li><a href="#%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0">日期函数</a></li>
<li><a href="#others">Others</a></li>
<li><a href="#%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%87%BD%E6%95%B0">字符串函数</a></li>
<li><a href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0">窗口函数</a></li>
<li><a href="#scalar-function">Scalar Function</a><ul>
<li><a href="#udf">UDF</a></li>
</ul>
</li>
<li><a href="#aggregate-function">Aggregate Function</a><ul>
<li><a href="#udaf">UDAF</a></li>
</ul>
</li>
<li><a href="#hive-udf">Hive UDF</a><ul>
<li><a href="#spark-%E5%AE%98%E6%96%B9hive-udf-example">Spark 官方hive udf Example</a></li>
<li><a href="#hive-udf-%E5%9C%A8hive%E4%B8%AD%E4%BD%BF%E7%94%A8">Hive UDF 在Hive中使用</a></li>
<li><a href="#hive-udf-%E5%9C%A8spark-%E4%B8%AD%E4%BD%BF%E7%94%A8">HIVE UDF 在Spark 中使用</a></li>
<li><a href="#hive-udf%E5%85%B6%E4%BB%96%E6%96%87%E6%A1%A3">HIVE UDF其他文档</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#dataset-%E7%94%9F%E6%88%90">Dataset 生成</a><ul>
<li><a href="#%E9%9B%86%E5%90%88%E7%94%9F%E6%88%90dataset">集合生成Dataset</a></li>
<li><a href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90dataset">加载数据集生成Dataset</a></li>
<li><a href="#rdd-%E7%94%9F%E6%88%90dataset">RDD 生成Dataset</a><ul>
<li><a href="#inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</a></li>
<li><a href="#%E7%BC%96%E7%A0%81%E6%8C%87%E5%AE%9Aschema">编码指定schema</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-%E6%95%B0%E6%8D%AE%E6%BA%90">Spark 数据源</a></li>
</ul>
<!-- tocstop -->

<h2><span id="example">Example</span></h2><h3><span id="wordcount">wordcount</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val textFile = spark.read.textFile(&quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/README.md&quot;)</span><br><span class="line"></span><br><span class="line">//    textFile.show()</span><br><span class="line">//    textFile.count()</span><br><span class="line">//    textFile.first()</span><br><span class="line">    //word最多的行有多少个word</span><br><span class="line">//    textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; if (a &gt; b) a else b)</span><br><span class="line">    //identity : scala predef预定义的一个函数，返回值等于传入参数</span><br><span class="line">    val wordCounts = textFile</span><br><span class="line">            .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">            .groupByKey(identity)</span><br><span class="line">            .count()</span><br><span class="line">            .selectExpr(&quot;key as value&quot;, &quot;`count(1)` as count&quot;)</span><br><span class="line"></span><br><span class="line">    wordCounts.printSchema()</span><br><span class="line">    wordCounts.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /* wordCounts2 等价于 上面的 wordCounts，</span><br><span class="line">       等价于sql中的 select value, count(1) count from tb1 group by value</span><br><span class="line">       等价于mapreduce中的wordcount</span><br><span class="line"></span><br><span class="line">    val wordCounts2 = textFile</span><br><span class="line">      .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">      .groupBy(&quot;value&quot;)</span><br><span class="line">      .count()</span><br><span class="line"></span><br><span class="line">    wordCounts2.show()</span><br><span class="line">     */</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="sparklauncher">SparkLauncher</span></h3><p>java代码里提交spark任务到指定集群，</p>
<p>（官方说法：The <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/launcher/package-summary.html">org.apache.spark.launcher</a> package provides classes for launching Spark jobs as child processes using a simple Java API.）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.launcher.SparkAppHandle;</span><br><span class="line">import org.apache.spark.launcher.SparkLauncher;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line"></span><br><span class="line">public class TestSparkLauncher &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; envParams = new HashMap&lt;&gt;();</span><br><span class="line">//        envParams.put(&quot;YARN_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">//        envParams.put(&quot;HADOOP_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_HOME&quot;, &quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;, &quot;1&quot;);</span><br><span class="line">//        envParams.put(&quot;JAVA_HOME&quot;, &quot;/usr/java&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SparkLauncher launcher = new SparkLauncher(envParams)</span><br><span class="line">                .setAppResource(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;)</span><br><span class="line">                .setMainClass(&quot;org.apache.spark.examples.SparkPi&quot;)</span><br><span class="line">                .setMaster(&quot;local[2]&quot;)</span><br><span class="line">                .setDeployMode(&quot;client&quot;)</span><br><span class="line">                .addAppArgs(&quot;20&quot;);</span><br><span class="line"></span><br><span class="line">        String appName = &quot;sparklancherTest&quot;;</span><br><span class="line">        launcher.setConf(&quot;spark.executor.memory&quot;, &quot;1g&quot;);</span><br><span class="line">        launcher.setAppName( appName );</span><br><span class="line"></span><br><span class="line">//        launcher.addJar(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;);</span><br><span class="line">//        launcher.redirectOutput(new File(&quot;/Users/zhouqingfeng/Desktop/mydirect/data/spark/redir/test1&quot;) );</span><br><span class="line"></span><br><span class="line">        SparkAppHandle sparkHandle = launcher.startApplication();</span><br><span class="line"></span><br><span class="line">        while(!sparkHandle.getState().isFinal()) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(1000);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="dataset">DataSet</span></h2><p>dataframe是dataset的一种特殊形式，所有元素类型被泛化为row（untyped）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type DataFrame = Dataset[org.apache.spark.sql.Row]</span><br></pre></td></tr></table></figure>





<h3><span id="dataset-api">Dataset API</span></h3><h4><span id="common">Common</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetUsageExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df  = spark.read.json(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">    val ds1 = df.as[Person]</span><br><span class="line"></span><br><span class="line">    ds1.cache()</span><br><span class="line"></span><br><span class="line">//    ds1.checkpoint()</span><br><span class="line">    ds1.explain(true)</span><br><span class="line"></span><br><span class="line">    val ds2 = ds1.where(&quot;name = &#x27;Andy&#x27;&quot;)</span><br><span class="line">    //hint 提示词</span><br><span class="line">    ds1.join(ds2.hint(&quot;broadcast&quot;))</span><br><span class="line"></span><br><span class="line">    //ds1.repartition($&quot;age&quot;).write.save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/out2/&quot;)</span><br><span class="line"></span><br><span class="line">    //添加一列</span><br><span class="line">    ds1.withColumn(&quot;age1&quot;, $&quot;age&quot; + 1 ).show</span><br><span class="line"></span><br><span class="line">    //重命名</span><br><span class="line">    ds1.select($&quot;name&quot;.as(&quot;name1&quot;))</span><br><span class="line"></span><br><span class="line">    //删除一列</span><br><span class="line">    ds1.withColumn(&quot;age1&quot;, $&quot;age&quot; + 1).drop(&quot;age&quot;)</span><br><span class="line"></span><br><span class="line">    //join</span><br><span class="line">    val ds3 = ds1</span><br><span class="line">    ds1.join(ds3, ds1.col(&quot;age&quot;) === ds3.col(&quot;age&quot;) &amp;&amp; ds1.col(&quot;name&quot;) === ds3.col(&quot;name&quot;), &quot;inner&quot;)</span><br><span class="line">       .show()</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    ds1.agg( max($&quot;age&quot;) ).show()</span><br><span class="line"></span><br><span class="line">    //statistics for numeric and string columns</span><br><span class="line">    ds1.describe(&quot;age&quot;)</span><br><span class="line">    ds1.summary()</span><br><span class="line"></span><br><span class="line">    // 替换字段中的null为11</span><br><span class="line">    ds1.na.fill(11).show</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="cube-数据立方">Cube 数据立方</span></h4><p>Cube  + rollup + pivot透视</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetCubeExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val df1 = Seq((&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)</span><br><span class="line">                 ).toDF(&quot;dept&quot;, &quot;sex&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line">    //rollup 分组,  这里等价于 group by 1 (不分组) + group by dept + group by dept, sex,  agg指明分组后的聚合函数</span><br><span class="line">    df1.rollup(&quot;dept&quot;, &quot;sex&quot;).agg(sum(&quot;salary&quot;).as(&quot;salary_sum&quot;)).show(20)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    分组聚合结果如下</span><br><span class="line">    +----+----+----------+</span><br><span class="line">    |dept| sex|salary_sum|</span><br><span class="line">    +----+----+----------+</span><br><span class="line">    |sale|null|       180|</span><br><span class="line">    |sale|   m|        85|</span><br><span class="line">    | dev|   m|       232|</span><br><span class="line">    | dev|null|       349|</span><br><span class="line">    |null|null|       529|</span><br><span class="line">    |sale|   f|        95|</span><br><span class="line">    | dev|   f|       117|</span><br><span class="line">    +----+----+----------+</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    //cube 分组,  表示取dept和sex两个维度，任意维度组合 分组</span><br><span class="line">    // 这里等价于 group by 1 (不分组) + group by dept + group by sex + group by dept, sex</span><br><span class="line">    df1.cube(&quot;dept&quot;, &quot;sex&quot;).agg(&quot;salary&quot; -&gt; &quot;sum&quot;).show(20)</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    分组聚合结果如下</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">    |dept| sex|sum(salary)|</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">    |sale|null|        180|</span><br><span class="line">    |null|   m|        317|</span><br><span class="line">    |sale|   m|         85|</span><br><span class="line">    | dev|   m|        232|</span><br><span class="line">    | dev|null|        349|</span><br><span class="line">    |null|null|        529|</span><br><span class="line">    |null|   f|        212|</span><br><span class="line">    |sale|   f|         95|</span><br><span class="line">    | dev|   f|        117|</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    //分组之后的 数据透视pivot, group by dept分组之后, 以sex为透视列, sex的取值(m\f)作为透视结果表中列的名字</span><br><span class="line">    val df_pivot = df1.groupBy(&quot;dept&quot;)</span><br><span class="line">      .pivot(&quot;sex&quot;, Seq(&quot;m&quot;, &quot;f&quot;))</span><br><span class="line">      .sum(&quot;salary&quot;)</span><br><span class="line">    df_pivot.show()</span><br><span class="line">    /*</span><br><span class="line">    分组之后的透视结果如下, sex</span><br><span class="line">    +----+---+---+</span><br><span class="line">    |dept|  m|  f|</span><br><span class="line">    +----+---+---+</span><br><span class="line">    | dev|232|117|</span><br><span class="line">    |sale| 85| 95|</span><br><span class="line">    +----+---+---+</span><br><span class="line"></span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">    //数据反透视，数据透视的逆向</span><br><span class="line">    val df_unpivot = df_pivot.unpivot(Array($&quot;dept&quot;), Array($&quot;m&quot;, $&quot;f&quot;), &quot;sex&quot;, &quot;sum&quot;)</span><br><span class="line">    df_unpivot.show()</span><br><span class="line">    /*</span><br><span class="line">    逆向透视结果如下,</span><br><span class="line">    +----+---+---+</span><br><span class="line">    |dept|sex|sum|</span><br><span class="line">    +----+---+---+</span><br><span class="line">    | dev|  m|232|</span><br><span class="line">    | dev|  f|117|</span><br><span class="line">    |sale|  m| 85|</span><br><span class="line">    |sale|  f| 95|</span><br><span class="line">    +----+---+---+</span><br><span class="line"></span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>参考资料</strong>：</p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">spark dataset api</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html">org.apache.spark.sql.functions</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html">spark中的透视函数</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36785151">excel数据透视表制作</a></p>
<h2><span id="sparksql">SparkSql</span></h2><h3><span id="group-by-分组">Group by 分组</span></h3><h4><span id="cube用法">cube用法</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">GROUP BY city, car_model WITH ROLLUP</span><br><span class="line">GROUP BY city, car_model WITH CUBE</span><br><span class="line">GROUP BY CUBE(city, car_model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE dealer (id INT, city STRING, car_model STRING, quantity INT);</span><br><span class="line">INSERT INTO dealer VALUES</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda Civic&#x27;, 10),</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda Accord&#x27;, 15),</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda CRV&#x27;, 7),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda Civic&#x27;, 20),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda Accord&#x27;, 10),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda CRV&#x27;, 3),</span><br><span class="line">    (300, &#x27;San Jose&#x27;, &#x27;Honda Civic&#x27;, 5),</span><br><span class="line">    (300, &#x27;San Jose&#x27;, &#x27;Honda Accord&#x27;, 8);</span><br><span class="line"></span><br><span class="line">-- Sum of quantity per dealership. Group by `id`.</span><br><span class="line">SELECT id, sum(quantity) FROM dealer GROUP BY id ORDER BY id;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           32|</span><br><span class="line">|200|           33|</span><br><span class="line">|300|           13|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Use column position in GROUP by clause.</span><br><span class="line">SELECT id, sum(quantity) FROM dealer GROUP BY 1 ORDER BY 1;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           32|</span><br><span class="line">|200|           33|</span><br><span class="line">|300|           13|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Multiple aggregations.</span><br><span class="line">-- 1. Sum of quantity per dealership.</span><br><span class="line">-- 2. Max quantity per dealership.</span><br><span class="line">SELECT id, sum(quantity) AS sum, max(quantity) AS max FROM dealer GROUP BY id ORDER BY id;</span><br><span class="line">+---+---+---+</span><br><span class="line">| id|sum|max|</span><br><span class="line">+---+---+---+</span><br><span class="line">|100| 32| 15|</span><br><span class="line">|200| 33| 20|</span><br><span class="line">|300| 13|  8|</span><br><span class="line">+---+---+---+</span><br><span class="line"></span><br><span class="line">-- Count the number of distinct dealer cities per car_model.</span><br><span class="line">SELECT car_model, count(DISTINCT city) AS count FROM dealer GROUP BY car_model;</span><br><span class="line">+------------+-----+</span><br><span class="line">|   car_model|count|</span><br><span class="line">+------------+-----+</span><br><span class="line">| Honda Civic|    3|</span><br><span class="line">|   Honda CRV|    2|</span><br><span class="line">|Honda Accord|    3|</span><br><span class="line">+------------+-----+</span><br><span class="line"></span><br><span class="line">-- Sum of only &#x27;Honda Civic&#x27; and &#x27;Honda CRV&#x27; quantities per dealership.</span><br><span class="line">SELECT id, sum(quantity) FILTER (</span><br><span class="line">            WHERE car_model IN (&#x27;Honda Civic&#x27;, &#x27;Honda CRV&#x27;)</span><br><span class="line">        ) AS `sum(quantity)` FROM dealer</span><br><span class="line">    GROUP BY id ORDER BY id;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           17|</span><br><span class="line">|200|           23|</span><br><span class="line">|300|            5|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Aggregations using multiple sets of grouping columns in a single statement.</span><br><span class="line">-- Following performs aggregations based on four sets of grouping columns.</span><br><span class="line">-- 1. city, car_model</span><br><span class="line">-- 2. city</span><br><span class="line">-- 3. car_model</span><br><span class="line">-- 4. Empty grouping set. Returns quantities for all city and car models.</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">    ORDER BY city;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|     null| HondaAccord| 33|</span><br><span class="line">|     null|    HondaCRV| 10|</span><br><span class="line">|     null|  HondaCivic| 35|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line">-- Group by processing with `ROLLUP` clause.</span><br><span class="line">-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), ())</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY city, car_model WITH ROLLUP</span><br><span class="line">    ORDER BY city, car_model;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line">-- Group by processing with `CUBE` clause.</span><br><span class="line">-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY city, car_model WITH CUBE</span><br><span class="line">    ORDER BY city, car_model;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|     null| HondaAccord| 33|</span><br><span class="line">|     null|    HondaCRV| 10|</span><br><span class="line">|     null|  HondaCivic| 35|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--Prepare data for ignore nulls example</span><br><span class="line">CREATE TABLE person (id INT, name STRING, age INT);</span><br><span class="line">INSERT INTO person VALUES</span><br><span class="line">    (100, &#x27;Mary&#x27;, NULL),</span><br><span class="line">    (200, &#x27;John&#x27;, 30),</span><br><span class="line">    (300, &#x27;Mike&#x27;, 80),</span><br><span class="line">    (400, &#x27;Dan&#x27;, 50);</span><br><span class="line"></span><br><span class="line">--Select the first row in column age</span><br><span class="line">SELECT FIRST(age) FROM person;</span><br><span class="line">+--------------------+</span><br><span class="line">| first(age, false)  |</span><br><span class="line">+--------------------+</span><br><span class="line">| NULL               |</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">--Get the first row in column `age` ignore nulls,last row in column `id` and sum of column `id`.</span><br><span class="line">SELECT FIRST(age IGNORE NULLS), LAST(id), SUM(id) FROM person;</span><br><span class="line">+-------------------+------------------+----------+</span><br><span class="line">| first(age, true)  | last(id, false)  | sum(id)  |</span><br><span class="line">+-------------------+------------------+----------+</span><br><span class="line">| 30                | 400              | 1000     |</span><br><span class="line">+-------------------+------------------+----------+</span><br></pre></td></tr></table></figure>



<h3><span id="orgapachesparksqlfunctions">org.apache.spark.sql.functions</span></h3><p>sql: <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/sql/index.html">https://spark.apache.org/docs/latest/api/sql/index.html</a></p>
<p>Api: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html</span><br></pre></td></tr></table></figure>

<p>资料：<a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1">https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1</a></p>
<h4><span id="collect_list">collect_list</span></h4><p>collect_list | collect_set   返回数组ArrayType</p>
<p>ds1.groupBy(“name”).agg(collect_list(“age”).as(“lis”)).show</p>
<h4><span id="min">min</span></h4><p>min_by(c1, c2)  </p>
<p>返回c2取最小值时，c1对应的值</p>
<p>val df1 &#x3D; Seq((1, 322), (2, 211)).toDF(“val”, “id”)</p>
<p>df1.agg(min_by($”val”, $”id”)).show</p>
<h4><span id="percentile_approx">percentile_approx</span></h4><p>percentile_approx百分位数：</p>
<p>val df1 &#x3D; Seq(1, 2, 3,4,5,6,7,8,9,10).toDF(“val”)</p>
<p>df1.agg(percentile_approx( $”val”, lit(0.1), lit(1000))).show</p>
<h4><span id="kurtosis">kurtosis</span></h4><p>kurtosis）又称<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B3%B0%E6%80%81%E7%B3%BB%E6%95%B0/0?fromModule=lemma_inlink">峰态系数</a>。表征<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%A6%82%E7%8E%87/0?fromModule=lemma_inlink">概率</a><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%AF%86%E5%BA%A6%E5%88%86%E5%B8%83%E6%9B%B2%E7%BA%BF/485777?fromModule=lemma_inlink">密度分布曲线</a>在<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B9%B3%E5%9D%87%E5%80%BC/0?fromModule=lemma_inlink">平均值</a>处<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B3%B0%E5%80%BC/11008657?fromModule=lemma_inlink">峰值</a>高低的特征数。直观看来，峰度反映了峰部的尖度</p>
<h4><span id="product">Product</span></h4><p>分组内元素的乘积</p>
<p>val df1 &#x3D; Seq((1, 322), (2, 211)).toDF(“val”, “id”)</p>
<p>df1.agg(product($”id”)).show</p>
<h4><span id="skewness">skewness</span></h4><p>数据倾斜度</p>
<p>val df1 &#x3D; Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(“val”, “id”)</p>
<p>df1.agg(skewness($”val”)).show</p>
<p>df1.agg(skewness($”id”)).show</p>
<h4><span id="stddev_pop">stddev_pop</span></h4><p>整体标准差 seigema (方差&#x3D;标准差的平方 )</p>
<h4><span id="stddev">stddev</span></h4><p>样本标准差</p>
<h4><span id="var_pop">var_pop</span></h4><p>总体方差</p>
<h4><span id="var_samp">var_samp</span></h4><p>样本方差</p>
<h4><span id="variance">variance</span></h4><p>样本方差</p>
<h4><span id="collection">Collection</span></h4><p>集合函数， 作用对象是一个集合，也就是说，列对应数据类型是一个集合（array, list等）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">aggregate </span><br><span class="line"></span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy(&quot;val&quot;).agg(collect_list(&quot;id&quot;).as(&quot;lis&quot;)).select(aggregate(col(&quot;lis&quot;), lit(0), (acc, x) =&gt; acc + x).as(&quot;lis_sum&quot;), $&quot;val&quot;).show</span><br><span class="line"></span><br><span class="line">array_append</span><br><span class="line">数组追加</span><br><span class="line"></span><br><span class="line">array_distinct</span><br><span class="line">数组去重</span><br><span class="line"></span><br><span class="line">array_join</span><br><span class="line">数组元素  连接:Concatenates the elements of column using the delimiter</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(array_join($&quot;ids&quot;, &quot;||&quot;)).show</span><br><span class="line"></span><br><span class="line">exists  </span><br><span class="line">数组中是否存在满足条件的元素</span><br><span class="line">exists(column: Column, f: (Column) ⇒ Column)</span><br><span class="line">df.select(exists(col(&quot;i&quot;), _ % 2 === 0))</span><br><span class="line"></span><br><span class="line">explode(e: Column): Column</span><br><span class="line">一行变多行，数组中的每个元素对应一个新的行</span><br><span class="line">Creates a new row for each element in the given array or map column</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(explode($&quot;ids&quot;).as(&quot;id&quot;)).show</span><br><span class="line"></span><br><span class="line">filter</span><br><span class="line">数组元素过滤</span><br><span class="line">df.select(filter(col(&quot;s&quot;), (x, i) =&gt; i % 2 === 0)) </span><br><span class="line">i表示对应索引</span><br><span class="line">((col, index) =&gt; predicate, the Boolean predicate to filter the input column given the index. Indices start at 0.)</span><br><span class="line"></span><br><span class="line">get</span><br><span class="line">get(column: Column, index: Column)</span><br><span class="line">返回数组指定索引处的元素 Returns element of array at given (0-based) index.</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(get($&quot;ids&quot;, lit(1))).show</span><br><span class="line"></span><br><span class="line">get_json_object(e: Column, path: String)</span><br><span class="line">解析json字符串, 返回key对应值</span><br><span class="line">val df1 = Seq(&quot;&#123;\&quot;a\&quot;:1, \&quot;b\&quot;:2&#125;&quot;, &quot;&#123;\&quot;a\&quot;:11, \&quot;b\&quot;:22&#125;&quot; ).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(get_json_object($&quot;val&quot;, &quot;$.b&quot;)).show</span><br><span class="line"></span><br><span class="line">from_json(e: Column, schema: String, options: Map[String, String]): Column</span><br><span class="line">解析json字符串, 返回一个map (Parses a column containing a JSON string into a MapType)</span><br><span class="line">val df1 = Seq(&quot;&#123;\&quot;a\&quot;:1, \&quot;b\&quot;:2&#125;&quot;, &quot;&#123;\&quot;a\&quot;:11, \&quot;b\&quot;:22&#125;&quot; ).toDF(&quot;val&quot;)</span><br><span class="line">import scala.collection.JavaConverters._</span><br><span class="line">df1.select(from_json($&quot;val&quot;, &quot;a int, b int&quot;, Map.empty[String, String].asJava)).show</span><br><span class="line">df1.select(from_json($&quot;val&quot;, &quot;a int, b int&quot;, Map.empty[String, String].asJava).as(&quot;map1&quot;)).select($&quot;map1.b&quot;)</span><br><span class="line"></span><br><span class="line">json_tuple</span><br><span class="line">json_tuple(json: Column, fields: String*): Column</span><br><span class="line">返回tuple, 输入指定多个字段</span><br><span class="line">Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.</span><br><span class="line">df1.select(lit(0).as(&quot;cc11&quot;), json_tuple($&quot;val&quot;, &quot;b&quot;, &quot;a&quot;)).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">map_contains_key</span><br><span class="line">判断map是否包含key</span><br><span class="line">val df1 = Seq(Map(&quot;a&quot; -&gt; &quot;a&quot;, &quot;b&quot; -&gt; &quot;b&quot;), Map(&quot;aa&quot; -&gt; &quot;aa&quot;, &quot;bb&quot; -&gt; &quot;bb&quot;)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(map_contains_key($&quot;val&quot;, &quot;a&quot;)).show</span><br><span class="line"></span><br><span class="line">map_filter</span><br><span class="line">map进行过滤</span><br><span class="line">df1.select(map_filter(col(&quot;val&quot;), (k, v) =&gt; k !== &#x27;a&#x27;)).show</span><br><span class="line"></span><br><span class="line">map_zip_with</span><br><span class="line">两个map合并</span><br><span class="line"></span><br><span class="line">posexplode</span><br><span class="line">类似explode，一行变多行</span><br><span class="line">df1.select($&quot;val&quot;, posexplode($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">sequence </span><br><span class="line">产生一个序列</span><br><span class="line">df1.select(sequence(lit(1), lit(3)))</span><br><span class="line"></span><br><span class="line">shuffle</span><br><span class="line">产生一个随机排列的数组</span><br><span class="line">val df1 = Seq(Array(1, 2, 3), Array(4,5,6,7)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(shuffle($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">slice</span><br><span class="line">数组切片</span><br><span class="line">df1.select(slice($&quot;val&quot;, 1, 2)).show</span><br><span class="line"></span><br><span class="line">sort_array(e: Column, asc: Boolean)</span><br><span class="line">数组排序</span><br><span class="line"></span><br><span class="line">to_json</span><br><span class="line">Converts a column containing a StructType, ArrayType or a MapType into a JSON string</span><br><span class="line">val df1 = Seq(Map(&quot;a&quot; -&gt; &quot;a&quot;, &quot;b&quot; -&gt; &quot;b&quot;), Map(&quot;aa&quot; -&gt; &quot;aa&quot;, &quot;bb&quot; -&gt; &quot;bb&quot;)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(to_json($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">zip_with</span><br><span class="line">按元素合并两个数组</span><br><span class="line">df.select(zip_with(df1(&quot;val1&quot;), df1(&quot;val2&quot;), (x, y) =&gt; x + y))</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h4><span id="日期函数">日期函数</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">val df1 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line"></span><br><span class="line">df1.select($&quot;val&quot;, current_date().as(&quot;curd&quot;), current_timestamp.as(&quot;curt&quot;)).select(add_months($&quot;curd&quot;, 2)).show</span><br><span class="line"></span><br><span class="line">select(date_add($&quot;curd&quot;, 2))</span><br><span class="line"></span><br><span class="line">date_sub</span><br><span class="line">date_diff</span><br><span class="line"></span><br><span class="line">date_format</span><br><span class="line"></span><br><span class="line">窗口函数 </span><br><span class="line">window(timeColumn: Column, windowDuration: String) 滚动窗口  窗口长度 size固定, slide = size</span><br><span class="line">window(timeColumn: Column, windowDuration: String, slideDuration: String) 滑动窗口</span><br><span class="line">滑动窗口以一个步长（Slide）不断向前滑动，窗口的长度size固定, slide &lt; size</span><br><span class="line"></span><br><span class="line">session_window(timeColumn: Column, gapDuration: String) 会话窗口</span><br><span class="line">两个窗口之间有一个间隙，被称为Session Gap。当一个窗口在大于Session Gap的时间内没有接收到新数据时，窗口将关闭。在这种模式下，窗口的长度是可变的，每个窗口的开始和结束时间并不是确定的。</span><br></pre></td></tr></table></figure>

<h4><span id="others">Others</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def coalesce(e: Column*): Column</span><br><span class="line">返回非空的列对应值</span><br><span class="line">Returns the first column that is not null, or null if all inputs are null.</span><br><span class="line"></span><br><span class="line">def col(colName: String): Column</span><br><span class="line">取对应列</span><br><span class="line"></span><br><span class="line">expr(expr: String): Column</span><br><span class="line">表达式转换为列</span><br><span class="line">df.groupBy(expr(&quot;length(word)&quot;)).count()</span><br><span class="line"></span><br><span class="line">lit(literal: Any): Column</span><br><span class="line">创建一个常量类型的列</span><br><span class="line">Creates a Column of literal value.</span><br><span class="line"></span><br><span class="line">monotonically_increasing_id()</span><br><span class="line">产生一个单调递增列，保证唯一性，但整体不保证是连续的</span><br><span class="line">val df1 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line">df1.select($&quot;val&quot;, monotonically_increasing_id()).show</span><br><span class="line"></span><br><span class="line">negate</span><br><span class="line">取负值</span><br><span class="line">df1.select(negate($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">rand()</span><br><span class="line">随机函数</span><br><span class="line"></span><br><span class="line">_*使用</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select(df1.columns.map(c =&gt; col(c)):_*)</span><br><span class="line"></span><br><span class="line">when使用</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select(when(col(&quot;id&quot;) &gt; 1, 1).otherwise(0), $&quot;id&quot;).show</span><br><span class="line"></span><br><span class="line">struct(colName: String, colNames: String*)</span><br><span class="line">Creates a new struct column  创建一个struct列</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select($&quot;id&quot;, struct(&quot;id&quot;, &quot;mark&quot;)).printSchema</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4><span id="字符串函数">字符串函数</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html</span><br></pre></td></tr></table></figure>

<p>concat_ws(sep: String, exprs: <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html">Column</a>*)</p>
<p>字符串连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select($&quot;id&quot;, concat_ws(&quot;|&quot;, $&quot;id&quot;, $&quot;mark&quot;)).show</span><br><span class="line"></span><br><span class="line">instr(str: Column, substring: String)</span><br><span class="line">查找substring 位置，如果未找到返回0</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="窗口函数">窗口函数</span></h4><p>关于hive的窗口函数 是类似的，<a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2015/04/190.htm">参考资料hive分析窗口函数</a></p>
<p><a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1">spark窗口函数example</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">row_number()  ( row_number() over(partition by a order by b))</span><br><span class="line">dense_rank()   1 2 2  3  排名rank不会跳级</span><br><span class="line">rank()         1 2 2 4 排名rank会跳级，如果有相等排名</span><br><span class="line">percent_rank()</span><br><span class="line">cume_dist()  //累加分布</span><br><span class="line">ntile()   //窗口分组</span><br><span class="line">lag</span><br><span class="line">lead</span><br><span class="line">nth_value  //当前这一行能看到的窗口对应的第n个值</span><br></pre></td></tr></table></figure>





<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.expressions.Window</span><br><span class="line"></span><br><span class="line">object DatasetWindowExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Michael&quot;, &quot;Sales&quot;, 4600),</span><br><span class="line">      (&quot;Robert&quot;, &quot;Sales&quot;, 4100),</span><br><span class="line">      (&quot;Maria&quot;, &quot;Finance&quot;, 3000),</span><br><span class="line">      (&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Scott&quot;, &quot;Finance&quot;, 3300),</span><br><span class="line">      (&quot;Jen&quot;, &quot;Finance&quot;, 3900),</span><br><span class="line">      (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000),</span><br><span class="line">      (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000),</span><br><span class="line">      (&quot;Saif&quot;, &quot;Sales&quot;, 4100)</span><br><span class="line">    )</span><br><span class="line">    val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //row_number</span><br><span class="line">    val windowSpec  = Window.partitionBy(&quot;department&quot;).orderBy($&quot;salary&quot;)</span><br><span class="line">    df.withColumn(&quot;row_number&quot;,row_number().over(windowSpec))</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line">    df.withColumn(&quot;dense_rank&quot;,dense_rank().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    df.withColumn(&quot;rank&quot;,rank().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //returns the relative rank (i.e. percentile) of rows within a window partition. 相对排名(取值0到1)</span><br><span class="line">    df.withColumn(&quot;percent_rank&quot;,percent_rank().over(windowSpec)).show()</span><br><span class="line"></span><br><span class="line">    //ntile(2) 表示将窗口 2等分, 对应的分组id，前一部分数据对应分组id 1，后一部分数据对应分组id 2</span><br><span class="line">    df.withColumn(&quot;ntile&quot;,ntile(2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    // 窗口累加值分布</span><br><span class="line">    //   * Window function: returns the cumulative distribution of values within a window partition,</span><br><span class="line">    //   * i.e. the fraction of rows that are below the current row.</span><br><span class="line">    df.withColumn(&quot;cume_dist&quot;,cume_dist().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //当前行之前 2 行 对应列的值</span><br><span class="line">    df.withColumn(&quot;lag&quot;,lag(&quot;salary&quot;,2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //当前行之后 2 行 对应列的值</span><br><span class="line">    df.withColumn(&quot;lead&quot;,lead(&quot;salary&quot;,2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    val windowSpecAgg  = Window.partitionBy(&quot;department&quot;)</span><br><span class="line">    val aggDF = df.withColumn(&quot;row&quot;,row_number.over(windowSpec))</span><br><span class="line">      .withColumn(&quot;avg&quot;, avg(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;sum&quot;, sum(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;min&quot;, min(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;max&quot;, max(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .where(col(&quot;row&quot;)===1).select(&quot;department&quot;,&quot;avg&quot;,&quot;sum&quot;,&quot;min&quot;,&quot;max&quot;)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.groupBy(&quot;department&quot;).agg(</span><br><span class="line">      avg($&quot;salary&quot;).as(&quot;avg&quot;),</span><br><span class="line">      sum($&quot;salary&quot;).as(&quot;sum&quot;),</span><br><span class="line">      min($&quot;salary&quot;).as(&quot;min&quot;),</span><br><span class="line">      max($&quot;salary&quot;).as(&quot;max&quot;)).show</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="scalar-function">Scalar Function</span></h4><p>标量函数</p>
<p>一行返回一个值</p>
<p>Scalar functions are functions that return a single value per row</p>
<p>系统内置( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions.html#scalar-functions">Built-in Scalar Functions</a>) +  用户定义UDF ( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html">User Defined Scalar Functions</a>.)</p>
<h5><span id="udf">UDF</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.udf</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark SQL UDF scalar example&quot;)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">// Define and register a zero-argument non-deterministic UDF</span><br><span class="line">// UDF is deterministic by default, i.e. produces the same result for the same input.</span><br><span class="line">val random = udf(() =&gt; Math.random())</span><br><span class="line">spark.udf.register(&quot;random&quot;, random.asNondeterministic())</span><br><span class="line">spark.sql(&quot;SELECT random()&quot;).show()</span><br><span class="line">// +-------+</span><br><span class="line">// |UDF()  |</span><br><span class="line">// +-------+</span><br><span class="line">// |xxxxxxx|</span><br><span class="line">// +-------+</span><br><span class="line"></span><br><span class="line">// Define and register a one-argument UDF</span><br><span class="line">val plusOne = udf((x: Int) =&gt; x + 1)</span><br><span class="line">spark.udf.register(&quot;plusOne&quot;, plusOne)</span><br><span class="line">spark.sql(&quot;SELECT plusOne(5)&quot;).show()</span><br><span class="line">// +------+</span><br><span class="line">// |UDF(5)|</span><br><span class="line">// +------+</span><br><span class="line">// |     6|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Define a two-argument UDF and register it with Spark in one step</span><br><span class="line">spark.udf.register(&quot;strLenScala&quot;, (_: String).length + (_: Int))</span><br><span class="line">spark.sql(&quot;SELECT strLenScala(&#x27;test&#x27;, 1)&quot;).show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |strLenScala(test, 1)|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |                   5|</span><br><span class="line">// +--------------------+</span><br><span class="line"></span><br><span class="line">// UDF in a WHERE clause</span><br><span class="line">spark.udf.register(&quot;oneArgFilter&quot;, (n: Int) =&gt; &#123; n &gt; 5 &#125;)</span><br><span class="line"></span><br><span class="line">//注册成临时表</span><br><span class="line">spark.range(1, 10).createOrReplaceTempView(&quot;test&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM test WHERE oneArgFilter(id)&quot;).show()</span><br><span class="line">// +---+</span><br><span class="line">// | id|</span><br><span class="line">// +---+</span><br><span class="line">// |  6|</span><br><span class="line">// |  7|</span><br><span class="line">// |  8|</span><br><span class="line">// |  9|</span><br><span class="line">// +---+</span><br></pre></td></tr></table></figure>



<h4><span id="aggregate-function">Aggregate  Function</span></h4><p>聚合函数</p>
<p>多行返回一个值</p>
<p>return a single value on a group of rows</p>
<p>系统内置( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions">Built-in Aggregation Functions</a>) + 用户定义UDAF(<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-aggregate.html">User Defined Aggregate Functions</a>.)</p>
<h5><span id="udaf">UDAF</span></h5><p>Reduce  汇总、汇聚</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Type-Safe User-Defined Aggregate Functions</span><br></pre></td></tr></table></figure>

<p>针对dataset操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.expressions.Aggregator</span><br><span class="line"></span><br><span class="line">case class Employee(name: String, salary: Long)</span><br><span class="line">case class Average(var sum: Long, var count: Long)</span><br><span class="line"></span><br><span class="line">object MyAverage extends Aggregator[Employee, Average, Double] &#123;</span><br><span class="line">  // A zero value for this aggregation. Should satisfy the property that any b + zero = b</span><br><span class="line">  def zero: Average = Average(0L, 0L)</span><br><span class="line">  // Combine two values to produce a new value. For performance, the function may modify `buffer`</span><br><span class="line">  // and return it instead of constructing a new object</span><br><span class="line">  def reduce(buffer: Average, employee: Employee): Average = &#123;</span><br><span class="line">    buffer.sum += employee.salary</span><br><span class="line">    buffer.count += 1</span><br><span class="line">    buffer</span><br><span class="line">  &#125;</span><br><span class="line">  // Merge two intermediate values</span><br><span class="line">  def merge(b1: Average, b2: Average): Average = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  // Transform the output of the reduction</span><br><span class="line">  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count</span><br><span class="line">  // Specifies the Encoder for the intermediate value type</span><br><span class="line">  def bufferEncoder: Encoder[Average] = Encoders.product</span><br><span class="line">  // Specifies the Encoder for the final output value type</span><br><span class="line">  def outputEncoder: Encoder[Double] = Encoders.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val ds = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;).as[Employee]</span><br><span class="line">ds.show()</span><br><span class="line">// +-------+------+</span><br><span class="line">// |   name|salary|</span><br><span class="line">// +-------+------+</span><br><span class="line">// |Michael|  3000|</span><br><span class="line">// |   Andy|  4500|</span><br><span class="line">// | Justin|  3500|</span><br><span class="line">// |  Berta|  4000|</span><br><span class="line">// +-------+------+</span><br><span class="line"></span><br><span class="line">// Convert the function to a `TypedColumn` and give it a name</span><br><span class="line">val averageSalary = MyAverage.toColumn.name(&quot;average_salary&quot;)</span><br><span class="line">val result = ds.select(averageSalary)</span><br><span class="line">result.show()</span><br><span class="line">// +--------------+</span><br><span class="line">// |average_salary|</span><br><span class="line">// +--------------+</span><br><span class="line">// |        3750.0|</span><br><span class="line">// +--------------+</span><br></pre></td></tr></table></figure>

<p>针对DataFrame操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Untyped User-Defined Aggregate Functions</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.expressions.Aggregator</span><br><span class="line">import org.apache.spark.sql.functions</span><br><span class="line"></span><br><span class="line">case class Average(var sum: Long, var count: Long)</span><br><span class="line"></span><br><span class="line">object MyAverage extends Aggregator[Long, Average, Double] &#123;</span><br><span class="line">  // A zero value for this aggregation. Should satisfy the property that any b + zero = b</span><br><span class="line">  def zero: Average = Average(0L, 0L)</span><br><span class="line">  // Combine two values to produce a new value. For performance, the function may modify `buffer`</span><br><span class="line">  // and return it instead of constructing a new object</span><br><span class="line">  def reduce(buffer: Average, data: Long): Average = &#123;</span><br><span class="line">    buffer.sum += data</span><br><span class="line">    buffer.count += 1</span><br><span class="line">    buffer</span><br><span class="line">  &#125;</span><br><span class="line">  // Merge two intermediate values</span><br><span class="line">  def merge(b1: Average, b2: Average): Average = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  // Transform the output of the reduction</span><br><span class="line">  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count</span><br><span class="line">  // Specifies the Encoder for the intermediate value type</span><br><span class="line">  def bufferEncoder: Encoder[Average] = Encoders.product</span><br><span class="line">  // Specifies the Encoder for the final output value type</span><br><span class="line">  def outputEncoder: Encoder[Double] = Encoders.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Register the function to access it</span><br><span class="line">spark.udf.register(&quot;myAverage&quot;, functions.udaf(MyAverage))</span><br><span class="line"></span><br><span class="line">val df = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;)</span><br><span class="line">df.createOrReplaceTempView(&quot;employees&quot;)</span><br><span class="line">df.show()</span><br><span class="line">// +-------+------+</span><br><span class="line">// |   name|salary|</span><br><span class="line">// +-------+------+</span><br><span class="line">// |Michael|  3000|</span><br><span class="line">// |   Andy|  4500|</span><br><span class="line">// | Justin|  3500|</span><br><span class="line">// |  Berta|  4000|</span><br><span class="line">// +-------+------+</span><br><span class="line"></span><br><span class="line">val result = spark.sql(&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;)</span><br><span class="line">result.show()</span><br><span class="line">// +--------------+</span><br><span class="line">// |average_salary|</span><br><span class="line">// +--------------+</span><br><span class="line">// |        3750.0|</span><br><span class="line">// +--------------+</span><br></pre></td></tr></table></figure>

<h4><span id="hive-udf">Hive UDF</span></h4><p>Spark SQL supports integration of Hive UDFs, UDAFs and UDTFs. Similar to Spark UDFs and UDAFs, Hive UDFs work on a single row as input and generate a single row as output, while Hive UDAFs operate on multiple rows and return a single aggregated row as a result. In addition, Hive also supports UDTFs (User Defined Tabular Functions) that act on one row as input and return multiple rows as output. </p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-hive.html">spark 官方</a></p>
<h5><span id="spark-官方hive-udf-example">Spark 官方hive udf Example</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">-- Register `GenericUDFAbs` and use it in Spark SQL.</span><br><span class="line">-- Note that, if you use your own programmed one, you need to add a JAR containing it</span><br><span class="line">-- into a classpath,</span><br><span class="line">-- e.g., ADD JAR yourHiveUDF.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION testUDF AS &#x27;org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs&#x27;;</span><br><span class="line"></span><br><span class="line">SELECT * FROM t;</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| -1.0|</span><br><span class="line">|  2.0|</span><br><span class="line">| -3.0|</span><br><span class="line">+-----+</span><br><span class="line"></span><br><span class="line">SELECT testUDF(value) FROM t;</span><br><span class="line">+--------------+</span><br><span class="line">|testUDF(value)|</span><br><span class="line">+--------------+</span><br><span class="line">|           1.0|</span><br><span class="line">|           2.0|</span><br><span class="line">|           3.0|</span><br><span class="line">+--------------+</span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf-在hive中使用">Hive UDF 在Hive中使用</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1 创建一个class 继承UDF class，实现evaluate 方法</span><br><span class="line">(you need to create a new class that extends UDF, with one or more methods named evaluate)</span><br><span class="line"></span><br><span class="line">2 把项目打成jar包，将jar包添加到hive classpath</span><br><span class="line">(After compiling your code to a jar, you need to add this to the Hive classpath)</span><br><span class="line"></span><br><span class="line">3 注册一个函数(可以是临时的、永久的) 对应用户定义的class (register your function as described)</span><br><span class="line">之后，就可以像内置函数一样，使用udf了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive0.13似乎支持，注册UDF时 指定用户jar path， for example:</span><br><span class="line"></span><br><span class="line">As of Hive 0.13, UDFs also have the option of being able to specify required jars in the CREATE FUNCTION statement:</span><br><span class="line">`CREATE FUNCTION myfunc AS ``&#x27;myclass&#x27;` `USING JAR ``&#x27;hdfs:///path/to/jar&#x27;``;`</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf-在spark-中使用">HIVE  UDF 在Spark 中使用</span></h5><p>定义class GeohashUtilty 继承udf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class GeohashUtilty extends UDF &#123;</span><br><span class="line"></span><br><span class="line">    public String evaluate(double lat, double lon, int precision) &#123;</span><br><span class="line">        return getGeoHashString(lat, lon, precision);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //get geohash string with precision n</span><br><span class="line">    public static String getGeoHashString(double lat, double lon, int precision) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            GeoHash geoHash = GeoHash.withCharacterPrecision(lat, lon, precision);</span><br><span class="line">            String geoHashString = geoHash.toBase32();</span><br><span class="line">            return geoHashString;</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(1) 经纬度获取geohash值，用于判断哪些点在同一个区域内</span><br><span class="line">spark sql 用法:</span><br><span class="line">add jar /home/etl/until/GenericUDF-1.0-jar-with-dependencies.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION geohashVal AS &#x27;com.weidai.udf.generic.GeohashUtilty&#x27;;</span><br><span class="line">select geohashVal( 28.561104, 121.186142, 8 );</span><br><span class="line">最后一位表示hash值的精度，既字符串长度，上述结果为字符串 wtn4mxmh</span><br><span class="line"></span><br><span class="line">当两个点的geohash值相同，表示这两个点在同一个区域内</span><br><span class="line">geohash值取8位长度，表示区域范围38.2*19m，</span><br><span class="line">geohash值取7位长度，表示区域范围152.9*152.4m，</span><br><span class="line">geohash值取6位长度，表示区域范围1200*609m，</span><br><span class="line">geohash值取5位长度，表示区域范围4.9km*4.9km，</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(2) 经纬度 转换为 地址 （调用百度api, 调用次数有限制，一天不要超过10万次）</span><br><span class="line">spark sql  用法:</span><br><span class="line">add jar /home/etl/until/GenericUDF-1.0-jar-with-dependencies.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION latlonToAddr AS &#x27;com.weidai.udf.generic.LatLonToAddress&#x27;;</span><br><span class="line">select latlonToAddr( 28.561104, 121.186142 );</span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf其他文档">HIVE UDF其他文档</span></h5><p><strong>hive SQL 目前支持 三种用户自定义的function: UDF, UDAF, UDTF</strong></p>
<p><strong>UDF</strong>: 输入对应一行，输出对应一行，一对一关系</p>
<p>UDFs works on a single row in a table and produces a single row as output. Its one to one relationship between input and output of a function， e.g Hive built in TRIM() function</p>
<p><strong>UDAF</strong>：<strong>User Defined Aggregate Function</strong></p>
<p>输入多行，输出1行，关系对应是多对1</p>
<p>User defined aggregate functions works on more than one row and gives single row as output. e.g Hive built in MAX() or COUNT() functions. here the relation is many to one</p>
<p><strong>UDTF</strong></p>
<p>输入一行，输出多行，关系对应是1对多。UDTF can be used to split a column into multiple column。 </p>
<p>User defined tabular function works on one row as input and returns multiple rows as output. So here the relation in one to many. e.g Hive built in EXPLODE() function(输入一行，输出多行). </p>
<p><strong>references</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.linkedin.com/pulse/hive-functions-udfudaf-udtf-examples-gaurav-singh">linked in的一篇blog：Hive Functions – UDF,UDAF and UDTF with Examples</a></p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inOperators">hive wiki: hive built-in function</a></p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">Hive wiki: Creating Custom UDFs</a></p>
<h2><span id="dataset-生成">Dataset 生成</span></h2><h3><span id="集合生成dataset">集合生成Dataset</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val df1 = Seq((&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)).toDF(&quot;dept&quot;, &quot;sex&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line">val df2 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line"></span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">// Encoders are created for case classes</span><br><span class="line">val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"></span><br><span class="line">val rdd1 = spark.range(1, 10)</span><br><span class="line">#生成rdd1, rdd1: org.apache.spark.sql.Dataset[Long] = [id: bigint]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3><span id="加载数据集生成dataset">加载数据集生成Dataset</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span><br><span class="line">import spark.implicits._</span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDS = spark.read.json(path).as[Person]</span><br><span class="line">peopleDS.show()</span><br></pre></td></tr></table></figure>



<h3><span id="rdd-生成dataset">RDD 生成Dataset</span></h3><h4><span id="inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于反射推测schema</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">object DatasetExample2 &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val peopleDF  = spark.sparkContext</span><br><span class="line">      .textFile(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.txt&quot;)</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)).toDF()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">    //dsl api</span><br><span class="line">    peopleDF.select( $&quot;name&quot;, $&quot;age&quot;, concat_ws(&quot;&quot;, lit(&quot;Name: &quot;), $&quot;name&quot;) ).show()</span><br><span class="line"></span><br><span class="line">    //函数式api The columns of a row in the result can be accessed by field index</span><br><span class="line">    peopleDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line"></span><br><span class="line">    //函数式api The columns of a row in the result can be accessed by field name</span><br><span class="line">    peopleDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span><br><span class="line">    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]</span><br><span class="line">    // Primitive types and case classes can be also defined as</span><br><span class="line">    // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span><br><span class="line"></span><br><span class="line">    // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span><br><span class="line">    peopleDF.map(teenager =&gt; teenager.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="编码指定schema">编码指定schema</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Row, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;</span><br><span class="line"></span><br><span class="line">object DatasetExample3 &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val peopleRDD  = spark.sparkContext</span><br><span class="line">      .textFile(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.txt&quot;)</span><br><span class="line"></span><br><span class="line">    // The schema is encoded in a string</span><br><span class="line">    val schemaString = &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">    // Generate the schema based on the string of schema</span><br><span class="line">    val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">      .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">    val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">    // Convert records of the RDD (people) to Rows</span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(0), attributes(1).trim))</span><br><span class="line"></span><br><span class="line">    // Apply the schema to the RDD</span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.printSchema()</span><br><span class="line"></span><br><span class="line">    peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">    // SQL can be run over a temporary view created using DataFrames</span><br><span class="line">    val results = spark.sql(&quot;SELECT name FROM people&quot;)</span><br><span class="line"></span><br><span class="line">    // The results of SQL queries are DataFrames and support all the normal RDD operations</span><br><span class="line">    // The columns of a row in the result can be accessed by field index or by field name</span><br><span class="line">    results.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="spark-数据源">Spark 数据源</span></h2><p>DataSource</p>
<p>通用写法指定fileformat，比如parquet|orc|json|csv|avro，默认格式为parquet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># read format | write format</span><br><span class="line">val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># read option | write option</span><br><span class="line">val peopleDFCsv = spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;examples/src/main/resources/people.csv&quot;)</span><br><span class="line">  </span><br><span class="line">usersDF.write.format(&quot;orc&quot;)</span><br><span class="line">  .option(&quot;orc.bloom.filter.columns&quot;, &quot;favorite_color&quot;)</span><br><span class="line">  .option(&quot;orc.dictionary.key.threshold&quot;, &quot;1.0&quot;)</span><br><span class="line">  .option(&quot;orc.column.encoding.direct&quot;, &quot;name&quot;)</span><br><span class="line">  .save(&quot;users_with_options.orc&quot;)  </span><br><span class="line"></span><br><span class="line"># 直接通过sql 读取文件</span><br><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/" data-id="cljo12rbr0000k99a2y8w8wtx" data-title="spark3学习笔记20230704" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          kyuubi_simple_usage
        
      </div>
    </a>
  
  
    <a href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">hadoop本地环境搭建</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdate-spark-spark-env/">bigdate/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/">kyuubi_simple_usage</a>
          </li>
        
          <li>
            <a href="/2023/07/04/bigdata/spark/spark3/spark3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B020230704/">spark3学习笔记20230704</a>
          </li>
        
          <li>
            <a href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">hadoop本地环境搭建</a>
          </li>
        
          <li>
            <a href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">spark环境搭建</a>
          </li>
        
          <li>
            <a href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/">processon</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>