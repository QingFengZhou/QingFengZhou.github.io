<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>20230707_spark_data_format | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="DataSource 通用Optition设置   Paquet mergeSchema Merge Schema Test     ORC mergeSchema Merge Schema Test     JSON CSV Text HIVE Conncet SparkSession 配置文件   Hive 参数 创建Hive 表 Hive 内部表、外部表、动态分区表 SQL Hive动">
<meta property="og:type" content="article">
<meta property="og:title" content="20230707_spark_data_format">
<meta property="og:url" content="https://qingfengzhou.github.io/2023/07/07/bigdata/spark/spark3/20230707_spark_data_format/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="DataSource 通用Optition设置   Paquet mergeSchema Merge Schema Test     ORC mergeSchema Merge Schema Test     JSON CSV Text HIVE Conncet SparkSession 配置文件   Hive 参数 创建Hive 表 Hive 内部表、外部表、动态分区表 SQL Hive动">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-07T14:24:20.000Z">
<meta property="article:modified_time" content="2023-07-17T03:16:52.221Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bigdata/spark/spark3/20230707_spark_data_format" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/07/bigdata/spark/spark3/20230707_spark_data_format/" class="article-date">
  <time class="dt-published" datetime="2023-07-07T14:24:20.000Z" itemprop="datePublished">2023-07-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      20230707_spark_data_format
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#datasource">DataSource</a><ul>
<li><a href="#%E9%80%9A%E7%94%A8optition%E8%AE%BE%E7%BD%AE">通用Optition设置</a></li>
</ul>
</li>
<li><a href="#paquet">Paquet</a><ul>
<li><a href="#mergeschema">mergeSchema</a><ul>
<li><a href="#merge-schema-test">Merge Schema Test</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orc">ORC</a><ul>
<li><a href="#mergeschema-1">mergeSchema</a><ul>
<li><a href="#merge-schema-test-1">Merge Schema Test</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#json">JSON</a></li>
<li><a href="#csv">CSV</a></li>
<li><a href="#text">Text</a></li>
<li><a href="#hive">HIVE</a><ul>
<li><a href="#conncet">Conncet</a><ul>
<li><a href="#sparksession">SparkSession</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">配置文件</a></li>
</ul>
</li>
<li><a href="#hive-%E5%8F%82%E6%95%B0">Hive 参数</a></li>
<li><a href="#%E5%88%9B%E5%BB%BAhive-%E8%A1%A8">创建Hive 表</a></li>
<li><a href="#hive-%E5%86%85%E9%83%A8%E8%A1%A8-%E5%A4%96%E9%83%A8%E8%A1%A8-%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8">Hive 内部表、外部表、动态分区表</a></li>
<li><a href="#sql-hive%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA">SQL Hive动态分区</a></li>
<li><a href="#%E5%A4%96%E9%83%A8%E8%A1%A8strogehandler">外部表StrogeHandler</a></li>
</ul>
</li>
<li><a href="#jdbc">JDBC</a><ul>
<li><a href="#connect">Connect</a><ul>
<li><a href="#option">Option</a></li>
</ul>
</li>
<li><a href="#sql">SQL</a></li>
<li><a href="#scala">Scala</a></li>
</ul>
</li>
<li><a href="#avro">Avro</a><ul>
<li><a href="#%E5%8F%82%E6%95%B0">参数</a></li>
<li><a href="#read">Read</a></li>
</ul>
</li>
<li><a href="#protobuf-data">Protobuf Data</a></li>
</ul>
<!-- tocstop -->

<h2><span id="datasource">DataSource</span></h2><h3><span id="通用optition设置">通用Optition设置</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">examples/src/main/resources/dir1</span><br><span class="line">├── dir2</span><br><span class="line">│   └── file2.parquet</span><br><span class="line">├── file1.parquet</span><br><span class="line">└── file3.json</span><br><span class="line"></span><br><span class="line">#ignoreCorruptFiles 忽略不规范文件, 直接跳过file3.json</span><br><span class="line">    val testCorruptDF0  = spark.read.option(&quot;ignoreCorruptFiles&quot;, &quot;true&quot;)</span><br><span class="line">    .parquet(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/&quot;,</span><br><span class="line">    &quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/dir2/&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#递归查找parquet</span><br><span class="line">val recursiveLoadedDF = spark.read.format(&quot;parquet&quot;)</span><br><span class="line">  .option(&quot;recursiveFileLookup&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;ignoreCorruptFiles&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1&quot;)</span><br><span class="line">  </span><br><span class="line"># 模式匹配, 找到路径下的所有parquet文件</span><br><span class="line">val testGlobFilterDF = spark.read.format(&quot;parquet&quot;)</span><br><span class="line">  .option(&quot;pathGlobFilter&quot;, &quot;*.parquet&quot;) // json file should be filtered out</span><br><span class="line">  .load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1&quot;)</span><br><span class="line">testGlobFilterDF.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="paquet">Paquet</span></h2><p><a target="_blank" rel="noopener" href="https://parquet.apache.org/">Parquet</a> is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. </p>
<p>spark.sql.parquet.enableVectorizedReader 向量化读取</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> Hive metastore Parquet <span class="keyword">table</span>  parquet元数据刷新</span><br><span class="line">REFRESH <span class="keyword">TABLE</span> my_table;</span><br><span class="line">spark.catalog.refreshTable(&quot;my_table&quot;)</span><br></pre></td></tr></table></figure>



<h3><span id="mergeschema">mergeSchema</span></h3><p>spark.sql.parquet.mergeSchema</p>
<p>schema自动合并，自动扩展 识别到新的字段，相当于实现自动添加新字段，默认是关闭的</p>
<p>schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</span><br></pre></td></tr></table></figure>

<h4><span id="merge-schema-test">Merge Schema Test</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, store into a partition directory</span><br><span class="line">val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i =&gt; (i, i * i)).toDF(&quot;value&quot;, &quot;square&quot;)</span><br><span class="line">squaresDF.write.parquet(&quot;/tmp/parquet/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i =&gt; (i, i * i * i)).toDF(&quot;value&quot;, &quot;cube&quot;)</span><br><span class="line">cubesDF.write.parquet(&quot;/tmp/parquet/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val mergedDF = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;/tmp/parquet/test_table&quot;)</span><br><span class="line">mergedDF.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths</span><br><span class="line">// root</span><br><span class="line">//  |-- value: int (nullable = true)</span><br><span class="line">//  |-- square: int (nullable = true)</span><br><span class="line">//  |-- cube: int (nullable = true)</span><br><span class="line">//  |-- key: int (nullable = true)</span><br><span class="line"></span><br><span class="line">mergedDF.show()</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|value|square|cube|key|</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|    1|     1|null|  1|</span><br><span class="line">|    2|     4|null|  1|</span><br><span class="line">|    4|    16|null|  1|</span><br><span class="line">|    5|    25|null|  1|</span><br><span class="line">|    3|     9|null|  1|</span><br><span class="line">|    6|  null| 216|  2|</span><br><span class="line">|    7|  null| 343|  2|</span><br><span class="line">|    8|  null| 512|  2|</span><br><span class="line">|    9|  null| 729|  2|</span><br><span class="line">|   10|  null|1000|  2|</span><br><span class="line"></span><br><span class="line"># mergeSchema 默认关闭</span><br><span class="line">val mergedDF = spark.read.parquet(&quot;/tmp/parquet/test_table&quot;)</span><br><span class="line">mergedDF.show()</span><br><span class="line">// show 结果如下</span><br><span class="line">|value|square|key|</span><br><span class="line">+-----+------+---+</span><br><span class="line">|    1|     1|  1|</span><br><span class="line">|    2|     4|  1|</span><br><span class="line">|    4|    16|  1|</span><br><span class="line">|    5|    25|  1|</span><br><span class="line">|    3|     9|  1|</span><br><span class="line">|    6|  null|  2|</span><br><span class="line">|    7|  null|  2|</span><br><span class="line">|    8|  null|  2|</span><br><span class="line">|    9|  null|  2|</span><br><span class="line">|   10|  null|  2|</span><br><span class="line">+-----+------+---+</span><br></pre></td></tr></table></figure>



<h2><span id="orc">ORC</span></h2><p><a target="_blank" rel="noopener" href="https://orc.apache.org/">Apache ORC</a> is a columnar format which has more advanced features like native zstd compression, bloom filter and columnar encryption.  支持zstd压缩,  bloom过滤器和列加密</p>
<p>spark.sql.orc.enableVectorizedReader 向量化读取</p>
<h3><span id="mergeschema">mergeSchema</span></h3><p>spark.sql.orc.mergeSchema</p>
<p>schema自动合并，自动扩展 识别到新的字段，相当于实现自动添加新字段，默认是关闭的</p>
<p>schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default.</p>
<ol>
<li>setting data source option <code>mergeSchema</code> to <code>true</code> when reading ORC files, or</li>
<li>setting the global SQL option <code>spark.sql.orc.mergeSchema</code> to <code>true</code></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">When true, the ORC data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.</span><br></pre></td></tr></table></figure>

<h4><span id="merge-schema-test">Merge Schema Test</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, store into a partition directory</span><br><span class="line">val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i =&gt; (i, i * i)).toDF(&quot;value&quot;, &quot;square&quot;)</span><br><span class="line">squaresDF.write.orc(&quot;/tmp/orc/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i =&gt; (i, i * i * i)).toDF(&quot;value&quot;, &quot;cube&quot;)</span><br><span class="line">cubesDF.write.mode(&quot;overwrite&quot;).orc(&quot;/tmp/orc/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val mergedDF = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).orc(&quot;/tmp/orc/test_table&quot;)</span><br><span class="line">mergedDF.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths</span><br><span class="line">// root</span><br><span class="line">//  |-- value: int (nullable = true)</span><br><span class="line">//  |-- square: int (nullable = true)</span><br><span class="line">//  |-- cube: int (nullable = true)</span><br><span class="line">//  |-- key: int (nullable = true)</span><br><span class="line"></span><br><span class="line">mergedDF.show()</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|value|square|cube|key|</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|    1|     1|null|  1|</span><br><span class="line">|    2|     4|null|  1|</span><br><span class="line">|    4|    16|null|  1|</span><br><span class="line">|    5|    25|null|  1|</span><br><span class="line">|    3|     9|null|  1|</span><br><span class="line">|    6|  null| 216|  2|</span><br><span class="line">|    7|  null| 343|  2|</span><br><span class="line">|    8|  null| 512|  2|</span><br><span class="line">|    9|  null| 729|  2|</span><br><span class="line">|   10|  null|1000|  2|</span><br><span class="line"></span><br><span class="line"># mergeSchema 默认关闭</span><br><span class="line">val mergedDF = spark.read.orc(&quot;/tmp/orc/test_table&quot;)</span><br><span class="line">mergedDF.show()</span><br><span class="line">// show 结果如下</span><br><span class="line">|value|square|key|</span><br><span class="line">+-----+------+---+</span><br><span class="line">|    1|     1|  1|</span><br><span class="line">|    2|     4|  1|</span><br><span class="line">|    4|    16|  1|</span><br><span class="line">|    5|    25|  1|</span><br><span class="line">|    3|     9|  1|</span><br><span class="line">|    6|  null|  2|</span><br><span class="line">|    7|  null|  2|</span><br><span class="line">|    8|  null|  2|</span><br><span class="line">|    9|  null|  2|</span><br><span class="line">|   10|  null|  2|</span><br><span class="line">+-----+------+---+</span><br></pre></td></tr></table></figure>







<h2><span id="json">JSON</span></h2><p> Spark SQL automatically infer the schema of a JSON dataset and load it as a <code>Dataset[Row]</code>.</p>
<h2><span id="csv">CSV</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// Read a csv with delimiter and a header</span><br><span class="line">val df3 = spark.read.option(&quot;delimiter&quot;, &quot;;&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(path)</span><br><span class="line">df3.show(</span><br></pre></td></tr></table></figure>



<h2><span id="text">Text</span></h2><h2><span id="hive">HIVE</span></h2><h3><span id="conncet">Conncet</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h4><span id="sparksession">SparkSession</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  #config hive support</span><br><span class="line">  </span><br><span class="line">  val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local[3]&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">//      .config(&quot;spark.sql.catalogImplementation&quot;, &quot;hive&quot;)</span><br><span class="line">      .enableHiveSupport()  //&quot;spark.sql.catalogImplementation&quot;  &quot;hive&quot;</span><br><span class="line">      .getOrCreate()</span><br><span class="line">      </span><br><span class="line">   import spark.implicits._</span><br><span class="line">   import spark.sql</span><br></pre></td></tr></table></figure>

<h4><span id="配置文件">配置文件</span></h4><p>配置Hive-site.xml</p>
<p>copy  hive-site.xml   $SPARK_HOME&#x2F;conf</p>
<p>mac本地连接配置xml：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sparkContext.hadoopConfiguration.addResource(new Path(&quot;*/hive-site.xml&quot;))</span><br></pre></td></tr></table></figure>



<h3><span id="hive-参数">Hive 参数</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 动态分区设置</span><br><span class="line">hive.exec.dynamic.partition=true; 是开启动态分区</span><br><span class="line">hive.exec.dynamic.partition.mode=nonstrict; 这个属性默认值是strict,就是要求分区字段必须有一个是静态的分区值，随后会讲到，当前设置为nonstrict,那么可以全部动态分区</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="创建hive-表">创建Hive 表</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;use test_zhou&quot;)</span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &#x27;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">  case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br></pre></td></tr></table></figure>



<h3><span id="hive-内部表-外部表-动态分区表">Hive 内部表、外部表、动态分区表</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">//内部表</span><br><span class="line">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span><br><span class="line">// `USING hive`</span><br><span class="line">sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">// Save DataFrame to the Hive managed table</span><br><span class="line">val df = spark.table(&quot;src&quot;)</span><br><span class="line">df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line">// After insertion, the Hive managed table has data now</span><br><span class="line">sql(&quot;SELECT * FROM hive_records&quot;).show()</span><br><span class="line"></span><br><span class="line">//外部表</span><br><span class="line">// Prepare a Parquet data directory</span><br><span class="line">val dataDir = &quot;/tmp/parquet_data&quot;</span><br><span class="line">spark.range(10).write.parquet(dataDir)</span><br><span class="line">// Create a Hive external Parquet table</span><br><span class="line">sql(s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#x27;$dataDir&#x27;&quot;)</span><br><span class="line">// The Hive external table should already have data</span><br><span class="line">sql(&quot;SELECT * FROM hive_bigints&quot;).show()</span><br><span class="line"></span><br><span class="line">//分区表</span><br><span class="line">// Turn on flag for Hive Dynamic Partitioning</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)</span><br><span class="line">// Create a Hive partitioned table using DataFrame API   </span><br><span class="line">//创建hive分区表, 指定format hive, 输出格式为text</span><br><span class="line">//查看schema: CREATE TABLE hive_part_tbl (value STRING,key INT)USING text PARTITIONED BY (key)</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;)</span><br><span class="line">// Partitioned column `key` will be moved to the end of the schema, 分区列移动到schema尾部</span><br><span class="line">sql(&quot;SELECT * FROM hive_part_tbl&quot;).show()</span><br><span class="line"></span><br><span class="line">//创建spark分区表, 如果不指定format hive, 默认格式parquet</span><br><span class="line">// 输出目录多了一个_SUCCESS文件, 输出snappy.parquet格式</span><br><span class="line">// 查看schema: CREATE TABLE hive_part_tb2 (value STRING,key INT)USING parquet PARTITIONED BY (key)</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).saveAsTable(&quot;hive_part_tb2&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//Spark SQL native syntax spark sql 原生(内置)语法创建 hive分区表</span><br><span class="line">CREATE external TABLE hive_part_tbl (value STRING,key INT)USING text PARTITIONED BY (key)</span><br><span class="line"></span><br><span class="line">//hive 语法 （hive HQL syntax ）创建 hive分区表</span><br><span class="line">//分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列</span><br><span class="line">CREATE external TABLE hive_par_tess(value STRING) STORED AS  textfile PARTITIONED BY (key INT) location &#x27;/user/hive/warehouse/test_zhou.db/hive_part_tbl&#x27;</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;msck repair table hive_par_tess&quot;).show  //外部表 元数据刷新之后, 才能查看到数据</span><br><span class="line"></span><br><span class="line">#这种写法是错误的, 分区字段key不能写在前面ddl字段定义里, 要写在PARTITIONED By 后面, 和spark sql写法不同</span><br><span class="line">CREATE external TABLE hive_par_tess2(value STRING, key INT) STORED AS  textfile PARTITIONED BY (key INT) location &#x27;/user/hive/warehouse/test_zhou.db/hive_part_tbl&#x27;</span><br></pre></td></tr></table></figure>



<h3><span id="sql-hive动态分区">SQL Hive动态分区</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">create table test_part2(id int, name string) </span><br><span class="line">partitioned by (country string, province string );</span><br><span class="line"></span><br><span class="line">--静态分区 指定分区字段的值</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;sh&#x27;)  select id, name from test_part1 where id = 1 and name = &#x27;lisa&#x27;;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;)  values(2, &#x27;dal&#x27;)</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;) select 2 as id, &#x27;cate&#x27; as name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--半动态分区(一部分静态分区 + 一部分动态分区)</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--全动态分区</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-sql \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;tez&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-shell   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;insert into table test_zhou.test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;newyork&#x27; as province&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="外部表strogehandler">外部表StrogeHandler</span></h3><p>外部表StrogeHandler使用</p>
<p>目前(Spark 3.4.1 ) spark sql原生内置sql语法还不支持使用Hive storage handler 建表，如果需要使用，</p>
<p>先通过hive建表，再使用Spark sql读取数据( Hive storage handler is not supported yet when creating table, you can create a table using storage handler at Hive side, and use Spark SQL to read it.)</p>
<p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/hive.html">创建ES外部表</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#待实践</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> artists (...)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.elasticsearch.hadoop.hive.EsStorageHandler&#x27;</span></span><br><span class="line">TBLPROPERTIES(<span class="string">&#x27;es.resource&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;radio/artists&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;es.index.auto.create&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;false&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>创建HBase外部表</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#待实践</span><br><span class="line">create external table tb2_cp (</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">addr string</span><br><span class="line">)</span><br><span class="line">STORED BY ‘org.apache.hadoop.hive.hbase.HBaseStorageHandler’</span><br><span class="line">WITH SERDEPROPERTIES (“hbase.columns.mapping” = “:key, cf:age, cf:addr”)</span><br><span class="line">TBLPROPERTIES (“hbase.table.name” = “test_tb2”);</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="jdbc">JDBC</span></h2><h3><span id="connect">Connect</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;mysqlConnectorVersion&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>



<h4><span id="option">Option</span></h4><p>prepareQuery 执行读取数据前，先执行此语句，相当于预处理</p>
<p>fetchsize  一次读取行数</p>
<p>batchsize  批量一次写入行数，默认1000</p>
<h3><span id="sql">SQL</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW mysql_users_view</span><br><span class="line">USING org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &quot;jdbc:mysql://localhost&quot;,</span><br><span class="line">  dbtable &quot;test_zhou.users&quot;,</span><br><span class="line">  user &#x27;***&#x27;,</span><br><span class="line">  password &#x27;***&#x27;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3><span id="scala">Scala</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">//read jdbc</span><br><span class="line">val df1 = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://127.0.0.1:3306/test_zhou?characterEncoding=utf8&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;users&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;**&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;***&quot;)</span><br><span class="line">      .option(&quot;abc&quot;, &quot;def&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">//read jdbc:  dbtable = subquey 指定一个子查询</span><br><span class="line">val df1 = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://127.0.0.1:3306/test_zhou?characterEncoding=utf8&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;(select id, name from users) as subq&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;**&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;**&quot;)</span><br><span class="line">      .option(&quot;abc&quot;, &quot;def&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">//并行多分区同时读取 jdbc, 指定分区列</span><br><span class="line">spark.read.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, jdbcUrl)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;(select c1, c2 from t1) as subq&quot;)</span><br><span class="line">.option(&quot;partitionColumn&quot;, &quot;c1&quot;)</span><br><span class="line">.option(&quot;lowerBound&quot;, &quot;1&quot;)</span><br><span class="line">.option(&quot;upperBound&quot;, &quot;100&quot;)</span><br><span class="line">.option(&quot;numPartitions&quot;, &quot;3&quot;)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line">//并行多分区同时读取 jdbc</span><br><span class="line">val url = &quot;jdbc:mysql://127.0.0.1:3306/test_zhou?characterEncoding=utf8&quot;</span><br><span class="line">val table = &quot;(select * from test_zhou.users) E&quot;</span><br><span class="line">//每个条件对应一个partition数据, 对应一个task</span><br><span class="line">val partitionStr = &quot;id&lt;13 or id is null,  13&lt;=id and id &lt;18, id&gt;=18&quot;</span><br><span class="line">val partitionArray = partitionStr.split(&quot;,&quot;).map(_.trim)</span><br><span class="line">val javaProps = new java.util.Properties</span><br><span class="line">javaProps.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">javaProps.put(&quot;password&quot;, &quot;Zoom@123&quot;)</span><br><span class="line">val df = spark.read.jdbc(url, table, partitionArray, javaProps)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//write jdbc</span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .save()</span><br><span class="line"></span><br><span class="line">//写入时 指定table schema</span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br></pre></td></tr></table></figure>



<h2><span id="avro">Avro</span></h2><p>Avro is built-in but external data source module since Spark 2.4</p>
<p>（schema和data分两部分存储，文件头部存储schema， 数据本身存储为二进制格式）</p>
<p>行式存储，写入速度快，文件可拆分，可压缩，可以独立地添加新的字段</p>
<p>是一种基于行的、可高度拆分的数据格式</p>
<p>为了最大程度地减小文件大小、并提高效率，它将schema存储为JSON格式，而将数据存储为二进制格式</p>
<ul>
<li>Avro是一种与语言无关的数据序列化。</li>
<li>Avro将schema存储在文件的标题中，因此数据具有自描述性。</li>
<li>Avro格式的文件既可以被拆分、又可以被压缩，因此非常适合于在Hadoop生态系统中进行数据的存储。</li>
<li>由于Avro文件将负责读取的schema与负责写入的schema区分开来，因此它可以独立地添加新的字段。</li>
<li>与序列文件(Sequence Files)相同，为了实现高度可拆分性，Avro文件也包含了用于分隔不同块(block)的同步标记。</li>
<li>可以使用诸如snappy之类的压缩格式，来压缩不同的目标块。</li>
</ul>
<h3><span id="参数">参数</span></h3><p>avroSchema </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.option(&quot;avroSchema&quot;, avroSchema)</span><br></pre></td></tr></table></figure>

<p>cat examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;user_1.avsc</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&quot;namespace&quot;: &quot;example.avro&quot;,</span><br><span class="line"> &quot;type&quot;: &quot;record&quot;,</span><br><span class="line"> &quot;name&quot;: &quot;User&quot;,</span><br><span class="line"> &quot;fields&quot;: [</span><br><span class="line">     &#123;&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">     &#123;&quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]&#125;,</span><br><span class="line">     &#123;&quot;type&quot;: &#123;&quot;items&quot;: &quot;int&quot;, &quot;type&quot;: &quot;array&quot;&#125;, &quot;name&quot;: &quot;favorite_numbers&quot;&#125;</span><br><span class="line"> ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># java 读取schema 文件</span><br><span class="line">import java.nio.file.&#123;Files, Paths&#125;</span><br><span class="line">val avroSchema = new String(Files.readAllBytes(Paths.get(&quot;/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/user_1.avsc&quot;)))</span><br><span class="line">val usersDF = spark.read.format(&quot;avro&quot;).load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.avro&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="read">Read</span></h3><p>The <code>spark-avro</code> module is external and not included in <code>spark-submit</code> or <code>spark-shell</code> by default.</p>
<p>读取时需要指定依赖, 或者添加到spark classpath</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./bin/spark-shell --packages org.apache.spark:spark-avro_2.12:3.4.1</span><br><span class="line"></span><br><span class="line">val usersDF = spark.read.format(&quot;avro&quot;).load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.avro&quot;)</span><br><span class="line"></span><br><span class="line">//write</span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.format(&quot;avro&quot;).save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/avro/namesAndFavColors.avro&quot;)</span><br><span class="line"></span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_numbers&quot;).write.mode(&quot;overwrite&quot;).format(&quot;avro&quot;).save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/avro/namesAndFavNums.avro&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import java.nio.file.&#123;Files, Paths&#125;</span><br><span class="line">val avroSchema = new String(Files.readAllBytes(Paths.get(&quot;/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/user_1.avsc&quot;)))</span><br><span class="line"></span><br><span class="line"># 读取失败</span><br><span class="line">val usersDF = spark.read.format(&quot;avro&quot;)</span><br><span class="line">.option(&quot;recursiveFileLookup&quot;, &quot;true&quot;)</span><br><span class="line">.option(&quot;ignoreCorruptFiles&quot;,&quot;true&quot;)</span><br><span class="line">.option(&quot;avroSchema&quot;, avroSchema)</span><br><span class="line">.load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/avro&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="protobuf-data">Protobuf Data</span></h2><p>Since Spark 3.4.0 release, <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> provides built-in support for reading and writing protobuf data.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/07/bigdata/spark/spark3/20230707_spark_data_format/" data-id="clk6akm6f0000lk9agn6d7e23" data-title="20230707_spark_data_format" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/09/bigdata/spark/spark3/20230707_sparksql/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          20230707_sparksql
        
      </div>
    </a>
  
  
    <a href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">kyuubi_simple_usage</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-flink-flink-doc/">bigdata/flink/flink_doc</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-env/">bigdata/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-tune/">bigdata/spark/spark_tune</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-sql/">bigdata/sql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-concurrent/">java/concurrent</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-designpattern/">java/designpattern</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-jvm/">java/jvm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql/" rel="tag">sql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/flink/" style="font-size: 10px;">flink</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sql/" style="font-size: 10px;">sql</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/21/bigdata/spark/spark_tune/spark_conf/">spark_conf</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/spark/spark_tune/spark_shuffle_usage/">spark_shuffle_usage</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/spark/spark_tune/spark_memory_usage/">spark_memory_usage</a>
          </li>
        
          <li>
            <a href="/2023/07/20/java/designpattern/designpattern_pactice1/">designpattern_pactice1</a>
          </li>
        
          <li>
            <a href="/2023/07/20/java/jvm/jvm_basic/">jvm_basic</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>