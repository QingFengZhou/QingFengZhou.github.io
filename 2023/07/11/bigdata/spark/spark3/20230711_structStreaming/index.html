<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>20230711_structStreaming | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="执行引擎 Example StructStreaming BatchProcessing   Basics outputmode Event time Time Window     Fault Tolerance Semantics Source Socket File Rate   延时乱序数据 定义 测试 结论   状态存储 HDFS state store provider Rock">
<meta property="og:type" content="article">
<meta property="og:title" content="20230711_structStreaming">
<meta property="og:url" content="https://qingfengzhou.github.io/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="执行引擎 Example StructStreaming BatchProcessing   Basics outputmode Event time Time Window     Fault Tolerance Semantics Source Socket File Rate   延时乱序数据 定义 测试 结论   状态存储 HDFS state store provider Rock">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-11T08:52:34.000Z">
<meta property="article:modified_time" content="2023-07-12T10:22:08.983Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bigdata/spark/spark3/20230711_structStreaming" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/" class="article-date">
  <time class="dt-published" datetime="2023-07-11T08:52:34.000Z" itemprop="datePublished">2023-07-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      20230711_structStreaming
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E">执行引擎</a><ul>
<li><a href="#example">Example</a><ul>
<li><a href="#structstreaming">StructStreaming</a></li>
<li><a href="#batchprocessing">BatchProcessing</a></li>
</ul>
</li>
<li><a href="#basics">Basics</a><ul>
<li><a href="#outputmode"><strong>outputmode</strong></a></li>
<li><a href="#event-time">Event time</a></li>
<li><a href="#time-window">Time Window</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#fault-tolerance-semantics">Fault Tolerance Semantics</a></li>
<li><a href="#source">Source</a><ul>
<li><a href="#socket">Socket</a></li>
<li><a href="#file">File</a></li>
<li><a href="#rate">Rate</a></li>
</ul>
</li>
<li><a href="#%E5%BB%B6%E6%97%B6%E4%B9%B1%E5%BA%8F%E6%95%B0%E6%8D%AE">延时乱序数据</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%89">定义</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95">测试</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li>
</ul>
</li>
<li><a href="#%E7%8A%B6%E6%80%81%E5%AD%98%E5%82%A8">状态存储</a><ul>
<li><a href="#hdfs-state-store-provider">HDFS state store provider</a></li>
<li><a href="#rocksdb-state-store-implementation">RocksDB state store implementation</a></li>
</ul>
</li>
<li><a href="#sink">Sink</a></li>
<li><a href="#trigger">Trigger</a></li>
<li><a href="#checkpoint">checkpoint</a></li>
<li><a href="#kafka%E9%9B%86%E6%88%90">Kafka集成</a><ul>
<li><a href="#%E8%AF%BB%E5%8F%96kafka%E5%86%99%E5%85%A5console">读取kafka写入console</a></li>
<li><a href="#%E8%AF%BB%E5%8F%96kafka-%E6%99%AE%E9%80%9A%E8%81%9A%E5%90%88%E4%B8%8D%E5%90%AB%E7%AA%97%E5%8F%A3">读取kafka 普通聚合(不含窗口)</a></li>
<li><a href="#%E8%AF%BB%E5%8F%96kafka-%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%90%AB%E6%B0%B4%E5%8D%B0">读取kafka 窗口聚合(含水印)</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="执行引擎">执行引擎</span></h2><p>Spark SQL engine</p>
<p>两种处理模式：</p>
<p>默认 <em>micro-batch processing</em> engine,  最低延时 100ms，exactly-once fault-tolerance guarantees</p>
<p>可配置模式<strong>Continuous Processing</strong>，最低延时 1ms，at-least-once guarantees</p>
<p>Internally, by default, Structured Streaming queries are processed using a <em>micro-batch processing</em> engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called <strong>Continuous Processing</strong>, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset&#x2F;DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.</p>
<h3><span id="example">Example</span></h3><h4><span id="structstreaming">StructStreaming</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * start server: nc -lk 9999</span><br><span class="line"> * 可以看到这里 的api写法基本跟batch processing 批处理的api保持一致，</span><br><span class="line"> * 不同的地方是：</span><br><span class="line"> * (1) spark.readStream | wordCounts.writeStream，batch api的写法 spark.read | wordCounts.write</span><br><span class="line"> * (2) 流处理触发运行，调用方法wordCounts.writeStream.start()，而批处理调用方法 wordCounts.write.save()</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleStructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCount&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">    val lines = spark.readStream</span><br><span class="line">      .format(&quot;socket&quot;)</span><br><span class="line">      .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">      .option(&quot;port&quot;, 9999)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    // dataframe to dataset; then Split the lines into words</span><br><span class="line">    val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">    // Generate running word count</span><br><span class="line">    val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the running counts to the console</span><br><span class="line">    //console outputMode支持 update和complete, 有聚合计算的情况不支持append模式</span><br><span class="line">    val query = wordCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="batchprocessing">BatchProcessing</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetWordCountExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCount&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val lines = spark.read.format(&quot;text&quot;).load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/README.md&quot;)</span><br><span class="line"></span><br><span class="line">    // dataframe to dataset; then Split the lines into words</span><br><span class="line">    val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">    // Generate running word count</span><br><span class="line">    val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">    wordCounts.write.mode(&quot;overwrite&quot;).format(&quot;parquet&quot;).save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="basics">Basics</span></h3><h4><span id="outputmode"><strong>outputmode</strong></span></h4><p><em>Complete Mode</em> -   结果集 每次输出所有数据</p>
<p><em>Append Mode</em> -  结果集 每次只输出新增的数据（只包括新增，不包括变更的）</p>
<p><em>Update Mode</em> -  结果集 每次只输出更新的数据（包括新增 和 变更的  new|update）</p>
<h4><span id="event-time">Event time</span></h4><p>事件时间，只与事件本身有关</p>
<h4><span id="time-window">Time Window</span></h4><p>滚动窗口：Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals</p>
<p>滑动窗口：Sliding windows are similar to the tumbling windows from the point of being “fixed-sized”, but windows can overlap if the duration of slide is smaller than the duration of window, and in this case an input can be bound to the multiple windows.</p>
<p>会话窗口： 窗口在一定间隔内没有接收到新的数据，进行关闭，窗口大小不固定</p>
<p>a session window closes when there’s no input received within gap duration after receiving the latest input.</p>
<h2><span id="fault-tolerance-semantics">Fault Tolerance Semantics</span></h2><p>如何实现容错 和 exactly-once  端到端一致性保证：</p>
<p>Delivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming. To achieve that, we have designed the Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and&#x2F;or reprocessing. Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure <strong>end-to-end exactly-once semantics</strong> under any failure.</p>
<p>Source:  replayable </p>
<p>Sink: idempotent</p>
<p>Engine: checkpointing and write-ahead logs </p>
<h2><span id="source">Source</span></h2><h3><span id="socket">Socket</span></h3><p>对应example</p>
<h3><span id="file">File</span></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Read all the csv files written atomically in a directory</span></span><br><span class="line"><span class="keyword">val</span> userSchema = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;string&quot;</span>).add(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;integer&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> csvDF = spark</span><br><span class="line">  .readStream</span><br><span class="line">  .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">  .schema(userSchema)      <span class="comment">// Specify schema of the csv files</span></span><br><span class="line">  .csv(<span class="string">&quot;/path/to/directory&quot;</span>)    <span class="comment">// Equivalent to format(&quot;csv&quot;).load(&quot;/path/to/directory&quot;)</span></span><br></pre></td></tr></table></figure>



<h3><span id="rate">Rate</span></h3><p>Generates data at the specified number of rows per second, each output row contains a <code>timestamp</code> and <code>value</code>. Where <code>timestamp</code> is a <code>Timestamp</code> type containing the time of message dispatch, and <code>value</code> is of <code>Long</code> type containing the message count, starting from 0 as the first row. </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a streaming DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = spark.readStream</span><br><span class="line">  .format(<span class="string">&quot;rate&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;rowsPerSecond&quot;</span>, <span class="number">10</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> query = df.writeStream</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/out/rate/&quot;</span>)</span><br><span class="line">  .outputMode(<span class="string">&quot;update&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;numRows&quot;</span>, <span class="number">20</span>)</span><br><span class="line">  .option(<span class="string">&quot;truncate&quot;</span>, <span class="literal">false</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"> query.awaitTermination()</span><br></pre></td></tr></table></figure>



<h2><span id="延时乱序数据">延时乱序数据</span></h2><h3><span id="定义">定义</span></h3><p><strong>Watermark</strong>: 水位线</p>
<p>Watemark &#x3D; the current max  event time - late threshold</p>
<p>基于事件时间产生，本质是一个时间戳，和时间窗口结合，用于处理延时晚到的乱序时间。随着事件的不断流入(到计算逻辑flink或spark代码)，水位线不断上涨，当水位线 &gt;&#x3D; 窗口结束时间，流入到这个窗口的新的延时数据将不再被该窗口计算处理。</p>
<p>水位线作用和定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用来处理数据延迟，数据乱序。</span><br><span class="line"></span><br><span class="line">水印（更准确的说法是水位线，随着接收事件时间的不断增长而不断上涨）本质上是一个时间戳，代表事件时间比这个时间戳更小的数据已全部到达，（所有比这时间戳更早的数据视为延迟数据）当水印watermark&gt;=窗口对应的结束时间，对应窗口会触发计算并进行关闭</span><br></pre></td></tr></table></figure>



<p><strong>水印只与事件时间有关</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark 水印 和flink的水印 均是来自google的dataflow model,  其核心思想是在无边界、乱序的数据流中，在数据准确性、数据延迟性和计算性能开销之间 追求一个平衡。</span><br><span class="line">水位线本质是事件时间戳，仅仅只与 事件本身对应的时间属性event_time 和用户定义的允许数据延迟的时间（late threshold）有关，与系统本身的处理时间processing time 没有关系。 「之前一直理解不了，将processing time 和event time混淆到一块，误以为event time 和processing time时钟是同步的，其实两者是没有关系的，一个是事件本身决定的，一个是计算系统处理时间，我们通常讲窗口、水印是针对事件而言的，而现实中事件时间和处理时间可能中间间隔了好几个小时，水印本质解决的问题是有关在事件中多条数据乱序到达的问题，只能依赖与事件本身的时间去想办法解决，通过引入水位线的概念（由事件时间生成，随着数据的不断流入水位线不断上涨），当水位线&gt;=某个窗口结束时间（当然，这里窗口也是完全依据流入的数据事件时间划分的） 意味着在这之后流入到这个窗口的新的数据将不再被该窗口计算处理 」</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="测试">测试</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 测试水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> * 1、start server: nc -lk 9999</span><br><span class="line"> * 2、params: localhost 9999 10 (设置滚动窗口 10s间隔)</span><br><span class="line"> *</span><br><span class="line"> * 3、late threshold = 10s</span><br><span class="line"> */</span><br><span class="line">object ExampleWatermark_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if (args.length &lt; 3) &#123;</span><br><span class="line">      System.err.println(&quot;Usage: StructuredNetworkWordCountWindowed &lt;hostname&gt; &lt;port&gt;&quot; +</span><br><span class="line">        &quot; &lt;window duration in seconds&gt; [&lt;slide duration in seconds&gt;]&quot;)</span><br><span class="line">      System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val host = args(0)</span><br><span class="line">    val port = args(1).toInt</span><br><span class="line">    val windowSize = args(2).toInt</span><br><span class="line">    val slideSize = if (args.length == 3) windowSize else args(3).toInt</span><br><span class="line">    if (slideSize &gt; windowSize) &#123;</span><br><span class="line">      System.err.println(&quot;&lt;slide duration&gt; must be less than or equal to &lt;window duration&gt;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val windowDuration = s&quot;$windowSize seconds&quot;</span><br><span class="line">    val slideDuration = s&quot;$slideSize seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCountWindowed&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line">    val lines = spark.readStream</span><br><span class="line">      .format(&quot;socket&quot;)</span><br><span class="line">      .option(&quot;host&quot;, host)</span><br><span class="line">      .option(&quot;port&quot;, port)</span><br><span class="line">      //      .option(&quot;includeTimestamp&quot;, true)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    // Split the lines into words, retaining timestamps</span><br><span class="line">    //    val words = lines.as[(String, Timestamp)].flatMap(line =&gt;</span><br><span class="line">    //      line._1.split(&quot; &quot;).map(word =&gt; (word, line._2))</span><br><span class="line">    //    ).toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">    val query = windowedCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>注意</strong>：spark 水印只在append 模式和update模式下有作用，在complete模式下没有任何作用，所有数据都会输出。</p>
<p>在上述example中，</p>
<p>Terminal 开启端口本地测试 （nc -lk 9000 10），窗口为10s, 最大延迟时间也是10s,</p>
<p>依次每行输入数据，</p>
<p>spark,2020-09-04 10:02:41<br>spark,2020-09-04 10:02:42<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:48<br>spark,2020-09-04 10:02:21<br>spark,2020-09-04 10:02:31<br>spark,2020-09-04 10:02:32<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:51<br>spark,2020-09-04 10:02:59<br>spark,2020-09-04 10:02:53<br>spark,2020-09-04 10:03:06</p>
<p>spark,2020-09-04 10:02:45</p>
<p>spark,2020-09-04 10:03:07<br>spark,2020-09-04 10:02:53<br>spark,2020-09-04 10:03:08<br>spark,2020-09-04 10:03:13<br>spark,2020-09-04 10:03:13<br>spark,2020-09-04 10:03:33<br>spark,2020-09-04 10:03:36</p>
<p>前四条数据依次输入，</p>
<p>spark,2020-09-04 10:02:41<br>spark,2020-09-04 10:02:42<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:48</p>
<p>输出结果为：</p>
<p>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]|spark|4    |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（可以看到窗口是[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]，当时大概是11点30分测试的，这里划定的窗口完全是由事件时间决定的，与其他没有关系，这里水位线watermark=2020-09-04 10:02:49 - 10s = 2020-09-04 10:02:39， 意味着窗口结束时间T                                      2020-09-04 10:02:39 &gt;=T, 对应的流入该窗口内的新数据不再被处理，最接近水位线的窗口是                                                                     [2020-09-04 10:02:20.0,2020-09-04 10:02:30.0] ,  因此后面接着输入了一条位于该窗口内的新的数据，spark,2020-09-04 10:02:21，但是没有结果输出，因为水位线高过了窗口结束时间T = 2020-09-04 10:02:30.0）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:21，</p>
<p>没有结果输出</p>
<p>输入spark,2020-09-04 10:02:31，</p>
<p>输出结果为：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]|spark|1    |<br>+———————————————+—–+—–+</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 &gt;= 2020-09-04 10:02:40.0 不成立， 老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据继续被处理）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:32，</p>
<p>输出结果为：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]|spark|2    |<br>+———————————————+—–+—–+    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 &gt;= 2020-09-04 10:02:40.0 不成立，老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据继续被处理 ）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:49，</p>
<p>输出：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]|spark|5    |    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 ）</span><br></pre></td></tr></table></figure>

<p>输入 </p>
<p>spark,2020-09-04 10:02:51<br>spark,2020-09-04 10:02:59<br>spark,2020-09-04 10:02:53</p>
<p>输出</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:50.0,2020-09-04 10:03:00.0]|spark|3    |<br>+———————————————+—–+—–+       </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线上涨2020-09-04 10:02:49 &gt;= 2020-09-04 10:02:40.0 成立 ，有新窗口出现，老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据不再处理）</span><br></pre></td></tr></table></figure>

<p>输入 spark,2020-09-04 10:03:06 </p>
<p>输出 </p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:03:00.0,2020-09-04 10:03:10.0]|spark|1    |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线上涨 2020-09-04 10:02:56 &gt;= 2020-09-04 10:02:50.0 成立 ，有新窗口出现，老的窗口 [2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]  流入的新数据不再处理）</span><br></pre></td></tr></table></figure>

<p>输入 spark,2020-09-04 10:02:45 </p>
<p>输出为，没有结果输出</p>
<p>+——+—-+—–+<br>|window|word|count|<br>+——+—-+—–+<br>+——+—-+—–+</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:56 保持不变，老的窗口 [2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]  流入的新数据不再处理，因此流入 位于老窗口的 新数据spark,2020-09-04 10:02:45后，老窗口结果没更新）</span><br></pre></td></tr></table></figure>



<h3><span id="结论">结论</span></h3><p>Flink&#x2F;spark  用事件时间 + 窗口 + 水印来解决实际生产中的数据乱序问题，有如下的触发条件：</p>
<p>watermark 时间 &gt;&#x3D; window_end_time (后续到达的落入之前窗口的数据被丢弃掉)；</p>
<p>在 [window_start_time,window_end_time) 中有数据存在，这个窗口是左闭右开的。</p>
<p>此外，WaterMark 的生成是以对象的形式发送到下游，同样会消耗内存，因此水印的生成时间和频率都要进行严格控制，否则会影响我们的正常作业。</p>
<h2><span id="状态存储">状态存储</span></h2><h3><span id="hdfs-state-store-provider">HDFS state store provider</span></h3><p>The HDFS backend state store provider is the default implementation of [[StateStoreProvider]] and [[StateStore]] in which all the data is stored in memory map in the first stage, and then backed by files in an HDFS-compatible file system. 占用executor内存</p>
<h3><span id="rocksdb-state-store-implementation">RocksDB state store implementation</span></h3><p>(嵌入式key-value数据库，不需要额外安装）不消耗executor内存，适用于大状态存储</p>
<p>3.2 以及之后提供支持。</p>
<p>As of Spark 3.2, we add a new built-in state store implementation, RocksDB state store provider.</p>
<p>Rather than keeping the state in the JVM memory, this solution uses RocksDB to efficiently manage the state in the native memory and the local disk.</p>
<p>目前理解是：rocksdb存储状态，替代之前状态存储占用executor 内存，</p>
<p>checkpoint两者都需要配置，checkpoint是在任务故障恢复或重启时用到。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.config(&quot;spark.sql.streaming.stateStore.providerClass&quot;, &quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;)</span><br></pre></td></tr></table></figure>



<h2><span id="sink">Sink</span></h2><p>There are a few types of built-in output sinks.</p>
<p>官方目前只有file 支持exactly-one, kafka支持at-least once</p>
<ul>
<li><strong>File sink</strong> - Stores the output to a directory.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;parquet&quot;)        // can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span><br><span class="line">    .option(&quot;path&quot;, &quot;path/to/destination/dir&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;kafka&quot;)</span><br><span class="line">    .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">    .option(&quot;topic&quot;, &quot;updates&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. See later in the section for more details.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Console sink (for debugging)</strong> - Prints the output to the console&#x2F;stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;console&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;memory&quot;)</span><br><span class="line">    .queryName(&quot;tableName&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<p><strong>ForeachBatch</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: <span class="type">DataFrame</span>, batchId: <span class="type">Long</span>) =&gt;</span><br><span class="line">  batchDF.persist()</span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 1</span></span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 2</span></span><br><span class="line">  batchDF.unpersist()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="trigger">Trigger</span></h2><p>触发器，多长时间执行一次</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Default trigger (runs micro-batch as soon as it can)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ProcessingTime trigger with two-seconds micro-batch interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">&quot;2 seconds&quot;</span>))</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// One-time trigger (Deprecated, encouraged to use Available-now trigger)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Once</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Available-now trigger</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">AvailableNow</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Continuous trigger with one-second checkpointing interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Continuous</span>(<span class="string">&quot;1 second&quot;</span>))</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>



<h2><span id="checkpoint">checkpoint</span></h2><p>任务失败恢复、重启  （Recovering from Failures with Checkpointing）</p>
<p>In case of a failure or intentional shutdown, you can recover the previous progress and state of a previous query, and continue where it left off. This is done using checkpointing and write-ahead logs. You can configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/structured-streaming-programming-guide.html#quick-example">quick example</a>) to the checkpoint location. This checkpoint location has to be a path in an HDFS compatible file system, and can be set as an option in the DataStreamWriter when <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/structured-streaming-programming-guide.html#starting-streaming-queries">starting a query</a>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;path/to/HDFS/dir&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;memory&quot;</span>)</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>





<h2><span id="kafka集成">Kafka集成</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql-kafka-0-10_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka写入console">读取kafka写入console</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> * late threshold = 10s</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; console （watermark + window）</span><br><span class="line"> *</span><br><span class="line"> * 输入： spark,2020-09-04 10:02:49</span><br><span class="line"> * 输出：</span><br><span class="line"> * +------------------------------------------+-----+-----+</span><br><span class="line"> * |window                                    |word |count|</span><br><span class="line"> * +------------------------------------------+-----+-----+</span><br><span class="line"> * |&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">object ExampleKafka_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">    val query = windowedCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka-普通聚合不含窗口">读取kafka 普通聚合(不含窗口)</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; kafka （no window）</span><br><span class="line"> *</span><br><span class="line"> * 常规聚合计算   group by -&gt; count</span><br><span class="line"> * 累加统计 wordcount</span><br><span class="line"> *</span><br><span class="line"> * 输入： hadoop,2020-09-04 10:02:49</span><br><span class="line"> * 输出： hadoop,1</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleKafka3_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      //配置RocksDBStateStore, 默认HdfsStateStore</span><br><span class="line">      .config(&quot;spark.sql.streaming.stateStore.providerClass&quot;, &quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .groupBy(</span><br><span class="line">          $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    *</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|window                                    |word |count|</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line">    *</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">//    val query = windowedCounts</span><br><span class="line">//      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">//      .writeStream</span><br><span class="line">//      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">//      .format(&quot;console&quot;)</span><br><span class="line">//      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">//      .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Write key-value data from a DataFrame to a specific Kafka topic specified in an option</span><br><span class="line">    val query = windowedCounts</span><br><span class="line">      .selectExpr(&quot;concat_ws(&#x27;,&#x27;, word, cast(count as string)) as value&quot;)</span><br><span class="line">      .writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .option(&quot;checkpointLocation&quot;, &quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1/out1&quot;)</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;,  &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;topic&quot;, &quot;testzq2&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka-窗口聚合含水印">读取kafka 窗口聚合(含水印)</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> * late threshold = 10s</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; kafka （watermark + window）</span><br><span class="line"> *</span><br><span class="line"> * 输入： spark,2020-09-04 10:02:49</span><br><span class="line"> * 输出： &#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;,spark,1</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleKafka2_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    *</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|window                                    |word |count|</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line">    *</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">//    val query = windowedCounts</span><br><span class="line">//      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">//      .writeStream</span><br><span class="line">//      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">//      .format(&quot;console&quot;)</span><br><span class="line">//      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">//      .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Write key-value data from a DataFrame to a specific Kafka topic specified in an option</span><br><span class="line">    val query = windowedCounts</span><br><span class="line">      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">      .writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .option(&quot;checkpointLocation&quot;, &quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1/out&quot;)</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;,  &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;topic&quot;, &quot;testzq2&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/" data-id="cljy55ft700000n9a3c2vb7n4" data-title="20230711_structStreaming" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/12/bigdata/spark/spark3/20230712_pyspark/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          20230712_pyspark
        
      </div>
    </a>
  
  
    <a href="/2023/07/09/bigdata/spark/spark3/20230707_sparksql/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">20230707_sparksql</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ai-llm-langchain/">ai/llm/langchain</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai-llm-prompt/">ai/llm/prompt</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-flink-flink-doc/">bigdata/flink/flink_doc</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-kafka/">bigdata/kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-env/">bigdata/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-tune/">bigdata/spark/spark_tune</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-sql/">bigdata/sql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-concurrent/">java/concurrent</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-designpattern/">java/designpattern</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-jvm/">java/jvm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql/" rel="tag">sql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/flink/" style="font-size: 10px;">flink</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/java/" style="font-size: 16.67px;">java</a> <a href="/tags/kafka/" style="font-size: 10px;">kafka</a> <a href="/tags/llm/" style="font-size: 13.33px;">llm</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sql/" style="font-size: 10px;">sql</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/27/ai/llm/prompt/wu_en_da_prompt/">wu_en_da_prompt</a>
          </li>
        
          <li>
            <a href="/2023/07/21/ai/llm/langchain/langchain_usage_20230721/">langchain_usage_20230721</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/kafka/kafka_basic/">kafka_basic</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/spark/spark_tune/spark_conf/">spark_conf</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/spark/spark_tune/spark_shuffle_usage/">spark_shuffle_usage</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>