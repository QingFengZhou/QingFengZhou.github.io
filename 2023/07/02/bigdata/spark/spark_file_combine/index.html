<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>spark_file_combine | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="小文件合并 使用 Spark sql 其他总结   Spark code hive insert  overwrite Hive 分片和文件合并参数调节      小文件合并123456789101112131415161718create external table if not exists emp_ext(    	empno int,	ename string,	job strin">
<meta property="og:type" content="article">
<meta property="og:title" content="spark_file_combine">
<meta property="og:url" content="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_file_combine/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="小文件合并 使用 Spark sql 其他总结   Spark code hive insert  overwrite Hive 分片和文件合并参数调节      小文件合并123456789101112131415161718create external table if not exists emp_ext(    	empno int,	ename string,	job strin">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-02T15:30:09.000Z">
<meta property="article:modified_time" content="2023-07-04T02:09:23.393Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bigdata/spark/spark_file_combine" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_file_combine/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:30:09.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      spark_file_combine
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6">小文件合并</a><ul>
<li><a href="#%E4%BD%BF%E7%94%A8-spark-sql">使用 Spark sql</a><ul>
<li><a href="#%E5%85%B6%E4%BB%96%E6%80%BB%E7%BB%93">其他总结</a></li>
</ul>
</li>
<li><a href="#spark-code">Spark code</a></li>
<li><a href="#hive-insert-overwrite">hive insert  overwrite</a></li>
<li><a href="#hive-%E5%88%86%E7%89%87%E5%92%8C%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82">Hive 分片和文件合并参数调节</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="小文件合并">小文件合并</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists emp_ext(</span><br><span class="line">    	empno int,</span><br><span class="line">	ename string,</span><br><span class="line">	job string,</span><br><span class="line">	mgr int,</span><br><span class="line">	hiredate string,</span><br><span class="line">	sal double,</span><br><span class="line">	comm double,</span><br><span class="line">	deptno int</span><br><span class="line">) row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">LOCATION &#x27;/user/hive/warehouse/emp&#x27;;</span><br><span class="line"></span><br><span class="line">CREATE EXTERNAL TABLE parquet_test1 (value string) </span><br><span class="line">STORED AS PARQUET </span><br><span class="line">LOCATION &#x27;/tmp/test_out/parquet_big/&#x27;</span><br><span class="line">TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;SNAPPY&quot;)</span><br><span class="line">;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="使用-spark-sql">使用 Spark sql</span></h3><p>思路：创建临时表，通过查看文件目录大小判断分区个数，通过distribute by num_partitions重分区 写入到临时表，判断临时表和源表数据内容是否一致，如果一致，删除源表，重命名临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># insert overwrite源表 报错</span><br><span class="line">insert overwrite table parquet_test1 select * from parquet_test1;</span><br><span class="line"></span><br><span class="line">create table parquet_test2 like  parquet_test1 LOCATION &#x27;/tmp/test_out/parquet_big3/&#x27;</span><br><span class="line"></span><br><span class="line">insert overwrite table parquet_test2 select * from parquet_test1 distribute by floor(rand()*2);</span><br><span class="line"></span><br><span class="line"># 判断源表parquet_test1 和 parquet_test2是否数据量一致</span><br><span class="line"># 删除源表parquet_test1</span><br><span class="line">drop table parquet_test1;</span><br><span class="line"></span><br><span class="line">ALTER TABLE parquet_test2 RENAME TO parquet_test1</span><br></pre></td></tr></table></figure>

<h4><span id="其他总结">其他总结</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(1) 对于原始数据进行按照分区字段进行shuffle，可以规避小文件问题。但有可能引入数据倾斜的问题；</span><br><span class="line">(2) sql中引入 distribute by ，指定分区字段或分区表达式</span><br><span class="line">(3) 已知倾斜key的情况，将数据分为两部分处理，倾斜部分按rand()函数 重分区，未倾斜部分常规处理</span><br><span class="line">(4) 对于Spark 2.4 以上版本的用户，sql中 可以使用HINT提示</span><br><span class="line">    insert into select /*+ REPARTITION(2) */  id,name  from tb1 where id &gt; 0</span><br><span class="line">    insert into select /*+ COALESCE(2) */  id,name  from tb1 where id &gt; 0</span><br><span class="line">(5) spark3.0以上开启自适应查询执行：</span><br><span class="line">spark.sql.adaptive.enabled=true;</span><br><span class="line">spark.sql.adaptive.coalescePartitions.enabled=true;</span><br><span class="line">spark.sql.adaptive.coalescePartitions.parallelismFirst = false;</span><br><span class="line">spark.sql.adaptive.advisoryPartitionSizeInBytes = 64m;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="spark-code">Spark code</span></h3><p>思路：数据写入临时文件目录，判断临时文件目录大小设定重分区个数，数据写入正式文件目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.fs.&#123;FileSystem, Path&#125;</span><br><span class="line"></span><br><span class="line">    nowDF.persist()</span><br><span class="line">    val tempPath = &quot;/nativeinfopath/&quot;</span><br><span class="line">    nowDF.write.mode(&quot;overwrite&quot;).parquet(tempPath)</span><br><span class="line">    </span><br><span class="line">    val fs = FileSystem.get(sc.hadoopConfiguration)</span><br><span class="line">    val dirSize = fs.getContentSummary(new Path(tempPath)).getLength</span><br><span class="line">    val fileNum = dirSize / (128 * 1024 * 1024) </span><br><span class="line">    </span><br><span class="line">    val regularPath = &quot;&quot;</span><br><span class="line">    nowDF.coalesce(fileNum.toInt).write.mode(&quot;overwrite&quot;).parquet(regularPath)</span><br><span class="line">    nowDF.unpersist()</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h3><span id="hive-insert-overwrite">hive  insert  overwrite</span></h3><p>使用hive  insert  overwrite  (自动小文件合并，不需要创建临时表，分区表分区也适用)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE parquet_test1 (value string) </span><br><span class="line">STORED AS PARQUET </span><br><span class="line">LOCATION &#x27;/tmp/test_out/parquet_big/&#x27;</span><br><span class="line">TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;SNAPPY&quot;)</span><br><span class="line">;</span><br><span class="line">set hive.execution.engine = mr;</span><br><span class="line">insert overwrite table parquet_test1 select * from parquet_test1;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--分区表</span><br><span class="line">day=`date  +&quot;%Y%m%d&quot; -d  &quot;-1 days&quot;`</span><br><span class="line">hive  -d  day=$&#123;day&#125;  -e  &quot;use gps; </span><br><span class="line">                           alter table gpsinfo  add partition (day=$&#123;day&#125;);</span><br><span class="line">                           insert overwrite table gpsinfo partition(day)  select * from gpsinfo where day=$&#123;day&#125;;&quot; </span><br></pre></td></tr></table></figure>

<h3><span id="hive-分片和文件合并参数调节">Hive 分片和文件合并参数调节</span></h3><p>hive 分片大小和自动小文件合并参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//每个Map最大输入大小(这个值决定了合并后文件的数量)</span><br><span class="line">set mapred.max.split.size=256000000;  </span><br><span class="line">//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span><br><span class="line">set mapred.min.split.size.per.node=100000000;</span><br><span class="line">//一个rack下split的至少的大小(这个值决定了rack上的文件是否需要合并)  </span><br><span class="line">set mapred.min.split.size.per.rack=100000000;</span><br><span class="line">//执行Map前进行小文件合并</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; </span><br><span class="line">//设置map端输出进行合并，默认为true</span><br><span class="line">set hive.merge.mapfiles = true</span><br><span class="line">//设置reduce端输出进行合并，默认为false</span><br><span class="line">set hive.merge.mapredfiles = true</span><br><span class="line">//设置合并文件的大小</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000</span><br><span class="line">//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</span><br><span class="line">set hive.merge.smallfiles.avgsize=16000000</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_file_combine/" data-id="cljllibc40001029a724i9b3s" data-title="spark_file_combine" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/07/02/bigdata/spark/spark_file_read_partition_num/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          spark_file_read_partition_num
        
      </div>
    </a>
  
  
    <a href="/2023/07/02/bigdata/spark/spark_minio/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">spark_minio</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdate-spark-spark-env/">bigdate/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/spark/" style="font-size: 10px;">spark</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">spark环境搭建</a>
          </li>
        
          <li>
            <a href="/2023/07/04/tools/draw/%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/processon/">processon</a>
          </li>
        
          <li>
            <a href="/2023/07/04/system/linux/prometheus/prometheus%E4%BD%BF%E7%94%A8/">prometheus使用</a>
          </li>
        
          <li>
            <a href="/2023/07/03/system/linux/ansible/ansible%E4%BD%BF%E7%94%A8/">ansible使用</a>
          </li>
        
          <li>
            <a href="/2023/07/03/tools/java/maven/maven%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/">maven远程仓库配置</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>