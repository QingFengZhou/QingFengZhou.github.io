<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://qingfengzhou.github.io/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/flink/flink_doc/flink_sql_20230713" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/" class="article-date">
  <time class="dt-published" datetime="2023-07-13T08:39:40.000Z" itemprop="datePublished">2023-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-flink-flink-doc/">bigdata/flink/flink_doc</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/">flink_sql_20230713</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#env">Env</a></li>
<li><a href="#arch">Arch</a><ul>
<li><a href="#basic">Basic</a><ul>
<li><a href="#jobmanager"><strong>JobManager</strong></a></li>
<li><a href="#taskmanager"><strong>TaskManager</strong></a></li>
<li><a href="#client"><strong>Client</strong></a></li>
<li><a href="#%E4%BB%BB%E5%8A%A1%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F"><strong>任务部署模式</strong></a></li>
</ul>
</li>
<li><a href="#%E4%BB%BB%E5%8A%A1%E9%83%A8%E7%BD%B2">任务部署</a><ul>
<li><a href="#%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%B5%8B%E8%AF%95">任务提交测试</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sql">Sql</a><ul>
<li><a href="#example">Example</a></li>
<li><a href="#concepts">Concepts</a><ul>
<li><a href="#dynamic-table">dynamic table</a></li>
<li><a href="#versioned-table">Versioned table</a><ul>
<li><a href="#versioned-table-source">Versioned table source</a></li>
<li><a href="#versioned-table-views">Versioned table views</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#join">Join</a><ul>
<li><a href="#regular-join">Regular Join</a></li>
<li><a href="#interval-join">Interval Join</a></li>
<li><a href="#temporal-join">Temporal Join</a><ul>
<li><a href="#code-test">Code Test</a></li>
</ul>
</li>
<li><a href="#lookup-join">Lookup Join</a></li>
<li><a href="#window-join">Window Join</a></li>
</ul>
</li>
<li><a href="#connector">Connector</a><ul>
<li><a href="#datagen">DataGen</a></li>
<li><a href="#filesystem">FileSystem</a></li>
<li><a href="#blackhole">BlackHole</a></li>
<li><a href="#print">Print</a></li>
<li><a href="#jdbc">JDBC</a></li>
<li><a href="#kafka">Kafka</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<!-- tocstop -->

<h2><span id="env">Env</span></h2><p>local: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /Users/zhouqingfeng/Desktop/software/flink/flink-1.17.1</span><br><span class="line"></span><br><span class="line">./bin/start-cluster.sh</span><br><span class="line"></span><br><span class="line">http://localhost:8081/</span><br></pre></td></tr></table></figure>



<h2><span id="arch">Arch</span></h2><h3><span id="basic">Basic</span></h3><h4><span id="jobmanager"><strong>JobManager</strong></span></h4><p>resourceManager （资源管理(包括task slot) ）</p>
<p>任务分发(dispatcher) </p>
<p>任务管理 (JobMaster管理每个JobGraph)</p>
<h4><span id="taskmanager"><strong>TaskManager</strong></span></h4><p>执行job Task</p>
<h4><span id="client"><strong>Client</strong></span></h4><p>核心是执行每个Flink Application main( )方法，类似spark driver，</p>
<p>下载application 依赖到本地、执行main方法生成JobGraph、上传依赖和JobGraph到集群。</p>
<p>Application mode出现之前client 都运行在本地。</p>
<h4><span id="任务部署模式"><strong>任务部署模式</strong></span></h4><ul>
<li><p>in Application Mode     </p>
<p>main() 方法运行在 Jobmaster，类似spark 的yarn-cluster，driver单独运行，client开销非常小</p>
</li>
<li><p>in a Per-Job Mode (deprecated)</p>
<p>main() 方法运行在client，开销比较大 </p>
</li>
<li><p>in Session Mode</p>
<p>main() 方法运行在client，开销比较大</p>
</li>
</ul>
<h3><span id="任务部署">任务部署</span></h3><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/resource-providers/yarn/#deployment-modes-supported-by-flink-on-yarn">flink任务部署到集群</a></p>
<h4><span id="任务提交测试">任务提交测试</span></h4><p>Flink 1.12.*提交到hadoop yarn</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; </span><br><span class="line"></span><br><span class="line">  Options for yarn-cluster mode:</span><br><span class="line">     -d,--detached                        If present, runs the job in detached</span><br><span class="line">                                          mode</span><br><span class="line">     -m,--jobmanager &lt;arg&gt;                Set to yarn-cluster to use YARN</span><br><span class="line">                                          execution mode.</span><br><span class="line">     -yat,--yarnapplicationType &lt;arg&gt;     Set a custom application type for the</span><br><span class="line">                                          application on YARN</span><br><span class="line">     -yD &lt;property=value&gt;                 use value for given property</span><br><span class="line">     -yd,--yarndetached                   If present, runs the job in detached</span><br><span class="line">                                          mode (deprecated; use non-YARN</span><br><span class="line">                                          specific option instead)</span><br><span class="line">     -yh,--yarnhelp                       Help for the Yarn session CLI.</span><br><span class="line">     -yid,--yarnapplicationId &lt;arg&gt;       Attach to running YARN session</span><br><span class="line">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class="line">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container with</span><br><span class="line">                                          optional unit (default: MB)</span><br><span class="line">     -ynl,--yarnnodeLabel &lt;arg&gt;           Specify YARN node label for the YARN</span><br><span class="line">                                          application</span><br><span class="line">     -ynm,--yarnname &lt;arg&gt;                Set a custom name for the application</span><br><span class="line">                                          on YARN</span><br><span class="line">     -yq,--yarnquery                      Display available YARN resources</span><br><span class="line">                                          (memory, cores)</span><br><span class="line">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">     -ys,--yarnslots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class="line">                                          (t for transfer)</span><br><span class="line">     -ytm,--yarntaskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with</span><br><span class="line">                                          optional unit (default: MB)</span><br><span class="line">     -yz,--yarnzookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper</span><br><span class="line">                                          sub-paths for high availability mode</span><br><span class="line">     -z,--zookeeperNamespace &lt;arg&gt;        Namespace to create the Zookeeper</span><br><span class="line">                                          sub-paths for high availability mode</span><br></pre></td></tr></table></figure>



<p><strong>简单测试</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1  examples/batch/WordCount.jar  \</span><br><span class="line">--input   hdfs://localhost:9000/tmp/test/flink/ \</span><br><span class="line">--output  hdfs://localhost:9000/tmp/test/fout</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#file local</span><br><span class="line"></span><br><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1  examples/batch/WordCount.jar  --input    file:///Users/zhouqingfeng/Desktop/target/datalake/flink/flink-1.12.3/README.txt    --output  file:///Users/zhouqingfeng/Desktop/target/datalake/flink/flink-1.12.3/out.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1   -ynm  flinkTest -c com.test1.flink.connector.KafkaStreamTableTest1 \</span><br><span class="line">/Users/zhouqingfeng/Desktop/target/datalake/flink/projects/test/flink-playgrounds-master/table-walkthrough/target/spend-report-1.0.0.jar</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 -ys 1   -ynm  flinkTest -c com.test1.flink.connector.HiveTest2  /Users/zhouqingfeng/Desktop/target/datalake/flink/projects/test/flink-playgrounds-master/table-walkthrough/target/spend-report-1.0.0.jar</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="sql">Sql</span></h2><h3><span id="example">Example</span></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/sql-client.sh</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt;&gt; 12.csv</span></span><br><span class="line"><span class="string">1,lisa,fina</span></span><br><span class="line"><span class="string">2,john,sales</span></span><br><span class="line"><span class="string">3,jimmy,sales</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE employee_info (</span><br><span class="line">    emp_id INT,</span><br><span class="line">    name VARCHAR,</span><br><span class="line">    dept VARCHAR</span><br><span class="line">) WITH ( </span><br><span class="line">    &#x27;connector&#x27; = &#x27;filesystem&#x27;,</span><br><span class="line">    &#x27;path&#x27; = &#x27;file:/Users/zhouqingfeng/Desktop/software/flink/flink-1.17.1/test/12.csv&#x27;,</span><br><span class="line">    &#x27;format&#x27; = &#x27;csv&#x27;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">select * from employee_info;</span><br></pre></td></tr></table></figure>



<h3><span id="concepts">Concepts</span></h3><h4><span id="dynamic-table">dynamic table</span></h4><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/concepts/dynamic_tables/">流表转换</a></p>
<h4><span id="versioned-table">Versioned table</span></h4><p>key的值随时间不短变化，表记录key的历史值</p>
<p>Flink SQL operates over dynamic tables that evolve, which may either be append-only or updating.</p>
<p>Versioned tables represent a special type of updating table that remembers the past values for each key.</p>
<p>Flink SQL can define versioned tables over any dynamic table with a <code>PRIMARY KEY</code> constraint and time attribute.</p>
<p>Fink sql基于主键约束和时间属性 定义版本表。</p>
<h5><span id="versioned-table-source">Versioned table source</span></h5><h5><span id="versioned-table-views">Versioned table views</span></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.17.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> currency_rates (</span><br><span class="line">	currency      STRING,</span><br><span class="line">	rate          <span class="type">DECIMAL</span>(<span class="number">32</span>, <span class="number">10</span>),</span><br><span class="line">	`update_time` <span class="type">TIMESTAMP</span>(<span class="number">3</span>) METADATA <span class="keyword">FROM</span> <span class="string">&#x27;timestamp&#x27;</span>,</span><br><span class="line">	WATERMARK <span class="keyword">FOR</span> update_time <span class="keyword">AS</span> update_time</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">	<span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;topic&#x27;</span>	    <span class="operator">=</span> <span class="string">&#x27;rates&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;properties.bootstrap.servers&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.group.id&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;testGroup&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;latest-offset&#x27;</span>,</span><br><span class="line">	<span class="string">&#x27;format&#x27;</span>    <span class="operator">=</span> <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">&#123;&quot;currency&quot;:&quot;Yen&quot;, &quot;rate&quot;:<span class="number">102</span>&#125;</span><br><span class="line">&#123;&quot;currency&quot;:&quot;Euro&quot;, &quot;rate&quot;:<span class="number">114</span>&#125;</span><br><span class="line">&#123;&quot;currency&quot;:&quot;USD&quot;, &quot;rate&quot;:<span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Define a versioned view</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> versioned_rates <span class="keyword">AS</span>              </span><br><span class="line"><span class="keyword">SELECT</span> currency, rate, update_time              <span class="comment">-- (1) `update_time` keeps the event time</span></span><br><span class="line">  <span class="keyword">FROM</span> (</span><br><span class="line">      <span class="keyword">SELECT</span> <span class="operator">*</span>,</span><br><span class="line">      <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> currency  <span class="comment">-- (2) the inferred unique key `currency` can be a primary key</span></span><br><span class="line">         <span class="keyword">ORDER</span> <span class="keyword">BY</span> update_time <span class="keyword">DESC</span>) <span class="keyword">AS</span> rownum </span><br><span class="line">      <span class="keyword">FROM</span> currency_rates)</span><br><span class="line"><span class="keyword">WHERE</span> rownum <span class="operator">=</span> <span class="number">1</span>; </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> print_table ( </span><br><span class="line">	currency      STRING,</span><br><span class="line">	rate          <span class="type">DECIMAL</span>(<span class="number">32</span>, <span class="number">10</span>),</span><br><span class="line">	`update_time` <span class="type">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">with</span> ( </span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;print&#x27;</span> </span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> print_table  <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> versioned_rates;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3><span id="join">Join</span></h3><h4><span id="regular-join">Regular Join</span></h4><h4><span id="interval-join">Interval Join</span></h4><h4><span id="temporal-join">Temporal Join</span></h4><p>Temporal table 时态表，就是版本表，存储表的多个快照(历史版本)，这里就是版本表</p>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/679659?spm=a2c6h.12873639.0.0.27345d11AZJgKz#slide-10">Temporal Table JOIN内部原理</a>，内部类似拉链表，存储表的多个快照版本，</p>
<p>相当于flink在内存里存储了多个key的状态，每个状态对应不同的时间窗口(数据有效期)</p>
<p>其他资料：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/475660438">Flink temporal table join研究</a></p>
<h5><span id="code-test">Code Test</span></h5><h4><span id="lookup-join">Lookup Join</span></h4><p>A lookup join is typically used to enrich a table with data that is queried from an external system.</p>
<p>常规维表join，每次查询最新数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Customers is backed by the JDBC connector and can be used for lookup joins</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">TABLE</span> Customers (</span><br><span class="line">  id <span class="type">INT</span>,</span><br><span class="line">  name STRING,</span><br><span class="line">  country STRING,</span><br><span class="line">  zip STRING</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;url&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;jdbc:mysql://mysqlhost:3306/customerdb&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;table-name&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;customers&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- enrich each order with customer information</span></span><br><span class="line"><span class="keyword">SELECT</span> o.order_id, o.total, c.country, c.zip</span><br><span class="line"><span class="keyword">FROM</span> Orders <span class="keyword">AS</span> o</span><br><span class="line">  <span class="keyword">JOIN</span> Customers <span class="keyword">FOR</span> <span class="built_in">SYSTEM_TIME</span> <span class="keyword">AS</span> <span class="keyword">OF</span> o.proc_time <span class="keyword">AS</span> c</span><br><span class="line">    <span class="keyword">ON</span> o.customer_id <span class="operator">=</span> c.id;</span><br></pre></td></tr></table></figure>



<h4><span id="window-join">Window Join</span></h4><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/queries/window-join/">基于window 进行join</a></p>
<h3><span id="connector">Connector</span></h3><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/">https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/</a></p>
<h4><span id="datagen">DataGen</span></h4><p>The DataGen connector allows for creating tables based on in-memory data generation. </p>
<h4><span id="filesystem">FileSystem</span></h4><p>在mac本地测试 指定目录下 生成的json文件是隐藏的，以.part- 开头</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">--source</span><br><span class="line">CREATE TABLE orders (   </span><br><span class="line">                   order_id BIGINT,   </span><br><span class="line">                   product_id BIGINT,   </span><br><span class="line">                   order_time TIMESTAMP(3),   </span><br><span class="line">                   order_amount DECIMAL(10, 2)   </span><br><span class="line">                 ) WITH (   </span><br><span class="line">                   &#x27;connector&#x27; = &#x27;kafka&#x27;,   </span><br><span class="line">                   &#x27;topic&#x27; = &#x27;testzq1&#x27;,   </span><br><span class="line">                   &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,   </span><br><span class="line">                   &#x27;properties.group.id&#x27; = &#x27;orders-consumer&#x27;,   </span><br><span class="line">                   &#x27;scan.startup.mode&#x27; = &#x27;latest-offset&#x27;,   </span><br><span class="line">                   &#x27;format&#x27; = &#x27;json&#x27;   </span><br><span class="line">                 )</span><br><span class="line"></span><br><span class="line">--dest</span><br><span class="line">CREATE TABLE orders_history (  </span><br><span class="line">                   order_id BIGINT,  </span><br><span class="line">                   product_id BIGINT,  </span><br><span class="line">                   order_time TIMESTAMP(3),  </span><br><span class="line">                   order_amount DECIMAL(10, 2)  </span><br><span class="line">                 ) WITH (  </span><br><span class="line">                   &#x27;connector&#x27; = &#x27;filesystem&#x27;,  </span><br><span class="line">                   &#x27;path&#x27; = &#x27;file:///Users/zhouqingfeng/Desktop/mydirect/tmp/flink/v1&#x27;,  </span><br><span class="line">                   &#x27;format&#x27; = &#x27;json&#x27;  </span><br><span class="line">                 ) </span><br><span class="line"></span><br><span class="line">--insert source -&gt; dest</span><br><span class="line">INSERT INTO orders_history </span><br><span class="line">                SELECT * FROM orders</span><br></pre></td></tr></table></figure>



<h4><span id="blackhole">BlackHole</span></h4><p>The BlackHole connector allows for swallowing all input records. It is designed for:</p>
<ul>
<li>high performance testing.</li>
<li>UDF to output, not substantive sink.</li>
</ul>
<p>Just like &#x2F;dev&#x2F;null device on Unix-like operating systems.</p>
<p>The BlackHole connector is built-in.</p>
<h4><span id="print">Print</span></h4><p>The Print connector allows for writing every row to the standard output or standard error stream.</p>
<p>It is designed for:</p>
<ul>
<li>Easy test for streaming job.</li>
<li>Very useful in production debugging.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">String outputSql = &quot;CREATE TABLE print_table (\n&quot; +</span><br><span class="line">        &quot;    id  int,\n&quot; +</span><br><span class="line">        &quot;    name      string \n&quot; +</span><br><span class="line">        &quot;        ) WITH (\n&quot; +</span><br><span class="line">        &quot;                &#x27;connector&#x27; = &#x27;print&#x27;\n&quot; +</span><br><span class="line">        &quot;        )&quot;;</span><br></pre></td></tr></table></figure>



<h4><span id="jdbc">JDBC</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">String inputSql= &quot;CREATE TABLE myUser2 (\n&quot; +</span><br><span class="line">        &quot;    id  int,\n&quot; +</span><br><span class="line">        &quot;    name      string \n&quot; +</span><br><span class="line">        &quot;) WITH (\n&quot; +</span><br><span class="line">        &quot;  &#x27;connector&#x27;  = &#x27;jdbc&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;url&#x27;        = &#x27;jdbc:mysql://localhost:3306/test_zhou&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;table-name&#x27; = &#x27;users2&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;driver&#x27;     = &#x27;com.mysql.cj.jdbc.Driver&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;username&#x27;   = &#x27;root&#x27;,\n&quot; +</span><br><span class="line">        &quot;  &#x27;password&#x27;   = &#x27;Zoom@123&#x27;\n&quot; +</span><br><span class="line">        &quot;)&quot;;</span><br></pre></td></tr></table></figure>



<h4><span id="kafka">Kafka</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE orders (   </span><br><span class="line">                   order_id BIGINT,   </span><br><span class="line">                   product_id BIGINT,   </span><br><span class="line">                   order_time TIMESTAMP(3),   </span><br><span class="line">                   order_amount DECIMAL(10, 2)   </span><br><span class="line">                 ) WITH (   </span><br><span class="line">                   &#x27;connector&#x27; = &#x27;kafka&#x27;,   </span><br><span class="line">                   &#x27;topic&#x27; = &#x27;testzq1&#x27;,   </span><br><span class="line">                   &#x27;properties.bootstrap.servers&#x27; = &#x27;localhost:9092&#x27;,   </span><br><span class="line">                   &#x27;properties.group.id&#x27; = &#x27;orders-consumer&#x27;,   </span><br><span class="line">                   &#x27;scan.startup.mode&#x27; = &#x27;latest-offset&#x27;,   </span><br><span class="line">                   &#x27;format&#x27; = &#x27;json&#x27;   </span><br><span class="line">                 )</span><br></pre></td></tr></table></figure>





<h2><span id="references">References</span></h2><p><a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/overview/">Flink1.17 sql</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/" data-id="clk0zo6b70000cu9a9fpjej36" data-title="flink_sql_20230713" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flink/" rel="tag">flink</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_env/spark_terminal_tips" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/13/bigdata/spark/spark_env/spark_terminal_tips/" class="article-date">
  <time class="dt-published" datetime="2023-07-13T07:35:03.000Z" itemprop="datePublished">2023-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark-env/">bigdata/spark/spark_env</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/13/bigdata/spark/spark_env/spark_terminal_tips/">spark_terminal_tips</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3><span id="spark-shell">Spark-shell</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exit: :quit</span><br><span class="line">多行输入: :paste</span><br><span class="line">dataset 截断设置：dataset.show(false)</span><br></pre></td></tr></table></figure>



<h3><span id="spark-sql">Spark-sql</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sparksql 输出table head: spark.hadoop.hive.cli.print.header=true</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/13/bigdata/spark/spark_env/spark_terminal_tips/" data-id="clk0u7fk900009p9a7x29dzo0" data-title="spark_terminal_tips" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230712_spark_delta_lake" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/13/bigdata/spark/spark3/20230712_spark_delta_lake/" class="article-date">
  <time class="dt-published" datetime="2023-07-13T07:27:47.000Z" itemprop="datePublished">2023-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/13/bigdata/spark/spark3/20230712_spark_delta_lake/">20230713_delta_lake</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#env">Env</a></li>
<li><a href="#examples-test">Examples Test</a></li>
<li><a href="#batch">Batch</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">总结</a></li>
<li><a href="#%E8%B5%84%E6%96%99">资料</a></li>
</ul>
<!-- tocstop -->

<h2><span id="env">Env</span></h2><p>Spark 3.4.1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;io.delta&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;delta-core_2.12&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-sql --packages io.delta:delta-core_2.12:2.4.0 \</span><br><span class="line">--conf &quot;spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension&quot; \</span><br><span class="line">--conf &quot;spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog&quot; \</span><br><span class="line">--conf spark.hadoop.hive.cli.print.header=true</span><br><span class="line"></span><br><span class="line">bin/spark-shell --packages io.delta:delta-core_2.12:2.4.0 \</span><br><span class="line">--conf &quot;spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension&quot; \</span><br><span class="line">--conf &quot;spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog&quot; \</span><br><span class="line">--conf spark.hadoop.hive.cli.print.header=true</span><br></pre></td></tr></table></figure>



<h2><span id="examples-test">Examples Test</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># create delta table, 这里是设定了表的路径, 未指定表名</span><br><span class="line">CREATE TABLE delta.`/tmp/delta-table` USING DELTA AS SELECT col1 as id FROM VALUES 0,1,2,3,4;</span><br><span class="line"></span><br><span class="line">#overwrite</span><br><span class="line">INSERT OVERWRITE delta.`/tmp/delta-table` SELECT col1 as id FROM VALUES 5,6,7,8,9;</span><br><span class="line"></span><br><span class="line">-- Update every even value by adding 100 to it</span><br><span class="line">UPDATE delta.`/tmp/delta-table` SET id = id + 100 WHERE id % 2 == 0;</span><br><span class="line"></span><br><span class="line">-- Delete very even value</span><br><span class="line">DELETE FROM delta.`/tmp/delta-table` WHERE id % 2 == 0;</span><br><span class="line"></span><br><span class="line">-- Upsert (merge) new data</span><br><span class="line">CREATE TEMP VIEW newData AS SELECT col1 AS id FROM VALUES 1,3,5,7,9,11,13,15,17,19;</span><br><span class="line"></span><br><span class="line">MERGE INTO delta.`/tmp/delta-table` AS oldData</span><br><span class="line">USING newData</span><br><span class="line">ON oldData.id = newData.id</span><br><span class="line">WHEN MATCHED</span><br><span class="line">  THEN UPDATE SET id = newData.id</span><br><span class="line">WHEN NOT MATCHED</span><br><span class="line">  THEN INSERT (id) VALUES (newData.id);</span><br><span class="line"></span><br><span class="line">SELECT * FROM delta.`/tmp/delta-table`;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-- 查询快照：历史版本</span><br><span class="line">-- Read older versions of data using time travel</span><br><span class="line">SELECT * FROM delta.`/tmp/delta-table` VERSION AS OF 0;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--struct stream write to delta</span><br><span class="line">val streamingDf = spark.readStream.format(&quot;rate&quot;).load()</span><br><span class="line">val stream = streamingDf.select($&quot;value&quot; as &quot;id&quot;).writeStream.format(&quot;delta&quot;).option(&quot;checkpointLocation&quot;, &quot;/tmp/checkpoint&quot;).start(&quot;/tmp/delta-table2&quot;)</span><br><span class="line"></span><br><span class="line">--struct stream read from delta</span><br><span class="line"></span><br><span class="line">val stream2 = spark.readStream.format(&quot;delta&quot;).load(&quot;/tmp/delta-table2&quot;).writeStream.format(&quot;console&quot;).start()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="batch">Batch</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS default.people10m (</span><br><span class="line">  id INT,</span><br><span class="line">  firstName STRING,</span><br><span class="line">  middleName STRING,</span><br><span class="line">  lastName STRING,</span><br><span class="line">  gender STRING,</span><br><span class="line">  birthDate TIMESTAMP,</span><br><span class="line">  ssn STRING,</span><br><span class="line">  salary INT</span><br><span class="line">) USING DELTA</span><br><span class="line"></span><br><span class="line">insert into default.people10m values(1, &#x27;jimmy&#x27;, &#x27;dell&#x27;, &#x27;li&#x27;, &#x27;m&#x27;, &#x27;2001-11-12 12:21:21&#x27;, &#x27;ssn1&#x27;, 2600);</span><br><span class="line">insert into default.people10m values(2, &#x27;Jack&#x27;, &#x27;dell&#x27;, &#x27;li&#x27;, &#x27;m&#x27;, &#x27;2002-10-12 12:21:21&#x27;, &#x27;ssn1&#x27;, 2600);</span><br><span class="line">insert into default.people10m values(3, &#x27;lisa&#x27;, &#x27;dell&#x27;, &#x27;li&#x27;, &#x27;f&#x27;, &#x27;2001-09-16 12:21:21&#x27;, &#x27;ssn1&#x27;, 2600);</span><br><span class="line"></span><br><span class="line">DESCRIBE HISTORY default.people10m</span><br><span class="line">SELECT * FROM default.people10m TIMESTAMP AS OF &#x27;2018-10-18T22:15:12.013Z&#x27;</span><br><span class="line">SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="总结">总结</span></h2><p>Delta lake是数据湖的一种开源实现，</p>
<p>它是一种新型的存储格式(tableFormat)，可以看成是对传统数据仓库功能的扩展，弥补了传统数据仓库的不足</p>
<p>(1)  统一存储，相当于是大一统的储存，传统数仓主要是存储结构化数据，湖存储可以存储结构化、半结构化、非结构化数据。（表、音频、视频、文本等，底层是hdfs | oss对象存储，对象存储本身适合存储任意格式的数据）</p>
<p>(2)  支持事务，满足ACID，传统数仓不支持事务</p>
<p>(3)  流批统一。delta 表可以作为流计算的输入|输出，也可以作为批计算的输入|输出</p>
<p>(4)  支持time travel。可以存储数据的多个快照版本，查看历史版本数据。</p>
<p>(5)  支持upsert 和delete操作</p>
<p>(6) Scalable metadata handling: 支持管理超大规模表对应的元数据</p>
<h2><span id="资料">资料</span></h2><p><a target="_blank" rel="noopener" href="https://delta.io/">官网</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/13/bigdata/spark/spark3/20230712_spark_delta_lake/" data-id="clk0u7fkf00019p9aaj3hhad7" data-title="20230713_delta_lake" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230712_pyspark" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/12/bigdata/spark/spark3/20230712_pyspark/" class="article-date">
  <time class="dt-published" datetime="2023-07-12T14:54:38.000Z" itemprop="datePublished">2023-07-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/12/bigdata/spark/spark3/20230712_pyspark/">20230712_pyspark</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->



<!-- tocstop -->

<p>Pandas</p>
<p>用于数据分析、数据清洗</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/12/bigdata/spark/spark3/20230712_pyspark/" data-id="cljzvov5q0000pv9agj6hfomf" data-title="20230712_pyspark" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230711_structStreaming" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/" class="article-date">
  <time class="dt-published" datetime="2023-07-11T08:52:34.000Z" itemprop="datePublished">2023-07-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/">20230711_structStreaming</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E">执行引擎</a><ul>
<li><a href="#example">Example</a><ul>
<li><a href="#structstreaming">StructStreaming</a></li>
<li><a href="#batchprocessing">BatchProcessing</a></li>
</ul>
</li>
<li><a href="#basics">Basics</a><ul>
<li><a href="#outputmode"><strong>outputmode</strong></a></li>
<li><a href="#event-time">Event time</a></li>
<li><a href="#time-window">Time Window</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#fault-tolerance-semantics">Fault Tolerance Semantics</a></li>
<li><a href="#source">Source</a><ul>
<li><a href="#socket">Socket</a></li>
<li><a href="#file">File</a></li>
<li><a href="#rate">Rate</a></li>
</ul>
</li>
<li><a href="#%E5%BB%B6%E6%97%B6%E4%B9%B1%E5%BA%8F%E6%95%B0%E6%8D%AE">延时乱序数据</a><ul>
<li><a href="#%E5%AE%9A%E4%B9%89">定义</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95">测试</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li>
</ul>
</li>
<li><a href="#%E7%8A%B6%E6%80%81%E5%AD%98%E5%82%A8">状态存储</a><ul>
<li><a href="#hdfs-state-store-provider">HDFS state store provider</a></li>
<li><a href="#rocksdb-state-store-implementation">RocksDB state store implementation</a></li>
</ul>
</li>
<li><a href="#sink">Sink</a></li>
<li><a href="#trigger">Trigger</a></li>
<li><a href="#checkpoint">checkpoint</a></li>
<li><a href="#kafka%E9%9B%86%E6%88%90">Kafka集成</a><ul>
<li><a href="#%E8%AF%BB%E5%8F%96kafka%E5%86%99%E5%85%A5console">读取kafka写入console</a></li>
<li><a href="#%E8%AF%BB%E5%8F%96kafka-%E6%99%AE%E9%80%9A%E8%81%9A%E5%90%88%E4%B8%8D%E5%90%AB%E7%AA%97%E5%8F%A3">读取kafka 普通聚合(不含窗口)</a></li>
<li><a href="#%E8%AF%BB%E5%8F%96kafka-%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%90%AB%E6%B0%B4%E5%8D%B0">读取kafka 窗口聚合(含水印)</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="执行引擎">执行引擎</span></h2><p>Spark SQL engine</p>
<p>两种处理模式：</p>
<p>默认 <em>micro-batch processing</em> engine,  最低延时 100ms，exactly-once fault-tolerance guarantees</p>
<p>可配置模式<strong>Continuous Processing</strong>，最低延时 1ms，at-least-once guarantees</p>
<p>Internally, by default, Structured Streaming queries are processed using a <em>micro-batch processing</em> engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called <strong>Continuous Processing</strong>, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset&#x2F;DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.</p>
<h3><span id="example">Example</span></h3><h4><span id="structstreaming">StructStreaming</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * start server: nc -lk 9999</span><br><span class="line"> * 可以看到这里 的api写法基本跟batch processing 批处理的api保持一致，</span><br><span class="line"> * 不同的地方是：</span><br><span class="line"> * (1) spark.readStream | wordCounts.writeStream，batch api的写法 spark.read | wordCounts.write</span><br><span class="line"> * (2) 流处理触发运行，调用方法wordCounts.writeStream.start()，而批处理调用方法 wordCounts.write.save()</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleStructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCount&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">    val lines = spark.readStream</span><br><span class="line">      .format(&quot;socket&quot;)</span><br><span class="line">      .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">      .option(&quot;port&quot;, 9999)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    // dataframe to dataset; then Split the lines into words</span><br><span class="line">    val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">    // Generate running word count</span><br><span class="line">    val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the running counts to the console</span><br><span class="line">    //console outputMode支持 update和complete, 有聚合计算的情况不支持append模式</span><br><span class="line">    val query = wordCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="batchprocessing">BatchProcessing</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetWordCountExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCount&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val lines = spark.read.format(&quot;text&quot;).load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/README.md&quot;)</span><br><span class="line"></span><br><span class="line">    // dataframe to dataset; then Split the lines into words</span><br><span class="line">    val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">    // Generate running word count</span><br><span class="line">    val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">    wordCounts.write.mode(&quot;overwrite&quot;).format(&quot;parquet&quot;).save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="basics">Basics</span></h3><h4><span id="outputmode"><strong>outputmode</strong></span></h4><p><em>Complete Mode</em> -   结果集 每次输出所有数据</p>
<p><em>Append Mode</em> -  结果集 每次只输出新增的数据（只包括新增，不包括变更的）</p>
<p><em>Update Mode</em> -  结果集 每次只输出更新的数据（包括新增 和 变更的  new|update）</p>
<h4><span id="event-time">Event time</span></h4><p>事件时间，只与事件本身有关</p>
<h4><span id="time-window">Time Window</span></h4><p>滚动窗口：Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals</p>
<p>滑动窗口：Sliding windows are similar to the tumbling windows from the point of being “fixed-sized”, but windows can overlap if the duration of slide is smaller than the duration of window, and in this case an input can be bound to the multiple windows.</p>
<p>会话窗口： 窗口在一定间隔内没有接收到新的数据，进行关闭，窗口大小不固定</p>
<p>a session window closes when there’s no input received within gap duration after receiving the latest input.</p>
<h2><span id="fault-tolerance-semantics">Fault Tolerance Semantics</span></h2><p>如何实现容错 和 exactly-once  端到端一致性保证：</p>
<p>Delivering end-to-end exactly-once semantics was one of key goals behind the design of Structured Streaming. To achieve that, we have designed the Structured Streaming sources, the sinks and the execution engine to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and&#x2F;or reprocessing. Every streaming source is assumed to have offsets (similar to Kafka offsets, or Kinesis sequence numbers) to track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure <strong>end-to-end exactly-once semantics</strong> under any failure.</p>
<p>Source:  replayable </p>
<p>Sink: idempotent</p>
<p>Engine: checkpointing and write-ahead logs </p>
<h2><span id="source">Source</span></h2><h3><span id="socket">Socket</span></h3><p>对应example</p>
<h3><span id="file">File</span></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Read all the csv files written atomically in a directory</span></span><br><span class="line"><span class="keyword">val</span> userSchema = <span class="keyword">new</span> <span class="type">StructType</span>().add(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;string&quot;</span>).add(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;integer&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> csvDF = spark</span><br><span class="line">  .readStream</span><br><span class="line">  .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">  .schema(userSchema)      <span class="comment">// Specify schema of the csv files</span></span><br><span class="line">  .csv(<span class="string">&quot;/path/to/directory&quot;</span>)    <span class="comment">// Equivalent to format(&quot;csv&quot;).load(&quot;/path/to/directory&quot;)</span></span><br></pre></td></tr></table></figure>



<h3><span id="rate">Rate</span></h3><p>Generates data at the specified number of rows per second, each output row contains a <code>timestamp</code> and <code>value</code>. Where <code>timestamp</code> is a <code>Timestamp</code> type containing the time of message dispatch, and <code>value</code> is of <code>Long</code> type containing the message count, starting from 0 as the first row. </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a streaming DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = spark.readStream</span><br><span class="line">  .format(<span class="string">&quot;rate&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;rowsPerSecond&quot;</span>, <span class="number">10</span>)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> query = df.writeStream</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/out/rate/&quot;</span>)</span><br><span class="line">  .outputMode(<span class="string">&quot;update&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;numRows&quot;</span>, <span class="number">20</span>)</span><br><span class="line">  .option(<span class="string">&quot;truncate&quot;</span>, <span class="literal">false</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"> query.awaitTermination()</span><br></pre></td></tr></table></figure>



<h2><span id="延时乱序数据">延时乱序数据</span></h2><h3><span id="定义">定义</span></h3><p><strong>Watermark</strong>: 水位线</p>
<p>Watemark &#x3D; the current max  event time - late threshold</p>
<p>基于事件时间产生，本质是一个时间戳，和时间窗口结合，用于处理延时晚到的乱序时间。随着事件的不断流入(到计算逻辑flink或spark代码)，水位线不断上涨，当水位线 &gt;&#x3D; 窗口结束时间，流入到这个窗口的新的延时数据将不再被该窗口计算处理。</p>
<p>水位线作用和定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">用来处理数据延迟，数据乱序。</span><br><span class="line"></span><br><span class="line">水印（更准确的说法是水位线，随着接收事件时间的不断增长而不断上涨）本质上是一个时间戳，代表事件时间比这个时间戳更小的数据已全部到达，（所有比这时间戳更早的数据视为延迟数据）当水印watermark&gt;=窗口对应的结束时间，对应窗口会触发计算并进行关闭</span><br></pre></td></tr></table></figure>



<p><strong>水印只与事件时间有关</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark 水印 和flink的水印 均是来自google的dataflow model,  其核心思想是在无边界、乱序的数据流中，在数据准确性、数据延迟性和计算性能开销之间 追求一个平衡。</span><br><span class="line">水位线本质是事件时间戳，仅仅只与 事件本身对应的时间属性event_time 和用户定义的允许数据延迟的时间（late threshold）有关，与系统本身的处理时间processing time 没有关系。 「之前一直理解不了，将processing time 和event time混淆到一块，误以为event time 和processing time时钟是同步的，其实两者是没有关系的，一个是事件本身决定的，一个是计算系统处理时间，我们通常讲窗口、水印是针对事件而言的，而现实中事件时间和处理时间可能中间间隔了好几个小时，水印本质解决的问题是有关在事件中多条数据乱序到达的问题，只能依赖与事件本身的时间去想办法解决，通过引入水位线的概念（由事件时间生成，随着数据的不断流入水位线不断上涨），当水位线&gt;=某个窗口结束时间（当然，这里窗口也是完全依据流入的数据事件时间划分的） 意味着在这之后流入到这个窗口的新的数据将不再被该窗口计算处理 」</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="测试">测试</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 测试水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> * 1、start server: nc -lk 9999</span><br><span class="line"> * 2、params: localhost 9999 10 (设置滚动窗口 10s间隔)</span><br><span class="line"> *</span><br><span class="line"> * 3、late threshold = 10s</span><br><span class="line"> */</span><br><span class="line">object ExampleWatermark_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if (args.length &lt; 3) &#123;</span><br><span class="line">      System.err.println(&quot;Usage: StructuredNetworkWordCountWindowed &lt;hostname&gt; &lt;port&gt;&quot; +</span><br><span class="line">        &quot; &lt;window duration in seconds&gt; [&lt;slide duration in seconds&gt;]&quot;)</span><br><span class="line">      System.exit(1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val host = args(0)</span><br><span class="line">    val port = args(1).toInt</span><br><span class="line">    val windowSize = args(2).toInt</span><br><span class="line">    val slideSize = if (args.length == 3) windowSize else args(3).toInt</span><br><span class="line">    if (slideSize &gt; windowSize) &#123;</span><br><span class="line">      System.err.println(&quot;&lt;slide duration&gt; must be less than or equal to &lt;window duration&gt;&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    val windowDuration = s&quot;$windowSize seconds&quot;</span><br><span class="line">    val slideDuration = s&quot;$slideSize seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;StructuredNetworkWordCountWindowed&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line">    val lines = spark.readStream</span><br><span class="line">      .format(&quot;socket&quot;)</span><br><span class="line">      .option(&quot;host&quot;, host)</span><br><span class="line">      .option(&quot;port&quot;, port)</span><br><span class="line">      //      .option(&quot;includeTimestamp&quot;, true)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    // Split the lines into words, retaining timestamps</span><br><span class="line">    //    val words = lines.as[(String, Timestamp)].flatMap(line =&gt;</span><br><span class="line">    //      line._1.split(&quot; &quot;).map(word =&gt; (word, line._2))</span><br><span class="line">    //    ).toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">    val query = windowedCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>注意</strong>：spark 水印只在append 模式和update模式下有作用，在complete模式下没有任何作用，所有数据都会输出。</p>
<p>在上述example中，</p>
<p>Terminal 开启端口本地测试 （nc -lk 9000 10），窗口为10s, 最大延迟时间也是10s,</p>
<p>依次每行输入数据，</p>
<p>spark,2020-09-04 10:02:41<br>spark,2020-09-04 10:02:42<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:48<br>spark,2020-09-04 10:02:21<br>spark,2020-09-04 10:02:31<br>spark,2020-09-04 10:02:32<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:51<br>spark,2020-09-04 10:02:59<br>spark,2020-09-04 10:02:53<br>spark,2020-09-04 10:03:06</p>
<p>spark,2020-09-04 10:02:45</p>
<p>spark,2020-09-04 10:03:07<br>spark,2020-09-04 10:02:53<br>spark,2020-09-04 10:03:08<br>spark,2020-09-04 10:03:13<br>spark,2020-09-04 10:03:13<br>spark,2020-09-04 10:03:33<br>spark,2020-09-04 10:03:36</p>
<p>前四条数据依次输入，</p>
<p>spark,2020-09-04 10:02:41<br>spark,2020-09-04 10:02:42<br>spark,2020-09-04 10:02:49<br>spark,2020-09-04 10:02:48</p>
<p>输出结果为：</p>
<p>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]|spark|4    |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（可以看到窗口是[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]，当时大概是11点30分测试的，这里划定的窗口完全是由事件时间决定的，与其他没有关系，这里水位线watermark=2020-09-04 10:02:49 - 10s = 2020-09-04 10:02:39， 意味着窗口结束时间T                                      2020-09-04 10:02:39 &gt;=T, 对应的流入该窗口内的新数据不再被处理，最接近水位线的窗口是                                                                     [2020-09-04 10:02:20.0,2020-09-04 10:02:30.0] ,  因此后面接着输入了一条位于该窗口内的新的数据，spark,2020-09-04 10:02:21，但是没有结果输出，因为水位线高过了窗口结束时间T = 2020-09-04 10:02:30.0）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:21，</p>
<p>没有结果输出</p>
<p>输入spark,2020-09-04 10:02:31，</p>
<p>输出结果为：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]|spark|1    |<br>+———————————————+—–+—–+</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 &gt;= 2020-09-04 10:02:40.0 不成立， 老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据继续被处理）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:32，</p>
<p>输出结果为：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]|spark|2    |<br>+———————————————+—–+—–+    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 &gt;= 2020-09-04 10:02:40.0 不成立，老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据继续被处理 ）</span><br></pre></td></tr></table></figure>

<p>输入spark,2020-09-04 10:02:49，</p>
<p>输出：</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]|spark|5    |    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:39 ）</span><br></pre></td></tr></table></figure>

<p>输入 </p>
<p>spark,2020-09-04 10:02:51<br>spark,2020-09-04 10:02:59<br>spark,2020-09-04 10:02:53</p>
<p>输出</p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:02:50.0,2020-09-04 10:03:00.0]|spark|3    |<br>+———————————————+—–+—–+       </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线上涨2020-09-04 10:02:49 &gt;= 2020-09-04 10:02:40.0 成立 ，有新窗口出现，老的窗口 [2020-09-04 10:02:30.0,2020-09-04 10:02:40.0]  流入的新数据不再处理）</span><br></pre></td></tr></table></figure>

<p>输入 spark,2020-09-04 10:03:06 </p>
<p>输出 </p>
<p>+———————————————+—–+—–+<br>|window                                       |word |count|<br>+———————————————+—–+—–+<br>|[2020-09-04 10:03:00.0,2020-09-04 10:03:10.0]|spark|1    |</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线上涨 2020-09-04 10:02:56 &gt;= 2020-09-04 10:02:50.0 成立 ，有新窗口出现，老的窗口 [2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]  流入的新数据不再处理）</span><br></pre></td></tr></table></figure>

<p>输入 spark,2020-09-04 10:02:45 </p>
<p>输出为，没有结果输出</p>
<p>+——+—-+—–+<br>|window|word|count|<br>+——+—-+—–+<br>+——+—-+—–+</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（水位线2020-09-04 10:02:56 保持不变，老的窗口 [2020-09-04 10:02:40.0,2020-09-04 10:02:50.0]  流入的新数据不再处理，因此流入 位于老窗口的 新数据spark,2020-09-04 10:02:45后，老窗口结果没更新）</span><br></pre></td></tr></table></figure>



<h3><span id="结论">结论</span></h3><p>Flink&#x2F;spark  用事件时间 + 窗口 + 水印来解决实际生产中的数据乱序问题，有如下的触发条件：</p>
<p>watermark 时间 &gt;&#x3D; window_end_time (后续到达的落入之前窗口的数据被丢弃掉)；</p>
<p>在 [window_start_time,window_end_time) 中有数据存在，这个窗口是左闭右开的。</p>
<p>此外，WaterMark 的生成是以对象的形式发送到下游，同样会消耗内存，因此水印的生成时间和频率都要进行严格控制，否则会影响我们的正常作业。</p>
<h2><span id="状态存储">状态存储</span></h2><h3><span id="hdfs-state-store-provider">HDFS state store provider</span></h3><p>The HDFS backend state store provider is the default implementation of [[StateStoreProvider]] and [[StateStore]] in which all the data is stored in memory map in the first stage, and then backed by files in an HDFS-compatible file system. 占用executor内存</p>
<h3><span id="rocksdb-state-store-implementation">RocksDB state store implementation</span></h3><p>(嵌入式key-value数据库，不需要额外安装）不消耗executor内存，适用于大状态存储</p>
<p>3.2 以及之后提供支持。</p>
<p>As of Spark 3.2, we add a new built-in state store implementation, RocksDB state store provider.</p>
<p>Rather than keeping the state in the JVM memory, this solution uses RocksDB to efficiently manage the state in the native memory and the local disk.</p>
<p>目前理解是：rocksdb存储状态，替代之前状态存储占用executor 内存，</p>
<p>checkpoint两者都需要配置，checkpoint是在任务故障恢复或重启时用到。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.config(&quot;spark.sql.streaming.stateStore.providerClass&quot;, &quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;)</span><br></pre></td></tr></table></figure>



<h2><span id="sink">Sink</span></h2><p>There are a few types of built-in output sinks.</p>
<p>官方目前只有file 支持exactly-one, kafka支持at-least once</p>
<ul>
<li><strong>File sink</strong> - Stores the output to a directory.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;parquet&quot;)        // can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span><br><span class="line">    .option(&quot;path&quot;, &quot;path/to/destination/dir&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;kafka&quot;)</span><br><span class="line">    .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)</span><br><span class="line">    .option(&quot;topic&quot;, &quot;updates&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. See later in the section for more details.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Console sink (for debugging)</strong> - Prints the output to the console&#x2F;stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;console&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(&quot;memory&quot;)</span><br><span class="line">    .queryName(&quot;tableName&quot;)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<p><strong>ForeachBatch</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: <span class="type">DataFrame</span>, batchId: <span class="type">Long</span>) =&gt;</span><br><span class="line">  batchDF.persist()</span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 1</span></span><br><span class="line">  batchDF.write.format(...).save(...)  <span class="comment">// location 2</span></span><br><span class="line">  batchDF.unpersist()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="trigger">Trigger</span></h2><p>触发器，多长时间执行一次</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Default trigger (runs micro-batch as soon as it can)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ProcessingTime trigger with two-seconds micro-batch interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">&quot;2 seconds&quot;</span>))</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// One-time trigger (Deprecated, encouraged to use Available-now trigger)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Once</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Available-now trigger</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">AvailableNow</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Continuous trigger with one-second checkpointing interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Continuous</span>(<span class="string">&quot;1 second&quot;</span>))</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>



<h2><span id="checkpoint">checkpoint</span></h2><p>任务失败恢复、重启  （Recovering from Failures with Checkpointing）</p>
<p>In case of a failure or intentional shutdown, you can recover the previous progress and state of a previous query, and continue where it left off. This is done using checkpointing and write-ahead logs. You can configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/structured-streaming-programming-guide.html#quick-example">quick example</a>) to the checkpoint location. This checkpoint location has to be a path in an HDFS compatible file system, and can be set as an option in the DataStreamWriter when <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/structured-streaming-programming-guide.html#starting-streaming-queries">starting a query</a>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;checkpointLocation&quot;</span>, <span class="string">&quot;path/to/HDFS/dir&quot;</span>)</span><br><span class="line">  .format(<span class="string">&quot;memory&quot;</span>)</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure>





<h2><span id="kafka集成">Kafka集成</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql-kafka-0-10_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka写入console">读取kafka写入console</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> * late threshold = 10s</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; console （watermark + window）</span><br><span class="line"> *</span><br><span class="line"> * 输入： spark,2020-09-04 10:02:49</span><br><span class="line"> * 输出：</span><br><span class="line"> * +------------------------------------------+-----+-----+</span><br><span class="line"> * |window                                    |word |count|</span><br><span class="line"> * +------------------------------------------+-----+-----+</span><br><span class="line"> * |&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line"> *</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">object ExampleKafka_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">    val query = windowedCounts.writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">      .format(&quot;console&quot;)</span><br><span class="line">      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka-普通聚合不含窗口">读取kafka 普通聚合(不含窗口)</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; kafka （no window）</span><br><span class="line"> *</span><br><span class="line"> * 常规聚合计算   group by -&gt; count</span><br><span class="line"> * 累加统计 wordcount</span><br><span class="line"> *</span><br><span class="line"> * 输入： hadoop,2020-09-04 10:02:49</span><br><span class="line"> * 输出： hadoop,1</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleKafka3_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      //配置RocksDBStateStore, 默认HdfsStateStore</span><br><span class="line">      .config(&quot;spark.sql.streaming.stateStore.providerClass&quot;, &quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .groupBy(</span><br><span class="line">          $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    *</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|window                                    |word |count|</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line">    *</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">//    val query = windowedCounts</span><br><span class="line">//      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">//      .writeStream</span><br><span class="line">//      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">//      .format(&quot;console&quot;)</span><br><span class="line">//      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">//      .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Write key-value data from a DataFrame to a specific Kafka topic specified in an option</span><br><span class="line">    val query = windowedCounts</span><br><span class="line">      .selectExpr(&quot;concat_ws(&#x27;,&#x27;, word, cast(count as string)) as value&quot;)</span><br><span class="line">      .writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .option(&quot;checkpointLocation&quot;, &quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1/out1&quot;)</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;,  &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;topic&quot;, &quot;testzq2&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3><span id="读取kafka-窗口聚合含水印">读取kafka 窗口聚合(含水印)</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 水位线 watermark =  the current max  event time - late threshold</span><br><span class="line"> *</span><br><span class="line"> * late threshold = 10s</span><br><span class="line"> *</span><br><span class="line"> * kafka -&gt; kafka （watermark + window）</span><br><span class="line"> *</span><br><span class="line"> * 输入： spark,2020-09-04 10:02:49</span><br><span class="line"> * 输出： &#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;,spark,1</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">object ExampleKafka2_StructStream &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val windowDuration = &quot;10 seconds&quot;</span><br><span class="line">    val slideDuration = &quot;10 seconds&quot;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder</span><br><span class="line">      .appName(&quot;ExampleKafka_StructStream&quot;)</span><br><span class="line">      .master(&quot;local[2]&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.setLogLevel(&quot;warn&quot;)</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    // Create DataFrame representing the stream of input lines from connection to host:port</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .readStream</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;subscribe&quot;, &quot;testzq1&quot;)</span><br><span class="line">      .option(&quot;includeHeaders&quot;, &quot;true&quot;)</span><br><span class="line">      .load()</span><br><span class="line">    //df.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;, &quot;headers&quot;)</span><br><span class="line">    val lines = df.selectExpr(&quot;CAST(value AS STRING)&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val words = lines</span><br><span class="line">      .as[String]</span><br><span class="line">      .map(x =&gt; &#123;</span><br><span class="line">        val cont =x.split(&quot;,&quot;)</span><br><span class="line">        (cont(0), cont(1))</span><br><span class="line">      &#125; )</span><br><span class="line">      .toDF(&quot;word&quot;, &quot;timestamp&quot;)</span><br><span class="line">      .selectExpr(&quot;CAST(word AS STRING)&quot;, &quot;CAST(timestamp AS TIMESTAMP)&quot; )</span><br><span class="line">      .as[(String,Timestamp)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Group the data by window and word and compute the count of each group</span><br><span class="line">    val windowedCounts =</span><br><span class="line">      words</span><br><span class="line">        .withWatermark(&quot;timestamp&quot;, &quot;10 seconds&quot;)</span><br><span class="line">        .groupBy(</span><br><span class="line">          window($&quot;timestamp&quot;, windowDuration, slideDuration), $&quot;word&quot;</span><br><span class="line">        ).count()</span><br><span class="line">    //.orderBy(&quot;window&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    *</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|window                                    |word |count|</span><br><span class="line">+------------------------------------------+-----+-----+</span><br><span class="line">|&#123;2020-09-04 10:02:40, 2020-09-04 10:02:50&#125;|spark|1    |</span><br><span class="line">    *</span><br><span class="line">    * */</span><br><span class="line"></span><br><span class="line">    // Start running the query that prints the windowed word counts to the console</span><br><span class="line">//    val query = windowedCounts</span><br><span class="line">//      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">//      .writeStream</span><br><span class="line">//      .outputMode(&quot;update&quot;)  //if you want to use watermark, you must choose append or update mode</span><br><span class="line">//      .format(&quot;console&quot;)</span><br><span class="line">//      .option(&quot;truncate&quot;, &quot;false&quot;)</span><br><span class="line">//      .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // Write key-value data from a DataFrame to a specific Kafka topic specified in an option</span><br><span class="line">    val query = windowedCounts</span><br><span class="line">      .selectExpr(&quot;cast(window as string) as key&quot;, &quot;concat_ws(&#x27;,&#x27;, cast(window as string), word, cast(count as string)) as value&quot;)</span><br><span class="line">      .writeStream</span><br><span class="line">      .outputMode(&quot;update&quot;)</span><br><span class="line">      .option(&quot;checkpointLocation&quot;, &quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/tmp1/out&quot;)</span><br><span class="line">      .format(&quot;kafka&quot;)</span><br><span class="line">      .option(&quot;kafka.bootstrap.servers&quot;,  &quot;localhost:9092&quot;)</span><br><span class="line">      .option(&quot;topic&quot;, &quot;testzq2&quot;)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/11/bigdata/spark/spark3/20230711_structStreaming/" data-id="cljy55ft700000n9a3c2vb7n4" data-title="20230711_structStreaming" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230707_sparksql" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/09/bigdata/spark/spark3/20230707_sparksql/" class="article-date">
  <time class="dt-published" datetime="2023-07-09T14:59:37.000Z" itemprop="datePublished">2023-07-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/09/bigdata/spark/spark3/20230707_sparksql/">20230707_sparksql</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#table-cache">Table Cache</a></li>
<li><a href="#config">Config</a></li>
<li><a href="#join-strategy-hint">Join Strategy Hint</a></li>
<li><a href="#coalesce-hints">Coalesce hints</a></li>
<li><a href="#operator">Operator</a></li>
<li><a href="#aqe">AQE</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C">数据倾斜</a></li>
<li><a href="#ddl">DDL</a><ul>
<li><a href="#cte">CTE</a></li>
<li><a href="#analyze">Analyze</a></li>
<li><a href="#cache-table">Cache table</a></li>
<li><a href="#refresh-table">REFRESH TABLE</a></li>
<li><a href="#msck-repair">msck repair</a></li>
<li><a href="#distribute-by">distribute by</a></li>
<li><a href="#cluster-by">cluster by</a></li>
<li><a href="#explain">explain</a></li>
<li><a href="#inline">Inline</a></li>
<li><a href="#file">File</a></li>
<li><a href="#join">Join</a><ul>
<li><a href="#semi-join"><strong>Semi Join</strong></a></li>
<li><a href="#anti-join"><strong>Anti Join</strong></a></li>
</ul>
</li>
<li><a href="#limit">limit</a></li>
<li><a href="#%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C">集合操作</a></li>
<li><a href="#tablesample">TABLESAMPLE</a></li>
<li><a href="#tvf">TVF</a></li>
<li><a href="#%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0">聚合函数</a><ul>
<li><a href="#ordered-set-aggregate-functions">Ordered-Set Aggregate Functions</a></li>
</ul>
</li>
<li><a href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0">窗口函数</a></li>
<li><a href="#pivot">PIVOT</a></li>
<li><a href="#unpivot">UNPIVOT</a></li>
<li><a href="#lateral">Lateral</a></li>
</ul>
</li>
<li><a href="#dml">DML</a><ul>
<li><a href="#insert-into">Insert into</a></li>
<li><a href="#insert-overwrite-directory">Insert overwrite directory</a><ul>
<li><a href="#spark-format">Spark format</a></li>
<li><a href="#hive-format">Hive format</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="table-cache">Table Cache</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.catalog.cacheTable(&quot;tableName&quot;)</span><br><span class="line">spark.catalog.uncacheTable(&quot;tableName&quot;)</span><br><span class="line"></span><br><span class="line">dataFrame.cache()</span><br><span class="line">dataFrame.unpersist()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="config">Config</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.autoBroadcastJoinThreshold  	10485760 (10m)</span><br><span class="line">Configures the maximum size in bytes for a table that will be broadcast</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="join-strategy-hint">Join Strategy Hint</span></h2><p>The join strategy hints, namely <code>BROADCAST</code>, <code>MERGE</code>, <code>SHUFFLE_HASH</code> and <code>SHUFFLE_REPLICATE_NL</code>, instruct Spark to use the hinted strategy.</p>
<p><code>BROADCAST</code> 细分为两种：broadcast hash join or broadcast nested loop join </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.table(<span class="string">&quot;src&quot;</span>).join(spark.table(<span class="string">&quot;records&quot;</span>).hint(<span class="string">&quot;broadcast&quot;</span>), <span class="string">&quot;key&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">-- <span class="type">We</span> accept <span class="type">BROADCAST</span>, <span class="type">BROADCASTJOIN</span> and <span class="type">MAPJOIN</span> <span class="keyword">for</span> broadcast hint</span><br><span class="line"><span class="type">SELECT</span> <span class="comment">/*+ BROADCAST(r) */</span> * <span class="type">FROM</span> records r <span class="type">JOIN</span> src s <span class="type">ON</span> r.key = s.key</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2><span id="coalesce-hints">Coalesce hints</span></h2><p>Coalesce hints allow Spark SQL users to control the number of output files just like <code>coalesce</code>, <code>repartition</code> and <code>repartitionByRange</code> in the Dataset API, they can be used for performance tuning and reducing the number of output files.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SELECT /*+ COALESCE(3) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION(3) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION(c) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION(3, c) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t;</span><br><span class="line">SELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE(3) */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE(c) */ * FROM t;</span><br><span class="line">SELECT /*+ REBALANCE(3, c) */ * FROM t;</span><br></pre></td></tr></table></figure>



<h2><span id="operator">Operator</span></h2><p>A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations.</p>
<p>支持 函数型算子 和  关系型算子 </p>
<p>Spark SQL 执行引擎：catalyst优化器和</p>
<p>tungsten优化(数据结构设计(基于unsaferow存储、内存页管理)和全阶段代码生成)</p>
<h2><span id="aqe">AQE</span></h2><p>AQE 是 Spark SQL 的一种动态优化机制，在运行时，每当 Shuffle Map 阶段执行完毕，AQE 都会结合这个阶段的统计信息，基于既定的规则动态地调整、修正尚未执行的逻辑计划和物理计划，来完成对原始查询语句的运行时优化。</p>
<p>Spark 3.2.0 以及之后的版本默认是开启状态。</p>
<p>Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0.  </p>
<p>Spark SQL can turn on and off AQE by <code>spark.sql.adaptive.enabled</code> as an umbrella configuration。</p>
<p>As of Spark 3.0, there are three major features in AQE: including coalescing post-shuffle partitions, converting sort-merge join to broadcast join, and skew join optimization.</p>
<p>（AQE 赖以优化的统计信息与 CBO 不同，这些统计信息并不是关于某张表或是哪个列，而是 Shuffle Map 阶段输出的中间文件。）</p>
<p>自动分区合并(拆分) : reduce  合并数据量小的分区，减少小文件数量</p>
<p>Join 策略调整：sort-merge join to broadcast join</p>
<p>Join倾斜优化：倾斜分区会被拆分成多个分区 (倾斜分区拆分为多个数据分区、对另外一张表对应的数据分区进行复制)</p>
<h2><span id="数据倾斜">数据倾斜</span></h2><p>单表引起的倾斜：</p>
<p>1、map端聚合</p>
<p>2、单次聚合改为分两次聚合完成 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct uid) from t1 group by day</span><br><span class="line">select count(1) from ( select day, uid from t1 group by day, uid ) tmp group by day</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>多表join引起的倾斜：</p>
<p>1、调整 spark.sql.autoBroadcastJoinThreshold,  尽量使用broadcastjoin</p>
<p>2、<code>spark.sql.adaptive.enabled</code>  开启AQE，实现Join倾斜自动优化，可能出现executor粒度的数据倾斜(倾斜分区划分的任务重复分配到一个executor)</p>
<p>3、使用两阶段shuffle，将倾斜key和非倾斜key分开处理，最后结果union。</p>
<p>对于非倾斜key，常规处理；</p>
<p>对于倾斜key，外表对应的key：加盐、添加随机后缀，内表对应的key：添加后缀、数据进行复制，内外表进行一阶段 join 聚合，第二阶段 将第一阶段聚合结果 的joinkey 去除后缀后，进行二次聚合。</p>
<h2><span id="ddl">DDL</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">--Create bucketed table through CTAS and CTE</span><br><span class="line">CREATE TABLE student_bucket</span><br><span class="line">    USING parquet</span><br><span class="line">    CLUSTERED BY (id) INTO 4 buckets (</span><br><span class="line">    WITH tmpTable AS (</span><br><span class="line">        SELECT * FROM student WHERE id &gt; 100</span><br><span class="line">    )</span><br><span class="line">    SELECT * FROM tmpTable</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">CREATE TABLE student_parquet(id INT, name STRING, age INT) USING PARQUET</span><br><span class="line">    OPTIONS (</span><br><span class="line">      &#x27;parquet.bloom.filter.enabled&#x27;=&#x27;true&#x27;,</span><br><span class="line">      &#x27;parquet.bloom.filter.enabled#age&#x27;=&#x27;false&#x27;</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line"># 添加分区    </span><br><span class="line">ALTER TABLE table_identifier ADD [IF NOT EXISTS] </span><br><span class="line">    ( partition_spec [ partition_spec ... ] )</span><br><span class="line"></span><br><span class="line"># 刷新分区</span><br><span class="line">[MSCK] REPAIR TABLE table_identifier [&#123;ADD|DROP|SYNC&#125; PARTITIONS]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="cte">CTE</span></h3><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-cte.html">https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-cte.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CTE</span><br><span class="line"># Common Table Expression (CTE)</span><br><span class="line">A common table expression (CTE) defines a temporary result set that a user can reference possibly multiple times within the scope of a SQL statement. </span><br><span class="line">表示一个临时结果集，用户可以在sql语句中引用多次</span><br><span class="line">-- CTE with multiple column aliases</span><br><span class="line">WITH t(x, y) AS (SELECT 1, 2)</span><br><span class="line">SELECT * FROM t WHERE x = 1 AND y = 2;</span><br></pre></td></tr></table></figure>



<h3><span id="analyze">Analyze</span></h3><p>获取表的统计信息，包括表的大小、行数，也可以针对字段进行统计</p>
<p>The <code>ANALYZE TABLE</code> statement collects statistics about one specific table or all the tables in one specified database, that are to be used by the query optimizer to find a better query execution plan.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">analyze table emp compute statistics</span><br><span class="line"># noscan 使用noscan仅获取表大小, 不需要扫描整个表</span><br><span class="line"># Collects only the table’s size in bytes (which does not require scanning the entire table).</span><br><span class="line">analyze table emp compute statistics noscan;</span><br><span class="line">desc extended emp;</span><br></pre></td></tr></table></figure>



<h3><span id="cache-table">Cache table</span></h3><p><code>CACHE TABLE</code> statement caches contents of a table or output of a query with the given storage level. If a query is cached, then a temp view will be created for this query. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CACHE TABLE emp;</span><br><span class="line"></span><br><span class="line">CACHE TABLE emp OPTIONS (&#x27;storageLevel&#x27; &#x27;DISK_ONLY&#x27;) SELECT salary FROM emp</span><br><span class="line"></span><br><span class="line">CLEAR CACHE;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="refresh-table">REFRESH TABLE</span></h3><p>（Invalidates and refreshes all the cached data and metadata of the given table. For performance reasons, Spark SQL or the external data source library it uses might cache certain metadata about a table, such as the location of blocks (数据块的位置信息). When those change outside of Spark SQL, users should call this function to invalidate the cache.）</p>
<p>更新表的缓存信息，包括元数据等。主要用于表的数据修改、表中元数据未修改的场景。(无法更新分区，msck是更新分区信息)</p>
<p>Spark为了提高性能会缓存Parquet的元数据信息。当更新了Parquet表数据时，缓存的元数据信息未更新，导致Spark SQL查询不到新插入的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- The cached entries of the table will be refreshed  </span><br><span class="line">-- The table is resolved from the current database as the table name is unqualified.</span><br><span class="line"># 这个表的缓存信息 会更新</span><br><span class="line">REFRESH TABLE tbl1;</span><br><span class="line"></span><br><span class="line">##测试的情况，parquet表的数据目录新增数据后(手动增加)，spark sql查询不到新的数据，执行refresh，</span><br><span class="line">spark sql可以查询到新的数据</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>和msck区别：</p>
<p>msck repair table </p>
<p>作用是检查HDFS目录下存在（不存在）但表的metastore中不存在（存在）的元数据信息，更新metastore到hive。</p>
<p>每次执行msck repair这个命令，都会检查所有分区的目录是否在hive元数据中存在，如果是每次新增一个分区的任务（daily的),那么使用这个语句将会越来越耗费时间，建议使用ALTER TABLE ADD PARTITION 命令。MSCK适合一次导入很多分区，需要将这些分区都更新到元数据信息中。</p>
<h3><span id="msck-repair">msck repair</span></h3><p>只能作用与分区表，否则提示信息(because it is not a partitioned table.)</p>
<p>更新分区元数据信息到hive ,  （适用于分区数据已更新，但是分区元数据未更新的情况）</p>
<p><code>REPAIR TABLE</code> recovers all the partitions in the directory of a table and updates the Hive metastore. When creating a table using <code>PARTITIONED BY</code> clause, partitions are generated and registered in the Hive metastore. However, if the partitioned table is created from existing data, partitions are not registered automatically in the Hive metastore. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">-- create a partitioned table from existing data /tmp/namesAndAges.parquet</span><br><span class="line">CREATE TABLE tt1 (employee_name STRING, salary INT, department string) USING parquet PARTITIONED BY (department) location &#x27;/tmp/test/spark_ext&#x27;;</span><br><span class="line"></span><br><span class="line">-- SELECT * FROM tt1 does not return results</span><br><span class="line">SELECT * FROM tt1;</span><br><span class="line"></span><br><span class="line">refresh table tt1;</span><br><span class="line">-- SELECT * FROM tt1 does not return results</span><br><span class="line">SELECT * FROM tt1;</span><br><span class="line"></span><br><span class="line">-- run REPAIR TABLE to recovers all the partitions</span><br><span class="line">msck REPAIR TABLE tt1;</span><br><span class="line"></span><br><span class="line">-- SELECT * FROM tt1  returns results</span><br><span class="line">SELECT * FROM tt1;</span><br><span class="line">James	3000	Sales</span><br><span class="line">Robert	4100	Sales</span><br><span class="line">James	3000	Sales</span><br><span class="line">Michael	4600	Sales</span><br><span class="line">Maria	3000	Finance</span><br><span class="line">Kumar	2000	Marketing</span><br><span class="line">Scott	3300	Finance</span><br><span class="line">Jen	3900	Finance</span><br><span class="line">Jeff	3000	Marketing</span><br><span class="line">Saif	4100	Sales</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="distribute-by">distribute by</span></h3><p>数据重分区</p>
<h3><span id="cluster-by">cluster by</span></h3><p>Distribute  by A sort by A</p>
<p>数据重分区 + 分区内排序</p>
<h3><span id="explain">explain</span></h3><p>explain  extended  select * from emp</p>
<h3><span id="inline">Inline</span></h3><p>values 创建临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- three rows with a table alias</span><br><span class="line">SELECT * FROM VALUES (&quot;one&quot;, 1), (&quot;two&quot;, 2), (&quot;three&quot;, null) AS data(a, b);</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="file">File</span></h3><p>sql查询文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM parquet.`examples/src/main/resources/users.parquet`;</span><br><span class="line"></span><br><span class="line">SELECT * FROM orc.`examples/src/main/resources/users.orc`;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="join">Join</span></h3><ul>
<li><p><strong>join_type</strong></p>
<p>Specifies the join type.</p>
<p><strong>Syntax:</strong></p>
<p><code>[ INNER ] | CROSS | LEFT [ OUTER ] | [ LEFT ] SEMI | RIGHT [ OUTER ] | FULL [ OUTER ] | [ LEFT ] ANTI</code></p>
</li>
</ul>
<h4><span id="semi-join"><strong>Semi Join</strong></span></h4><p>A semi join returns values from the left side of the relation that has a match with the right. It is also referred to as a left semi join.</p>
<p>只返回左表中的值，这些值在右表存在</p>
<p><strong>Syntax:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">relation [ LEFT ] SEMI JOIN relation [ join_criteria ]</span><br></pre></td></tr></table></figure>

<h4><span id="anti-join"><strong>Anti Join</strong></span></h4><p>An anti join returns values from the left relation that has no match with the right. It is also referred to as a left anti join.</p>
<p>只返回左表中的值，这些值在右表不存在</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">relation [ LEFT ] ANTI JOIN relation [ join_criteria ]</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">-- Use employee and department tables to demonstrate different type of joins.</span><br><span class="line">SELECT * FROM employee;</span><br><span class="line">+---+-----+------+</span><br><span class="line">| id| name|deptno|</span><br><span class="line">+---+-----+------+</span><br><span class="line">|105|Chloe|     5|</span><br><span class="line">|103| Paul|     3|</span><br><span class="line">|101| John|     1|</span><br><span class="line">|102| Lisa|     2|</span><br><span class="line">|104| Evan|     4|</span><br><span class="line">|106|  Amy|     6|</span><br><span class="line">+---+-----+------+</span><br><span class="line"></span><br><span class="line">SELECT * FROM department;</span><br><span class="line">+------+-----------+</span><br><span class="line">|deptno|   deptname|</span><br><span class="line">+------+-----------+</span><br><span class="line">|     3|Engineering|</span><br><span class="line">|     2|      Sales|</span><br><span class="line">|     1|  Marketing|</span><br><span class="line">+------+-----------+</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate inner join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee INNER JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate left join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee LEFT JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|105|Chloe|     5|       NULL|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">|104| Evan|     4|       NULL|</span><br><span class="line">|106|  Amy|     6|       NULL|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate right join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee RIGHT JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate full join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname</span><br><span class="line">    FROM employee FULL JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|106|  Amy|     6|       NULL|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|105|Chloe|     5|       NULL|</span><br><span class="line">|104| Evan|     4|       NULL|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate cross join.</span><br><span class="line">SELECT id, name, employee.deptno, deptname FROM employee CROSS JOIN department;</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">| id| name|deptno|   deptname|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line">|105|Chloe|     5|Engineering|</span><br><span class="line">|105|Chloe|     5|  Marketing|</span><br><span class="line">|105|Chloe|     5|      Sales|</span><br><span class="line">|103| Paul|     3|Engineering|</span><br><span class="line">|103| Paul|     3|  Marketing|</span><br><span class="line">|103| Paul|     3|      Sales|</span><br><span class="line">|101| John|     1|Engineering|</span><br><span class="line">|101| John|     1|  Marketing|</span><br><span class="line">|101| John|     1|      Sales|</span><br><span class="line">|102| Lisa|     2|Engineering|</span><br><span class="line">|102| Lisa|     2|  Marketing|</span><br><span class="line">|102| Lisa|     2|      Sales|</span><br><span class="line">|104| Evan|     4|Engineering|</span><br><span class="line">|104| Evan|     4|  Marketing|</span><br><span class="line">|104| Evan|     4|      Sales|</span><br><span class="line">|106|  Amy|     4|Engineering|</span><br><span class="line">|106|  Amy|     4|  Marketing|</span><br><span class="line">|106|  Amy|     4|      Sales|</span><br><span class="line">+---+-----+------+-----------|</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate semi join.</span><br><span class="line">SELECT * FROM employee SEMI JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+</span><br><span class="line">| id| name|deptno|</span><br><span class="line">+---+-----+------+</span><br><span class="line">|103| Paul|     3|</span><br><span class="line">|101| John|     1|</span><br><span class="line">|102| Lisa|     2|</span><br><span class="line">+---+-----+------+</span><br><span class="line"></span><br><span class="line">-- Use employee and department tables to demonstrate anti join.</span><br><span class="line">SELECT * FROM employee ANTI JOIN department ON employee.deptno = department.deptno;</span><br><span class="line">+---+-----+------+</span><br><span class="line">| id| name|deptno|</span><br><span class="line">+---+-----+------+</span><br><span class="line">|105|Chloe|     5|</span><br><span class="line">|104| Evan|     4|</span><br><span class="line">|106|  Amy|     6|</span><br><span class="line">+---+-----+------+</span><br></pre></td></tr></table></figure>

<h3><span id="limit">limit</span></h3><p>The <code>OFFSET</code> clause is used to specify the number of rows to skip before beginning to return rows returned by the <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/sql-ref-syntax-qry-select.html">SELECT</a> statement. In general, this clause is used in conjunction with <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.4.1/sql-ref-syntax-qry-select-orderby.html">ORDER BY</a> to ensure that the results are deterministic.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-- Skip the first two rows.</span><br><span class="line">SELECT name, age FROM person ORDER BY name OFFSET 2;</span><br><span class="line">+-------+---+</span><br><span class="line">|   name|age|</span><br><span class="line">+-------+---+</span><br><span class="line">| John A| 18|</span><br><span class="line">| Mike A| 25|</span><br><span class="line">|Shone S| 16|</span><br><span class="line">|Zen Hui| 25|</span><br></pre></td></tr></table></figure>



<h3><span id="集合操作">集合操作</span></h3><ul>
<li><code>EXCEPT</code> or <code>MINUS</code></li>
<li><code>INTERSECT</code></li>
<li><code>UNION</code></li>
</ul>
<p>union  | union all    前者结果集去重，后者不去重</p>
<h3><span id="tablesample">TABLESAMPLE</span></h3><p>表数据采样</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM test;</span><br><span class="line">+--+----+</span><br><span class="line">|id|name|</span><br><span class="line">+--+----+</span><br><span class="line">| 5|Alex|</span><br><span class="line">| 8|Lucy|</span><br><span class="line">| 2|Mary|</span><br><span class="line">| 4|Fred|</span><br><span class="line">| 1|Lisa|</span><br><span class="line">| 9|Eric|</span><br><span class="line">|10|Adam|</span><br><span class="line">| 6|Mark|</span><br><span class="line">| 7|Lily|</span><br><span class="line">| 3|Evan|</span><br><span class="line">+--+----+</span><br><span class="line"></span><br><span class="line">SELECT * FROM test TABLESAMPLE (50 PERCENT);</span><br><span class="line">+--+----+</span><br><span class="line">|id|name|</span><br><span class="line">+--+----+</span><br><span class="line">| 5|Alex|</span><br><span class="line">| 2|Mary|</span><br><span class="line">| 4|Fred|</span><br><span class="line">| 9|Eric|</span><br><span class="line">|10|Adam|</span><br><span class="line">| 3|Evan|</span><br><span class="line">+--+----+</span><br><span class="line"></span><br><span class="line">SELECT * FROM test TABLESAMPLE (5 ROWS);</span><br></pre></td></tr></table></figure>



<h3><span id="tvf">TVF</span></h3><p>表值函数</p>
<p>返回多行数据</p>
<p>A table-valued function (TVF) is a function that returns a relation or a set of rows. There are two types of TVFs in Spark SQL:</p>
<ol>
<li>a TVF that can be specified in a FROM clause, e.g. range;</li>
<li>a TVF that can be specified in SELECT&#x2F;LATERAL VIEW clauses, e.g. explode.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">-- range call with end</span><br><span class="line">SELECT * FROM range(6 + cos(3));</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  0|</span><br><span class="line">|  1|</span><br><span class="line">|  2|</span><br><span class="line">|  3|</span><br><span class="line">|  4|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">-- range call with start and end</span><br><span class="line">SELECT * FROM range(5, 10);</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  5|</span><br><span class="line">|  6|</span><br><span class="line">|  7|</span><br><span class="line">|  8|</span><br><span class="line">|  9|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">-- range call with numPartitions</span><br><span class="line">SELECT * FROM range(0, 10, 2, 200);</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  0|</span><br><span class="line">|  2|</span><br><span class="line">|  4|</span><br><span class="line">|  6|</span><br><span class="line">|  8|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">-- range call with a table alias</span><br><span class="line">SELECT * FROM range(5, 8) AS test;</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  5|</span><br><span class="line">|  6|</span><br><span class="line">|  7|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">SELECT explode(array(10, 20));</span><br><span class="line">+---+</span><br><span class="line">|col|</span><br><span class="line">+---+</span><br><span class="line">| 10|</span><br><span class="line">| 20|</span><br><span class="line">+---+</span><br><span class="line"></span><br><span class="line">SELECT inline(array(struct(1, &#x27;a&#x27;), struct(2, &#x27;b&#x27;)));</span><br><span class="line">+----+----+</span><br><span class="line">|col1|col2|</span><br><span class="line">+----+----+</span><br><span class="line">|   1|   a|</span><br><span class="line">|   2|   b|</span><br><span class="line">+----+----+</span><br><span class="line"></span><br><span class="line">SELECT posexplode(array(10,20));</span><br><span class="line">+---+---+</span><br><span class="line">|pos|col|</span><br><span class="line">+---+---+</span><br><span class="line">|  0| 10|</span><br><span class="line">|  1| 20|</span><br><span class="line">+---+---+</span><br><span class="line"></span><br><span class="line">SELECT stack(2, 1, 2, 3);</span><br><span class="line">+----+----+</span><br><span class="line">|col0|col1|</span><br><span class="line">+----+----+</span><br><span class="line">|   1|   2|</span><br><span class="line">|   3|null|</span><br><span class="line">+----+----+</span><br><span class="line"></span><br><span class="line">SELECT json_tuple(&#x27;&#123;&quot;a&quot;:1, &quot;b&quot;:2&#125;&#x27;, &#x27;a&#x27;, &#x27;b&#x27;);</span><br><span class="line">+---+---+</span><br><span class="line">| c0| c1|</span><br><span class="line">+---+---+</span><br><span class="line">|  1|  2|</span><br><span class="line">+---+---+</span><br><span class="line"></span><br><span class="line">SELECT parse_url(&#x27;http://spark.apache.org/path?query=1&#x27;, &#x27;HOST&#x27;);</span><br><span class="line">+-----------------------------------------------------+</span><br><span class="line">|parse_url(http://spark.apache.org/path?query=1, HOST)|</span><br><span class="line">+-----------------------------------------------------+</span><br><span class="line">|                                     spark.apache.org|</span><br><span class="line">+-----------------------------------------------------+</span><br><span class="line"></span><br><span class="line">-- Use explode in a LATERAL VIEW clause</span><br><span class="line">CREATE TABLE test (c1 INT);</span><br><span class="line">INSERT INTO test VALUES (1);</span><br><span class="line">INSERT INTO test VALUES (2);</span><br><span class="line">SELECT * FROM test LATERAL VIEW explode (ARRAY(3,4)) AS c2;</span><br><span class="line">+--+--+</span><br><span class="line">|c1|c2|</span><br><span class="line">+--+--+</span><br><span class="line">| 1| 3|</span><br><span class="line">| 1| 4|</span><br><span class="line">| 2| 3|</span><br><span class="line">| 2| 4|</span><br><span class="line">+--+--+</span><br></pre></td></tr></table></figure>



<h3><span id="聚合函数">聚合函数</span></h3><h4><span id="ordered-set-aggregate-functions">Ordered-Set Aggregate Functions</span></h4><p>These aggregate Functions use different syntax than the other aggregate functions so that to specify an expression (typically a column name) by which to order the values.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">CREATE OR REPLACE TEMPORARY VIEW basic_pays AS SELECT * FROM VALUES</span><br><span class="line">(&#x27;Diane Murphy&#x27;,&#x27;Accounting&#x27;,8435),</span><br><span class="line">(&#x27;Mary Patterson&#x27;,&#x27;Accounting&#x27;,9998),</span><br><span class="line">(&#x27;Jeff Firrelli&#x27;,&#x27;Accounting&#x27;,8992),</span><br><span class="line">(&#x27;William Patterson&#x27;,&#x27;Accounting&#x27;,8870),</span><br><span class="line">(&#x27;Gerard Bondur&#x27;,&#x27;Accounting&#x27;,11472),</span><br><span class="line">(&#x27;Anthony Bow&#x27;,&#x27;Accounting&#x27;,6627),</span><br><span class="line">(&#x27;Leslie Jennings&#x27;,&#x27;IT&#x27;,8113),</span><br><span class="line">(&#x27;Leslie Thompson&#x27;,&#x27;IT&#x27;,5186),</span><br><span class="line">(&#x27;Julie Firrelli&#x27;,&#x27;Sales&#x27;,9181),</span><br><span class="line">(&#x27;Steve Patterson&#x27;,&#x27;Sales&#x27;,9441),</span><br><span class="line">(&#x27;Foon Yue Tseng&#x27;,&#x27;Sales&#x27;,6660),</span><br><span class="line">(&#x27;George Vanauf&#x27;,&#x27;Sales&#x27;,10563),</span><br><span class="line">(&#x27;Loui Bondur&#x27;,&#x27;SCM&#x27;,10449),</span><br><span class="line">(&#x27;Gerard Hernandez&#x27;,&#x27;SCM&#x27;,6949),</span><br><span class="line">(&#x27;Pamela Castillo&#x27;,&#x27;SCM&#x27;,11303),</span><br><span class="line">(&#x27;Larry Bott&#x27;,&#x27;SCM&#x27;,11798),</span><br><span class="line">(&#x27;Barry Jones&#x27;,&#x27;SCM&#x27;,10586)</span><br><span class="line">AS basic_pays(employee_name, department, salary);</span><br><span class="line"></span><br><span class="line">SELECT * FROM basic_pays;</span><br><span class="line">+-----------------+----------+------+</span><br><span class="line">|    employee_name|department|salary|</span><br><span class="line">+-----------------+----------+------+</span><br><span class="line">|      Anthony Bow|Accounting|	6627|</span><br><span class="line">|      Barry Jones|       SCM| 10586|</span><br><span class="line">|     Diane Murphy|Accounting|	8435|</span><br><span class="line">|   Foon Yue Tseng|     Sales|	6660|</span><br><span class="line">|    George Vanauf|     Sales| 10563|</span><br><span class="line">|    Gerard Bondur|Accounting| 11472|</span><br><span class="line">| Gerard Hernandez|       SCM|	6949|</span><br><span class="line">|    Jeff Firrelli|Accounting|	8992|</span><br><span class="line">|   Julie Firrelli|     Sales|	9181|</span><br><span class="line">|       Larry Bott|       SCM| 11798|</span><br><span class="line">|  Leslie Jennings|        IT|	8113|</span><br><span class="line">|  Leslie Thompson|        IT|	5186|</span><br><span class="line">|      Loui Bondur|       SCM| 10449|</span><br><span class="line">|   Mary Patterson|Accounting|	9998|</span><br><span class="line">|  Pamela Castillo|       SCM| 11303|</span><br><span class="line">|  Steve Patterson|     Sales|	9441|</span><br><span class="line">|William Patterson|Accounting|	8870|</span><br><span class="line">+-----------------+----------+------+</span><br><span class="line"></span><br><span class="line"># percentile_disc是离散值(discrete)，percentile_cont是连续值</span><br><span class="line">SELECT</span><br><span class="line">    department,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) AS pc1,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pc2,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) AS pc3,</span><br><span class="line">    percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pc4,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) AS pd1,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pd2,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) AS pd3,</span><br><span class="line">    percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) FILTER (WHERE employee_name LIKE &#x27;%Bo%&#x27;) AS pd4</span><br><span class="line">FROM basic_pays</span><br><span class="line">GROUP BY department</span><br><span class="line">ORDER BY department;</span><br><span class="line">+----------+-------+--------+-------+--------+-----+-----+-----+-----+</span><br><span class="line">|department|    pc1|     pc2|    pc3|     pc4|  pd1|  pd2|  pd3|  pd4|</span><br><span class="line">+----------+-------+--------+-------+--------+-----+-----+-----+-----+</span><br><span class="line">|Accounting|8543.75| 7838.25| 9746.5|10260.75| 8435| 6627| 9998|11472|</span><br><span class="line">|        IT|5917.75|    NULL|7381.25|    NULL| 5186| NULL| 8113| NULL|</span><br><span class="line">|     Sales|8550.75|    NULL| 9721.5|    NULL| 6660| NULL|10563| NULL|</span><br><span class="line">|       SCM|10449.0|10786.25|11303.0|11460.75|10449|10449|11303|11798|</span><br><span class="line">+----------+-------+--------+-------+--------+-----+-----+-----+-----+</span><br></pre></td></tr></table></figure>



<h3><span id="窗口函数">窗口函数</span></h3><p><strong>window_function</strong></p>
<ul>
<li><p>Ranking Functions</p>
<p><strong>Syntax:</strong> <code>RANK | DENSE_RANK | PERCENT_RANK | NTILE | ROW_NUMBER</code></p>
</li>
<li><p>Analytic Functions</p>
<p><strong>Syntax:</strong> <code>CUME_DIST | LAG | LEAD | NTH_VALUE | FIRST_VALUE | LAST_VALUE</code></p>
</li>
<li><p>Aggregate Functions</p>
<p><strong>Syntax:</strong> <code>MAX | MIN | COUNT | SUM | AVG | ...</code></p>
</li>
</ul>
<h3><span id="pivot">PIVOT</span></h3><p>数据透视</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PIVOT ( &#123; aggregate_expression [ AS aggregate_expression_alias ] &#125; [ , ... ]</span><br><span class="line">    FOR column_list IN ( expression_list ) )</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">CREATE table emp111 AS select * from values</span><br><span class="line">(&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)</span><br><span class="line"></span><br><span class="line">CREATE OR REPLACE TEMPORARY VIEW emp11(dept, sex, salary) AS select * from values</span><br><span class="line">(&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)</span><br><span class="line"></span><br><span class="line">select * from emp11;</span><br><span class="line">dept	sex	salary</span><br><span class="line">dev	m	115</span><br><span class="line">sale	f	95</span><br><span class="line">sale	m	85</span><br><span class="line">dev	f	117</span><br><span class="line">dev	m	117</span><br><span class="line"></span><br><span class="line">SELECT * FROM emp11</span><br><span class="line">    PIVOT (</span><br><span class="line">        SUM(salary) AS sal_sum</span><br><span class="line">        FOR sex IN (&#x27;m&#x27;, &#x27;f&#x27;)</span><br><span class="line">    );</span><br><span class="line">-------    </span><br><span class="line">dept	m	f</span><br><span class="line">dev	232	117</span><br><span class="line">sale	85	95   </span><br><span class="line"></span><br><span class="line">SELECT * FROM emp11</span><br><span class="line">    PIVOT (</span><br><span class="line">        SUM(salary) AS sal_sum, avg(salary) AS sal_avg</span><br><span class="line">        FOR sex IN (&#x27;m&#x27;, &#x27;f&#x27;)</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">--------</span><br><span class="line"></span><br><span class="line">dept	m_sal_sum	m_sal_avg	f_sal_sum	f_sal_avg</span><br><span class="line">dev	232	116.0	117	117.0</span><br><span class="line">sale	85	85.0	95	95.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="unpivot">UNPIVOT</span></h3><p>数据透视的逆向，有点类似把 透视表的一行转成多行输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE sales_quarterly (year INT, q1 INT, q2 INT, q3 INT, q4 INT);</span><br><span class="line">INSERT INTO sales_quarterly VALUES</span><br><span class="line">    (2020, null, 1000, 2000, 2500),</span><br><span class="line">    (2021, 2250, 3200, 4200, 5900),</span><br><span class="line">    (2022, 4200, 3100, null, null);</span><br><span class="line"></span><br><span class="line">-- column names are used as unpivot columns</span><br><span class="line">SELECT * FROM sales_quarterly</span><br><span class="line">    UNPIVOT (</span><br><span class="line">        sales FOR quarter IN (q1, q2, q3, q4)</span><br><span class="line">    );</span><br><span class="line">      </span><br><span class="line">+------+---------+-------+</span><br><span class="line">| year | quarter | sales |</span><br><span class="line">+------+---------+-------+</span><br><span class="line">| 2020 | q2      | 1000  |</span><br><span class="line">| 2020 | q3      | 2000  |</span><br><span class="line">| 2020 | q4      | 2500  |</span><br><span class="line">| 2021 | q1      | 2250  |</span><br><span class="line">| 2021 | q2      | 3200  |</span><br><span class="line">| 2021 | q3      | 4200  |</span><br><span class="line">| 2021 | q4      | 5900  |</span><br><span class="line">| 2022 | q1      | 4200  |</span><br><span class="line">| 2022 | q2      | 3100  |</span><br><span class="line">+------+---------+-------+</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="lateral">Lateral</span></h3><p>LATERAL VIEW 侧边视图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LATERAL VIEW [ OUTER ] generator_function ( expression [ , ... ] ) [ table_alias ] AS column_alias [ , ... ]</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>generator_function</strong></p>
<p>Specifies a generator function (EXPLODE, INLINE, etc.).</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE person (id INT, name STRING, age INT, class INT, address STRING);</span><br><span class="line">INSERT INTO person VALUES</span><br><span class="line">    (100, &#x27;John&#x27;, 30, 1, &#x27;Street 1&#x27;),</span><br><span class="line">    (200, &#x27;Mary&#x27;, NULL, 1, &#x27;Street 2&#x27;),</span><br><span class="line">    (300, &#x27;Mike&#x27;, 80, 3, &#x27;Street 3&#x27;),</span><br><span class="line">    (400, &#x27;Dan&#x27;, 50, 4, &#x27;Street 4&#x27;);</span><br><span class="line"></span><br><span class="line">SELECT * FROM person</span><br><span class="line">    LATERAL VIEW EXPLODE(ARRAY(30, 60)) tableName AS c_age</span><br><span class="line">    LATERAL VIEW EXPLODE(ARRAY(40, 80)) AS d_age;</span><br><span class="line">+------+-------+-------+--------+-----------+--------+--------+</span><br><span class="line">|  id  | name  |  age  | class  |  address  | c_age  | d_age  |</span><br><span class="line">+------+-------+-------+--------+-----------+--------+--------+</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 30     | 40     |</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 30     | 80     |</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 60     | 40     |</span><br><span class="line">| 100  | John  | 30    | 1      | Street 1  | 60     | 80     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 30     | 40     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 30     | 80     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 60     | 40     |</span><br><span class="line">| 200  | Mary  | NULL  | 1      | Street 2  | 60     | 80     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 30     | 40     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 30     | 80     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 60     | 40     |</span><br><span class="line">| 300  | Mike  | 80    | 3      | Street 3  | 60     | 80     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 30     | 40     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 30     | 80     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 60     | 40     |</span><br><span class="line">| 400  | Dan   | 50    | 4      | Street 4  | 60     | 80     |</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="dml">DML</span></h2><h3><span id="insert-into">Insert into</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE students (name STRING, address  STRING) PARTITIONED BY (birthday DATE);</span><br><span class="line"></span><br><span class="line">INSERT INTO students PARTITION (birthday = date&#x27;2019-01-02&#x27;)</span><br><span class="line">    VALUES (&#x27;Amy Smith&#x27;, &#x27;123 Park Ave, San Jose&#x27;);</span><br><span class="line"></span><br><span class="line">SELECT * FROM students;</span><br><span class="line">+-------------+-------------------------+-----------+</span><br><span class="line">|         name|                  address|   birthday|</span><br><span class="line">+-------------+-------------------------+-----------+</span><br><span class="line">|    Amy Smith|   123 Park Ave, San Jose| 2019-01-02|</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE students PARTITION (student_id = 11215016) (address, name) VALUES</span><br><span class="line">    (&#x27;Hangzhou, China&#x27;, &#x27;Kent Yao Jr.&#x27;);</span><br></pre></td></tr></table></figure>





<h3><span id="insert-overwrite-directory">Insert overwrite directory</span></h3><h4><span id="spark-format">Spark format</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE DIRECTORY &#x27;/tmp/destination&#x27;</span><br><span class="line">    USING parquet</span><br><span class="line">    OPTIONS (col1 1, col2 2, col3 &#x27;test&#x27;)</span><br><span class="line">    SELECT * FROM test_table;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE DIRECTORY</span><br><span class="line">    USING parquet</span><br><span class="line">    OPTIONS (&#x27;path&#x27; &#x27;/tmp/destination&#x27;, col1 1, col2 2, col3 &#x27;test&#x27;)</span><br><span class="line">    SELECT * FROM test_table;</span><br></pre></td></tr></table></figure>

<h4><span id="hive-format">Hive format</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE LOCAL DIRECTORY &#x27;/tmp/destination&#x27;</span><br><span class="line">    STORED AS orc</span><br><span class="line">    SELECT * FROM test_table;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE LOCAL DIRECTORY &#x27;/tmp/destination&#x27;</span><br><span class="line">    ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;</span><br><span class="line">    SELECT * FROM test_table;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/09/bigdata/spark/spark3/20230707_sparksql/" data-id="cljvl2jh80000619a7e0a2rjw" data-title="20230707_sparksql" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230707_spark_data_format" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/07/bigdata/spark/spark3/20230707_spark_data_format/" class="article-date">
  <time class="dt-published" datetime="2023-07-07T14:24:20.000Z" itemprop="datePublished">2023-07-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/07/bigdata/spark/spark3/20230707_spark_data_format/">20230707_spark_data_format</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#datasource">DataSource</a><ul>
<li><a href="#%E9%80%9A%E7%94%A8optition%E8%AE%BE%E7%BD%AE">通用Optition设置</a></li>
</ul>
</li>
<li><a href="#paquet">Paquet</a><ul>
<li><a href="#mergeschema">mergeSchema</a><ul>
<li><a href="#merge-schema-test">Merge Schema Test</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orc">ORC</a><ul>
<li><a href="#mergeschema-1">mergeSchema</a><ul>
<li><a href="#merge-schema-test-1">Merge Schema Test</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#json">JSON</a></li>
<li><a href="#csv">CSV</a></li>
<li><a href="#text">Text</a></li>
<li><a href="#hive">HIVE</a><ul>
<li><a href="#conncet">Conncet</a><ul>
<li><a href="#sparksession">SparkSession</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">配置文件</a></li>
</ul>
</li>
<li><a href="#hive-%E5%8F%82%E6%95%B0">Hive 参数</a></li>
<li><a href="#%E5%88%9B%E5%BB%BAhive-%E8%A1%A8">创建Hive 表</a></li>
<li><a href="#hive-%E5%86%85%E9%83%A8%E8%A1%A8-%E5%A4%96%E9%83%A8%E8%A1%A8-%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E8%A1%A8">Hive 内部表、外部表、动态分区表</a></li>
<li><a href="#sql-hive%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA">SQL Hive动态分区</a></li>
<li><a href="#%E5%A4%96%E9%83%A8%E8%A1%A8strogehandler">外部表StrogeHandler</a></li>
</ul>
</li>
<li><a href="#jdbc">JDBC</a><ul>
<li><a href="#connect">Connect</a><ul>
<li><a href="#option">Option</a></li>
</ul>
</li>
<li><a href="#sql">SQL</a></li>
<li><a href="#scala">Scala</a></li>
</ul>
</li>
<li><a href="#avro">Avro</a><ul>
<li><a href="#%E5%8F%82%E6%95%B0">参数</a></li>
<li><a href="#read">Read</a></li>
</ul>
</li>
<li><a href="#protobuf-data">Protobuf Data</a></li>
</ul>
<!-- tocstop -->

<h2><span id="datasource">DataSource</span></h2><h3><span id="通用optition设置">通用Optition设置</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">examples/src/main/resources/dir1</span><br><span class="line">├── dir2</span><br><span class="line">│   └── file2.parquet</span><br><span class="line">├── file1.parquet</span><br><span class="line">└── file3.json</span><br><span class="line"></span><br><span class="line">#ignoreCorruptFiles 忽略不规范文件, 直接跳过file3.json</span><br><span class="line">    val testCorruptDF0  = spark.read.option(&quot;ignoreCorruptFiles&quot;, &quot;true&quot;)</span><br><span class="line">    .parquet(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/&quot;,</span><br><span class="line">    &quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/dir2/&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#递归查找parquet</span><br><span class="line">val recursiveLoadedDF = spark.read.format(&quot;parquet&quot;)</span><br><span class="line">  .option(&quot;recursiveFileLookup&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;ignoreCorruptFiles&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1&quot;)</span><br><span class="line">  </span><br><span class="line"># 模式匹配, 找到路径下的所有parquet文件</span><br><span class="line">val testGlobFilterDF = spark.read.format(&quot;parquet&quot;)</span><br><span class="line">  .option(&quot;pathGlobFilter&quot;, &quot;*.parquet&quot;) // json file should be filtered out</span><br><span class="line">  .load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1&quot;)</span><br><span class="line">testGlobFilterDF.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="paquet">Paquet</span></h2><p><a target="_blank" rel="noopener" href="https://parquet.apache.org/">Parquet</a> is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. </p>
<p>spark.sql.parquet.enableVectorizedReader 向量化读取</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> Hive metastore Parquet <span class="keyword">table</span>  parquet元数据刷新</span><br><span class="line">REFRESH <span class="keyword">TABLE</span> my_table;</span><br><span class="line">spark.catalog.refreshTable(&quot;my_table&quot;)</span><br></pre></td></tr></table></figure>



<h3><span id="mergeschema">mergeSchema</span></h3><p>spark.sql.parquet.mergeSchema</p>
<p>schema自动合并，自动扩展 识别到新的字段，相当于实现自动添加新字段，默认是关闭的</p>
<p>schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</span><br></pre></td></tr></table></figure>

<h4><span id="merge-schema-test">Merge Schema Test</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, store into a partition directory</span><br><span class="line">val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i =&gt; (i, i * i)).toDF(&quot;value&quot;, &quot;square&quot;)</span><br><span class="line">squaresDF.write.parquet(&quot;/tmp/parquet/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i =&gt; (i, i * i * i)).toDF(&quot;value&quot;, &quot;cube&quot;)</span><br><span class="line">cubesDF.write.parquet(&quot;/tmp/parquet/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val mergedDF = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;/tmp/parquet/test_table&quot;)</span><br><span class="line">mergedDF.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths</span><br><span class="line">// root</span><br><span class="line">//  |-- value: int (nullable = true)</span><br><span class="line">//  |-- square: int (nullable = true)</span><br><span class="line">//  |-- cube: int (nullable = true)</span><br><span class="line">//  |-- key: int (nullable = true)</span><br><span class="line"></span><br><span class="line">mergedDF.show()</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|value|square|cube|key|</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|    1|     1|null|  1|</span><br><span class="line">|    2|     4|null|  1|</span><br><span class="line">|    4|    16|null|  1|</span><br><span class="line">|    5|    25|null|  1|</span><br><span class="line">|    3|     9|null|  1|</span><br><span class="line">|    6|  null| 216|  2|</span><br><span class="line">|    7|  null| 343|  2|</span><br><span class="line">|    8|  null| 512|  2|</span><br><span class="line">|    9|  null| 729|  2|</span><br><span class="line">|   10|  null|1000|  2|</span><br><span class="line"></span><br><span class="line"># mergeSchema 默认关闭</span><br><span class="line">val mergedDF = spark.read.parquet(&quot;/tmp/parquet/test_table&quot;)</span><br><span class="line">mergedDF.show()</span><br><span class="line">// show 结果如下</span><br><span class="line">|value|square|key|</span><br><span class="line">+-----+------+---+</span><br><span class="line">|    1|     1|  1|</span><br><span class="line">|    2|     4|  1|</span><br><span class="line">|    4|    16|  1|</span><br><span class="line">|    5|    25|  1|</span><br><span class="line">|    3|     9|  1|</span><br><span class="line">|    6|  null|  2|</span><br><span class="line">|    7|  null|  2|</span><br><span class="line">|    8|  null|  2|</span><br><span class="line">|    9|  null|  2|</span><br><span class="line">|   10|  null|  2|</span><br><span class="line">+-----+------+---+</span><br></pre></td></tr></table></figure>



<h2><span id="orc">ORC</span></h2><p><a target="_blank" rel="noopener" href="https://orc.apache.org/">Apache ORC</a> is a columnar format which has more advanced features like native zstd compression, bloom filter and columnar encryption.  支持zstd压缩,  bloom过滤器和列加密</p>
<p>spark.sql.orc.enableVectorizedReader 向量化读取</p>
<h3><span id="mergeschema">mergeSchema</span></h3><p>spark.sql.orc.mergeSchema</p>
<p>schema自动合并，自动扩展 识别到新的字段，相当于实现自动添加新字段，默认是关闭的</p>
<p>schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default.</p>
<ol>
<li>setting data source option <code>mergeSchema</code> to <code>true</code> when reading ORC files, or</li>
<li>setting the global SQL option <code>spark.sql.orc.mergeSchema</code> to <code>true</code></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">When true, the ORC data source merges schemas collected from all data files, otherwise the schema is picked from a random data file.</span><br></pre></td></tr></table></figure>

<h4><span id="merge-schema-test">Merge Schema Test</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">// This is used to implicitly convert an RDD to a DataFrame.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// Create a simple DataFrame, store into a partition directory</span><br><span class="line">val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i =&gt; (i, i * i)).toDF(&quot;value&quot;, &quot;square&quot;)</span><br><span class="line">squaresDF.write.orc(&quot;/tmp/orc/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// Create another DataFrame in a new partition directory,</span><br><span class="line">// adding a new column and dropping an existing column</span><br><span class="line">val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i =&gt; (i, i * i * i)).toDF(&quot;value&quot;, &quot;cube&quot;)</span><br><span class="line">cubesDF.write.mode(&quot;overwrite&quot;).orc(&quot;/tmp/orc/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// Read the partitioned table</span><br><span class="line">val mergedDF = spark.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).orc(&quot;/tmp/orc/test_table&quot;)</span><br><span class="line">mergedDF.printSchema()</span><br><span class="line"></span><br><span class="line">// The final schema consists of all 3 columns in the Parquet files together</span><br><span class="line">// with the partitioning column appeared in the partition directory paths</span><br><span class="line">// root</span><br><span class="line">//  |-- value: int (nullable = true)</span><br><span class="line">//  |-- square: int (nullable = true)</span><br><span class="line">//  |-- cube: int (nullable = true)</span><br><span class="line">//  |-- key: int (nullable = true)</span><br><span class="line"></span><br><span class="line">mergedDF.show()</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|value|square|cube|key|</span><br><span class="line">+-----+------+----+---+</span><br><span class="line">|    1|     1|null|  1|</span><br><span class="line">|    2|     4|null|  1|</span><br><span class="line">|    4|    16|null|  1|</span><br><span class="line">|    5|    25|null|  1|</span><br><span class="line">|    3|     9|null|  1|</span><br><span class="line">|    6|  null| 216|  2|</span><br><span class="line">|    7|  null| 343|  2|</span><br><span class="line">|    8|  null| 512|  2|</span><br><span class="line">|    9|  null| 729|  2|</span><br><span class="line">|   10|  null|1000|  2|</span><br><span class="line"></span><br><span class="line"># mergeSchema 默认关闭</span><br><span class="line">val mergedDF = spark.read.orc(&quot;/tmp/orc/test_table&quot;)</span><br><span class="line">mergedDF.show()</span><br><span class="line">// show 结果如下</span><br><span class="line">|value|square|key|</span><br><span class="line">+-----+------+---+</span><br><span class="line">|    1|     1|  1|</span><br><span class="line">|    2|     4|  1|</span><br><span class="line">|    4|    16|  1|</span><br><span class="line">|    5|    25|  1|</span><br><span class="line">|    3|     9|  1|</span><br><span class="line">|    6|  null|  2|</span><br><span class="line">|    7|  null|  2|</span><br><span class="line">|    8|  null|  2|</span><br><span class="line">|    9|  null|  2|</span><br><span class="line">|   10|  null|  2|</span><br><span class="line">+-----+------+---+</span><br></pre></td></tr></table></figure>







<h2><span id="json">JSON</span></h2><p> Spark SQL automatically infer the schema of a JSON dataset and load it as a <code>Dataset[Row]</code>.</p>
<h2><span id="csv">CSV</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// Read a csv with delimiter and a header</span><br><span class="line">val df3 = spark.read.option(&quot;delimiter&quot;, &quot;;&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(path)</span><br><span class="line">df3.show(</span><br></pre></td></tr></table></figure>



<h2><span id="text">Text</span></h2><h2><span id="hive">HIVE</span></h2><h3><span id="conncet">Conncet</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h4><span id="sparksession">SparkSession</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  #config hive support</span><br><span class="line">  </span><br><span class="line">  val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local[3]&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">//      .config(&quot;spark.sql.catalogImplementation&quot;, &quot;hive&quot;)</span><br><span class="line">      .enableHiveSupport()  //&quot;spark.sql.catalogImplementation&quot;  &quot;hive&quot;</span><br><span class="line">      .getOrCreate()</span><br><span class="line">      </span><br><span class="line">   import spark.implicits._</span><br><span class="line">   import spark.sql</span><br></pre></td></tr></table></figure>

<h4><span id="配置文件">配置文件</span></h4><p>配置Hive-site.xml</p>
<p>copy  hive-site.xml   $SPARK_HOME&#x2F;conf</p>
<p>mac本地连接配置xml：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sparkContext.hadoopConfiguration.addResource(new Path(&quot;*/hive-site.xml&quot;))</span><br></pre></td></tr></table></figure>



<h3><span id="hive-参数">Hive 参数</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 动态分区设置</span><br><span class="line">hive.exec.dynamic.partition=true; 是开启动态分区</span><br><span class="line">hive.exec.dynamic.partition.mode=nonstrict; 这个属性默认值是strict,就是要求分区字段必须有一个是静态的分区值，随后会讲到，当前设置为nonstrict,那么可以全部动态分区</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="创建hive-表">创建Hive 表</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;use test_zhou&quot;)</span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &#x27;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">  case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br></pre></td></tr></table></figure>



<h3><span id="hive-内部表-外部表-动态分区表">Hive 内部表、外部表、动态分区表</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">//内部表</span><br><span class="line">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span><br><span class="line">// `USING hive`</span><br><span class="line">sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">// Save DataFrame to the Hive managed table</span><br><span class="line">val df = spark.table(&quot;src&quot;)</span><br><span class="line">df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line">// After insertion, the Hive managed table has data now</span><br><span class="line">sql(&quot;SELECT * FROM hive_records&quot;).show()</span><br><span class="line"></span><br><span class="line">//外部表</span><br><span class="line">// Prepare a Parquet data directory</span><br><span class="line">val dataDir = &quot;/tmp/parquet_data&quot;</span><br><span class="line">spark.range(10).write.parquet(dataDir)</span><br><span class="line">// Create a Hive external Parquet table</span><br><span class="line">sql(s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#x27;$dataDir&#x27;&quot;)</span><br><span class="line">// The Hive external table should already have data</span><br><span class="line">sql(&quot;SELECT * FROM hive_bigints&quot;).show()</span><br><span class="line"></span><br><span class="line">//分区表</span><br><span class="line">// Turn on flag for Hive Dynamic Partitioning</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)</span><br><span class="line">// Create a Hive partitioned table using DataFrame API   </span><br><span class="line">//创建hive分区表, 指定format hive, 输出格式为text</span><br><span class="line">//查看schema: CREATE TABLE hive_part_tbl (value STRING,key INT)USING text PARTITIONED BY (key)</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;)</span><br><span class="line">// Partitioned column `key` will be moved to the end of the schema, 分区列移动到schema尾部</span><br><span class="line">sql(&quot;SELECT * FROM hive_part_tbl&quot;).show()</span><br><span class="line"></span><br><span class="line">//创建spark分区表, 如果不指定format hive, 默认格式parquet</span><br><span class="line">// 输出目录多了一个_SUCCESS文件, 输出snappy.parquet格式</span><br><span class="line">// 查看schema: CREATE TABLE hive_part_tb2 (value STRING,key INT)USING parquet PARTITIONED BY (key)</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).saveAsTable(&quot;hive_part_tb2&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//Spark SQL native syntax spark sql 原生(内置)语法创建 hive分区表</span><br><span class="line">CREATE external TABLE hive_part_tbl (value STRING,key INT)USING text PARTITIONED BY (key)</span><br><span class="line"></span><br><span class="line">//hive 语法 （hive HQL syntax ）创建 hive分区表</span><br><span class="line">//分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列</span><br><span class="line">CREATE external TABLE hive_par_tess(value STRING) STORED AS  textfile PARTITIONED BY (key INT) location &#x27;/user/hive/warehouse/test_zhou.db/hive_part_tbl&#x27;</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;msck repair table hive_par_tess&quot;).show  //外部表 元数据刷新之后, 才能查看到数据</span><br><span class="line"></span><br><span class="line">#这种写法是错误的, 分区字段key不能写在前面ddl字段定义里, 要写在PARTITIONED By 后面, 和spark sql写法不同</span><br><span class="line">CREATE external TABLE hive_par_tess2(value STRING, key INT) STORED AS  textfile PARTITIONED BY (key INT) location &#x27;/user/hive/warehouse/test_zhou.db/hive_part_tbl&#x27;</span><br></pre></td></tr></table></figure>



<h3><span id="sql-hive动态分区">SQL Hive动态分区</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">create table test_part2(id int, name string) </span><br><span class="line">partitioned by (country string, province string );</span><br><span class="line"></span><br><span class="line">--静态分区 指定分区字段的值</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;sh&#x27;)  select id, name from test_part1 where id = 1 and name = &#x27;lisa&#x27;;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;)  values(2, &#x27;dal&#x27;)</span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province=&#x27;zj&#x27;) select 2 as id, &#x27;cate&#x27; as name</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--半动态分区(一部分静态分区 + 一部分动态分区)</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country=&#x27;china&#x27;, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--全动态分区</span><br><span class="line">set hive.exec.dynamici.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;js&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-sql \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">insert into table test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;tez&#x27; as province;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-shell   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition=true   \</span><br><span class="line">--conf spark.hadoop.hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;insert into table test_zhou.test_part2 partition(country, province)  select 2 as id, &#x27;cate&#x27; as name, &#x27;american&#x27; as country,  &#x27;newyork&#x27; as province&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="外部表strogehandler">外部表StrogeHandler</span></h3><p>外部表StrogeHandler使用</p>
<p>目前(Spark 3.4.1 ) spark sql原生内置sql语法还不支持使用Hive storage handler 建表，如果需要使用，</p>
<p>先通过hive建表，再使用Spark sql读取数据( Hive storage handler is not supported yet when creating table, you can create a table using storage handler at Hive side, and use Spark SQL to read it.)</p>
<p><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/hive.html">创建ES外部表</a></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#待实践</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> artists (...)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.elasticsearch.hadoop.hive.EsStorageHandler&#x27;</span></span><br><span class="line">TBLPROPERTIES(<span class="string">&#x27;es.resource&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;radio/artists&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;es.index.auto.create&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;false&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>创建HBase外部表</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#待实践</span><br><span class="line">create external table tb2_cp (</span><br><span class="line">name string,</span><br><span class="line">age int,</span><br><span class="line">addr string</span><br><span class="line">)</span><br><span class="line">STORED BY ‘org.apache.hadoop.hive.hbase.HBaseStorageHandler’</span><br><span class="line">WITH SERDEPROPERTIES (“hbase.columns.mapping” = “:key, cf:age, cf:addr”)</span><br><span class="line">TBLPROPERTIES (“hbase.table.name” = “test_tb2”);</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="jdbc">JDBC</span></h2><h3><span id="connect">Connect</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;mysqlConnectorVersion&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>



<h4><span id="option">Option</span></h4><p>prepareQuery 执行读取数据前，先执行此语句，相当于预处理</p>
<p>fetchsize  一次读取行数</p>
<p>batchsize  批量一次写入行数，默认1000</p>
<h3><span id="sql">SQL</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY VIEW mysql_users_view</span><br><span class="line">USING org.apache.spark.sql.jdbc</span><br><span class="line">OPTIONS (</span><br><span class="line">  url &quot;jdbc:mysql://localhost&quot;,</span><br><span class="line">  dbtable &quot;test_zhou.users&quot;,</span><br><span class="line">  user &#x27;***&#x27;,</span><br><span class="line">  password &#x27;***&#x27;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3><span id="scala">Scala</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">//read jdbc</span><br><span class="line">val df1 = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://127.0.0.1:3306/test_zhou?characterEncoding=utf8&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;users&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;**&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;***&quot;)</span><br><span class="line">      .option(&quot;abc&quot;, &quot;def&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">//read jdbc:  dbtable = subquey 指定一个子查询</span><br><span class="line">val df1 = spark</span><br><span class="line">      .read</span><br><span class="line">      .format(&quot;jdbc&quot;)</span><br><span class="line">      .option(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)</span><br><span class="line">      .option(&quot;url&quot;, &quot;jdbc:mysql://127.0.0.1:3306/test_zhou?characterEncoding=utf8&quot;)</span><br><span class="line">      .option(&quot;dbtable&quot;, &quot;(select id, name from users) as subq&quot;)</span><br><span class="line">      .option(&quot;user&quot;, &quot;**&quot;)</span><br><span class="line">      .option(&quot;password&quot;, &quot;**&quot;)</span><br><span class="line">      .option(&quot;abc&quot;, &quot;def&quot;)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">//并行多分区同时读取 jdbc, 指定分区列</span><br><span class="line">spark.read.format(&quot;jdbc&quot;)</span><br><span class="line">.option(&quot;url&quot;, jdbcUrl)</span><br><span class="line">.option(&quot;dbtable&quot;, &quot;(select c1, c2 from t1) as subq&quot;)</span><br><span class="line">.option(&quot;partitionColumn&quot;, &quot;c1&quot;)</span><br><span class="line">.option(&quot;lowerBound&quot;, &quot;1&quot;)</span><br><span class="line">.option(&quot;upperBound&quot;, &quot;100&quot;)</span><br><span class="line">.option(&quot;numPartitions&quot;, &quot;3&quot;)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line">//并行多分区同时读取 jdbc</span><br><span class="line">val url = &quot;jdbc:mysql://127.0.0.1:3306/test_zhou?characterEncoding=utf8&quot;</span><br><span class="line">val table = &quot;(select * from test_zhou.users) E&quot;</span><br><span class="line">//每个条件对应一个partition数据, 对应一个task</span><br><span class="line">val partitionStr = &quot;id&lt;13 or id is null,  13&lt;=id and id &lt;18, id&gt;=18&quot;</span><br><span class="line">val partitionArray = partitionStr.split(&quot;,&quot;).map(_.trim)</span><br><span class="line">val javaProps = new java.util.Properties</span><br><span class="line">javaProps.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">javaProps.put(&quot;password&quot;, &quot;Zoom@123&quot;)</span><br><span class="line">val df = spark.read.jdbc(url, table, partitionArray, javaProps)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//write jdbc</span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .save()</span><br><span class="line"></span><br><span class="line">//写入时 指定table schema</span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br></pre></td></tr></table></figure>



<h2><span id="avro">Avro</span></h2><p>Avro is built-in but external data source module since Spark 2.4</p>
<p>（schema和data分两部分存储，文件头部存储schema， 数据本身存储为二进制格式）</p>
<p>行式存储，写入速度快，文件可拆分，可压缩，可以独立地添加新的字段</p>
<p>是一种基于行的、可高度拆分的数据格式</p>
<p>为了最大程度地减小文件大小、并提高效率，它将schema存储为JSON格式，而将数据存储为二进制格式</p>
<ul>
<li>Avro是一种与语言无关的数据序列化。</li>
<li>Avro将schema存储在文件的标题中，因此数据具有自描述性。</li>
<li>Avro格式的文件既可以被拆分、又可以被压缩，因此非常适合于在Hadoop生态系统中进行数据的存储。</li>
<li>由于Avro文件将负责读取的schema与负责写入的schema区分开来，因此它可以独立地添加新的字段。</li>
<li>与序列文件(Sequence Files)相同，为了实现高度可拆分性，Avro文件也包含了用于分隔不同块(block)的同步标记。</li>
<li>可以使用诸如snappy之类的压缩格式，来压缩不同的目标块。</li>
</ul>
<h3><span id="参数">参数</span></h3><p>avroSchema </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.option(&quot;avroSchema&quot;, avroSchema)</span><br></pre></td></tr></table></figure>

<p>cat examples&#x2F;src&#x2F;main&#x2F;resources&#x2F;user_1.avsc</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">&#123;&quot;namespace&quot;: &quot;example.avro&quot;,</span><br><span class="line"> &quot;type&quot;: &quot;record&quot;,</span><br><span class="line"> &quot;name&quot;: &quot;User&quot;,</span><br><span class="line"> &quot;fields&quot;: [</span><br><span class="line">     &#123;&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">     &#123;&quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]&#125;,</span><br><span class="line">     &#123;&quot;type&quot;: &#123;&quot;items&quot;: &quot;int&quot;, &quot;type&quot;: &quot;array&quot;&#125;, &quot;name&quot;: &quot;favorite_numbers&quot;&#125;</span><br><span class="line"> ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># java 读取schema 文件</span><br><span class="line">import java.nio.file.&#123;Files, Paths&#125;</span><br><span class="line">val avroSchema = new String(Files.readAllBytes(Paths.get(&quot;/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/user_1.avsc&quot;)))</span><br><span class="line">val usersDF = spark.read.format(&quot;avro&quot;).load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.avro&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="read">Read</span></h3><p>The <code>spark-avro</code> module is external and not included in <code>spark-submit</code> or <code>spark-shell</code> by default.</p>
<p>读取时需要指定依赖, 或者添加到spark classpath</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./bin/spark-shell --packages org.apache.spark:spark-avro_2.12:3.4.1</span><br><span class="line"></span><br><span class="line">val usersDF = spark.read.format(&quot;avro&quot;).load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.avro&quot;)</span><br><span class="line"></span><br><span class="line">//write</span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.format(&quot;avro&quot;).save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/avro/namesAndFavColors.avro&quot;)</span><br><span class="line"></span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_numbers&quot;).write.mode(&quot;overwrite&quot;).format(&quot;avro&quot;).save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/avro/namesAndFavNums.avro&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import java.nio.file.&#123;Files, Paths&#125;</span><br><span class="line">val avroSchema = new String(Files.readAllBytes(Paths.get(&quot;/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/user_1.avsc&quot;)))</span><br><span class="line"></span><br><span class="line"># 读取失败</span><br><span class="line">val usersDF = spark.read.format(&quot;avro&quot;)</span><br><span class="line">.option(&quot;recursiveFileLookup&quot;, &quot;true&quot;)</span><br><span class="line">.option(&quot;ignoreCorruptFiles&quot;,&quot;true&quot;)</span><br><span class="line">.option(&quot;avroSchema&quot;, avroSchema)</span><br><span class="line">.load(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/avro&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="protobuf-data">Protobuf Data</span></h2><p>Since Spark 3.4.0 release, <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> provides built-in support for reading and writing protobuf data.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/07/bigdata/spark/spark3/20230707_spark_data_format/" data-id="clk6akm6f0000lk9agn6d7e23" data-title="20230707_spark_data_format" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/" class="article-date">
  <time class="dt-published" datetime="2023-07-05T04:28:09.000Z" itemprop="datePublished">2023-07-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/">kyuubi_simple_usage</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#kyuubi-%E6%98%AF%E4%BB%80%E4%B9%88">kyuubi 是什么</a></li>
<li><a href="#%E7%8E%AF%E5%A2%83-%E9%83%A8%E7%BD%B2kyuubi-080-rc2">环境 部署kyuubi-0.8.0-rc2</a><ul>
<li><a href="#compile">Compile</a></li>
<li><a href="#install">install</a><ul>
<li><a href="#%E5%8F%98%E9%87%8F%E8%AE%BE%E7%BD%AE">变量设置</a></li>
<li><a href="#sparkkyuubiauthenticationnone">spark.kyuubi.authentication&#x3D;NONE</a><ul>
<li><a href="#%E9%97%AE%E9%A2%98">问题</a></li>
</ul>
</li>
<li><a href="#sparkkyuubiauthenticationkerberos">spark.kyuubi.authentication&#x3D;kerberos</a></li>
<li><a href="#kerberos-ha">kerberos + ha</a></li>
</ul>
</li>
<li><a href="#test">Test</a><ul>
<li><a href="#connections">Connections</a></li>
<li><a href="#problems">Problems</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%8E%AF%E5%A2%83-%E9%83%A8%E7%BD%B2kyuubi-120">环境 部署kyuubi-1.2.0</a><ul>
<li><a href="#zookeeper">Zookeeper</a></li>
<li><a href="#ha">HA</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#hive">Hive</a></li>
<li><a href="#source">Source</a><ul>
<li><a href="#kyuubiserver">KyuubiServer</a></li>
<li><a href="#sparksqlengine">SparkSQLEngine</a></li>
<li><a href="#servicediscovery">ServiceDiscovery</a></li>
</ul>
</li>
<li><a href="#debug">Debug</a><ul>
<li><a href="#server-debug">server debug</a></li>
<li><a href="#app-debug">App debug</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<!-- tocstop -->

<h2><span id="kyuubi-是什么">kyuubi 是什么</span></h2><p>Kyuubi is a distributed multi-tenant JDBC server for large-scale data processing and analytics, built on top of Apache Spark。</p>
<p>Kyuubi has enhanced the Thrift JDBC&#x2F;ODBC Server in some ways for solving these existing problems, as shown in the following table.</p>
<p>简单的来说，是spark thrift server 的增强版，增加了权限管控、动态资源扩展、多租户等等。</p>
<h2><span id="环境-部署kyuubi-080-rc2">环境 部署kyuubi-0.8.0-rc2</span></h2><h3><span id="compile">Compile</span></h3><p><a target="_blank" rel="noopener" href="https://github.com/yaooqinn/kyuubi/tree/v0.8.0-rc2">Download</a></p>
<p>Tag &gt; v0.8.0-rc2 以后，编译需要spark3.0, </p>
<p>Tag &lt;&#x3D; v0.8.0-rc2 , 编译可在pom.xml文件中指定spark 2.*版本，也可以直接使用<a target="_blank" rel="noopener" href="https://github.com/yaooqinn/kyuubi/releases/tag/v0.8.0-rc2">github realease 包</a></p>
<p>由于生产中使用了spark 2.4, 实际测试编译 使用了<strong>v0.8.0-rc2 版本</strong>进行编译。</p>
<p>.&#x2F;build&#x2F;mvn clean package -DskipTests</p>
<p>.&#x2F;build&#x2F;dist –tgz （.&#x2F;build&#x2F;dist –tgz -Pspark-2.4.0 这里指定了spark 版本，但是没有生效，可在pom文件指定）</p>
<p>编译过程中，出现jar包pentaho-aggdesigner-algorithm&#x2F;5.1.5-jhyde 找不到，可以手动<a target="_blank" rel="noopener" href="http://conjars.org/repo/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/">下载</a>至本地maven仓库中</p>
<h3><span id="install">install</span></h3><h4><span id="变量设置">变量设置</span></h4><p>在 kyuubi_home&#x2F;bin&#x2F;kyuubi-env.sh中， 修改spark_home:</p>
<p>export SPARK_HOME&#x3D;&#x2F;Users&#x2F;Kent&#x2F;Documents&#x2F;spark</p>
<h4><span id="sparkkyuubiauthenticationx3dnone">spark.kyuubi.authentication&#x3D;NONE</span></h4><p>1、无任何认证的情况，启动kyuubi 服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.backend.session.check.interval=1s \</span><br><span class="line">--conf spark.kyuubi.backend.session.idle.timeout=0min</span><br><span class="line"></span><br><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.backend.session.check.interval=5s \</span><br><span class="line">--conf spark.kyuubi.backend.session.idle.timeout=1min</span><br><span class="line"></span><br><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.backend.session.check.interval=1s \</span><br><span class="line">--conf spark.kyuubi.backend.session.idle.timeout=0min</span><br></pre></td></tr></table></figure>



<p>2、使用beeline 连接</p>
<p>cd   $spark_home (spark version &#x3D; spark-2.4.0-bin-hadoop2.7)</p>
<p>bin&#x2F;beeline</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;192.168.1.110:10010</p>
<p>!connect jdbc:hive2:&#x2F;&#x2F;10.26.119.144:10010</p>
<p>hive ( or username)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://10.26.119.144:10010/;hive.server2.proxy.user=zhouqingfeng#spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://10.26.119.144:10010/;hive.server2.proxy.user=zhouqingfeng#spark.executor.instances=3;spark.executor.cores=1;spark.executor.memory=512m</span><br></pre></td></tr></table></figure>

<h5><span id="问题">问题</span></h5><p>1、这里hive 的版本是1.2.1，版本太高，貌似也会有连接不上的问题。</p>
<p>2、使用beeline 连接!connect jdbc:hive2:&#x2F;&#x2F;localhost:10010 一直提示连上不上: Connection refused (state&#x3D;08S01,code&#x3D;0)</p>
<p>最后使用命令查看端口 10010，lsof -i tcp:10010，发现服务是正常的， 切换成端口显示的Ip地址后，就可以正常连接了。  !connect jdbc:hive2:&#x2F;&#x2F;localhost:10010  -》  !connect jdbc:hive2:&#x2F;&#x2F;192.168.1.110:10010</p>
<h4><span id="sparkkyuubiauthenticationx3dkerberos">spark.kyuubi.authentication&#x3D;kerberos</span></h4><p>hadoop yarn 开启了kerberos 认证的情况：</p>
<p>–kerberos<br> Kyuubi requires a principal and keytab file specified in $SPARK_HOME&#x2F;conf&#x2F;spark-defaults.conf. </p>
<p>spark.yarn.principal – Kerberos principal for Kyuubi server.<br>spark.yarn.keytab – Keytab for Kyuubi server principal.</p>
<p>可以在 SPARK_HOME&#x2F;conf&#x2F;spark-defaults.conf 增加spark.yarn.principal 和 spark.yarn.keytab两个参数。</p>
<p>也可以，在启动kyuubi 服务的时候 命令配置参数。</p>
<p>1、启动kyuubi 服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--principal    principal1 \</span><br><span class="line">--keytab       Keytab1 \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=KERBEROS</span><br></pre></td></tr></table></figure>



<p>2、使用beeline 连接</p>
<p>cd   $spark_home (spark version &#x3D; spark-2.4.0-bin-hadoop2.7)</p>
<p>bin&#x2F;beeline</p>
<p>!connect  jdbc:hive2:&#x2F;&#x2F;10.20.30.111:10000&#x2F;test_zq;principal&#x3D;hive&#x2F;<a href="mailto:&#98;&#x64;&#x70;&#50;&#64;&#x54;&#69;&#83;&#x54;&#46;&#x43;&#x4f;&#77;">&#98;&#x64;&#x70;&#50;&#64;&#x54;&#69;&#83;&#x54;&#46;&#x43;&#x4f;&#77;</a>;authentication&#x3D;kerberos</p>
<p>hive</p>
<p><strong>beeline 每个连接可以设置不同的spark 参数</strong>：</p>
<p>jdbc:hive2:&#x2F;&#x2F;<host>:<port>&#x2F;;hive.server2.proxy.user&#x3D;tom#spark.yarn.queue&#x3D;theque;spark.executor.instances&#x3D;3;spark.executor.cores&#x3D;3;spark.executor.memory&#x3D;10g</port></host></p>
<p>.&#x2F;bin&#x2F;beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;   -n hive </p>
<h4><span id="kerberos-ha">kerberos + ha</span></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--num-executors 3 \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--principal    principal1 \</span><br><span class="line">--keytab       Keytab1 \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=KERBEROS \</span><br><span class="line">--conf spark.kyuubi.ha.enabled=true \</span><br><span class="line">--conf spark.kyuubi.ha.zk.quorum=zk1:port1,zk2:port2 \</span><br><span class="line">--conf spark.kyuubi.ha.zk.namespace=kyuubiserver \</span><br><span class="line">--conf spark.kyuubi.ha.mode=load-balance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 2g \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">--conf spark.kyuubi.authentication=NONE \</span><br><span class="line">--conf spark.kyuubi.ha.enabled=true \</span><br><span class="line">--conf spark.kyuubi.ha.zk.quorum=10.20.145.31:2181 \</span><br><span class="line">--conf spark.kyuubi.ha.zk.namespace=kyuubiserver \</span><br><span class="line">--conf spark.kyuubi.ha.mode=load-balance</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">./bin/start-kyuubi.sh \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--conf spark.kyuubi.frontend.bind.port=10010 \</span><br><span class="line">-agentlib:jdwp=transport=dt_socket,address=5005,server=y,suspend=y</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><strong>jdbc url</strong></p>
<p>jdbc:hive2:&#x2F;&#x2F;zk1:port1,zk2:port2&#x2F;test_zq;serviceDiscoveryMode&#x3D;zooKeeper;zooKeeperNamespace&#x3D;kyuubiserver;principal&#x3D;hive&#x2F;<a href="mailto:&#98;&#x64;&#112;&#x32;&#64;&#x54;&#69;&#x53;&#84;&#x2e;&#x43;&#x4f;&#77;">&#98;&#x64;&#112;&#x32;&#64;&#x54;&#69;&#x53;&#84;&#x2e;&#x43;&#x4f;&#77;</a>;authentication&#x3D;kerberos</p>
<p>username: hive or others</p>
<h3><span id="test">Test</span></h3><h4><span id="connections">Connections</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://10.20.30.111:10010/;hive.server2.proxy.user=tom#spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://10.20.30.111:10010/;hive.server2.proxy.user=tom#spark.executor.instances=4;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://10.20.30.111:10010/;hive.server2.proxy.user=bob#spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br></pre></td></tr></table></figure>



<h4><span id="problems">Problems</span></h4><p>修改同一个用户提交多个作业并发测试的问题：问题定位是SparkEnv导致，多个线程混用了同一个SparkEnv</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 3, bdp-1.rdc.com, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)</span><br><span class="line">        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:89)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)</span><br><span class="line">        at org.apache.spark.scheduler.Task.run(Task.scala:121)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at scala.collection.immutable.List.foreach(List.scala:392)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)</span><br><span class="line">        at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)</span><br><span class="line">        ... 14 more</span><br><span class="line"></span><br><span class="line">Driver stacktrace:</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)</span><br><span class="line">        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">        at scala.Option.foreach(Option.scala:257)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)</span><br><span class="line">        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)</span><br><span class="line">        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)</span><br><span class="line">        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)</span><br><span class="line">        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)</span><br><span class="line">        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)</span><br><span class="line">        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)</span><br><span class="line">        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)</span><br><span class="line">        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">        at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">        at org.apache.spark.rdd.RDD.collect(RDD.scala:944)</span><br><span class="line">        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)</span><br><span class="line">        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)</span><br><span class="line">        at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2783)</span><br><span class="line">        at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2783)</span><br><span class="line">        at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)</span><br><span class="line">        at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)</span><br><span class="line">        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)</span><br><span class="line">        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)</span><br><span class="line">        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)</span><br><span class="line">        at org.apache.spark.sql.Dataset.collect(Dataset.scala:2783)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementInClientMode.execute(ExecuteStatementInClientMode.scala:184)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1$$anon$2.run(ExecuteStatementOperation.scala:74)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1$$anon$2.run(ExecuteStatementOperation.scala:70)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)</span><br><span class="line">        at yaooqinn.kyuubi.operation.statement.ExecuteStatementOperation$$anon$1.run(ExecuteStatementOperation.scala:70)</span><br><span class="line">        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">        at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)</span><br><span class="line">        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:89)</span><br><span class="line">        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)</span><br><span class="line">        at org.apache.spark.scheduler.Task.run(Task.scala:121)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)</span><br><span class="line">        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)</span><br><span class="line">        ... 3 more</span><br><span class="line">Caused by: org.apache.spark.SparkException: Failed to get broadcast_1_piece0 of broadcast_1</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)</span><br><span class="line">        at scala.collection.immutable.List.foreach(List.scala:392)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)</span><br><span class="line">        at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)</span><br><span class="line">        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="环境-部署kyuubi-120">环境 部署kyuubi-1.2.0</span></h2><p>kyuubi-1.2.0-bin-spark-3.0-hadoop2.7</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">bin/kyuubi start</span><br><span class="line">bin/kyuubi stop</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bin/kyuubi run (print in the foreground)</span><br><span class="line"></span><br><span class="line">jdbc:hive2://localhost:10009/</span><br><span class="line"></span><br><span class="line">beeline</span><br><span class="line"></span><br><span class="line">!connect jdbc:hive2://localhost:10009/;#spark.master=yarn;spark.executor.instances=2;spark.executor.cores=1;spark.executor.memory=512m</span><br><span class="line"></span><br><span class="line">bin/beeline -u &#x27;jdbc:hive2://localhost:10009/&#x27; -n zhouqingfeng</span><br><span class="line"></span><br><span class="line">//不支持在单个任务上配置kerberos | spark.kerberos.keytab and spark.kerberos.principal should not //use now.</span><br><span class="line"></span><br><span class="line">kinit -kt /home/demo/hive.keytab  spark/hd137@TEST.COM</span><br><span class="line"></span><br><span class="line">//submit to yarn</span><br><span class="line"></span><br><span class="line">jdbc:hive2://localhost:10009/;hive.server2.proxy.user=zhouqingfeng#spark.master=yarn;spark.submit.deployMode=client;spark.executor.instances=1;spark.executor.cores=1;spark.executor.memory=512m;spark.executor.heartbeatInterval=1000s;spark.network.timeout=1001s&quot;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="zookeeper">Zookeeper</span></h3><p><strong>第一步</strong></p>
<p>找到thrift jdbc 连接地址：connection url (可配置多个KyuubiServer，对应多个jdbc url实现服务的Ha)</p>
<p>ls &#x2F;kyuubi</p>
<p>[serviceUri&#x3D;localhost:10009;version&#x3D;1.2.0;sequence&#x3D;0000000000]</p>
<p><strong>第二步:</strong></p>
<p>找到user对应的spark_submit应用的监听端口，user对应的sql 会由第一步转交给该spark_submit应用处理</p>
<p>ls &#x2F;kyuubi_USER&#x2F;zhouqingfeng</p>
<p>[serviceUri&#x3D;localhost:57486;version&#x3D;1.2.0;sequence&#x3D;0000000001]</p>
<h3><span id="ha">HA</span></h3><p>jdbc:hive2:&#x2F;&#x2F;zk1:port1,zk2:port2&#x2F;test_zq;serviceDiscoveryMode&#x3D;zooKeeper;zooKeeperNamespace&#x3D;kyuubiserver;principal&#x3D;hive&#x2F;<a href="mailto:&#x62;&#100;&#112;&#x32;&#x40;&#x54;&#69;&#83;&#84;&#x2e;&#67;&#x4f;&#77;">&#x62;&#100;&#112;&#x32;&#x40;&#x54;&#69;&#83;&#84;&#x2e;&#67;&#x4f;&#77;</a>;authentication&#x3D;kerberos</p>
<p>kyuubi.ha.enabled</p>
<p>kyuubi.ha.zookeeper.quorum</p>
<p>kyuubi.ha.zookeeper.client.port</p>
<p>kyuubi.ha.zookeeper.namespace</p>
<p>–conf spark.kyuubi.ha.enabled&#x3D;true <br>–conf spark.kyuubi.ha.zk.quorum&#x3D;zk1:port1,zk2:port2 <br>–conf spark.kyuubi.ha.zk.namespace&#x3D;kyuubiserver \</p>
<p>–conf spark.kyuubi.ha.enabled&#x3D;true \</p>
<h3><span id="configuration">Configuration</span></h3><p>$SPARK_HOME&#x2F;conf  -&gt; $Kyuubi_HOME&#x2F;conf -&gt; jdbc url  (low -&gt; high)</p>
<p> <strong>kyuubi.session.engine.idle.timeout</strong></p>
<p>engine timeout, the engine will self-terminate when it’s not accessed for this duration</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kyuubi.engine.share.level  //CONNECTION  USER  SERVER</span><br><span class="line"></span><br><span class="line">Engines will be shared in different levels, available configs are:</span><br><span class="line">CONNECTION: engine will not be shared but only used by the current client connection</span><br><span class="line">USER: engine will be shared by all sessions created by a unique username, see also kyuubi.engine.share.level.sub.domain</span><br><span class="line">SERVER: the App will be shared by Kyuubi servers</span><br></pre></td></tr></table></figure>

<h3><span id="hive">Hive</span></h3><p>For example, Spark 3.0 was released with a builtin Hive client (2.3.7), so, ideally, the version of server should &gt;&#x3D; 2.3.x.</p>
<p>To prevent this problem, we can use Spark’s <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html#interacting-with-different-versions-of-hive-metastore">Interacting with Different Versions of Hive Metastore</a></p>
<h3><span id="source">Source</span></h3><h4><span id="kyuubiserver">KyuubiServer</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">KyuubiServer  -&gt; KyuubiBackendService + FrontendService</span><br><span class="line"></span><br><span class="line">1、KinitAuxiliaryService   -&gt; kyuubiserver kerberos 认证</span><br><span class="line">2、MetricsSystem  -&gt; 指标收集</span><br><span class="line">3、KyuubiBackendService -&gt; kyuubi 后端</span><br><span class="line">4、FrontendService  -&gt; kyuubi前端接收用户jdbc connection (TServerSocket)</span><br><span class="line">5、KyuubiServiceDiscovery -&gt; A service for service discovery used by kyuubi server side.</span><br><span class="line">(创建启动zkclient, 与zkserver 建立连接， 创建节点对应kyuubi服务)</span><br><span class="line"></span><br><span class="line">(1)</span><br><span class="line">21/07/13 14:23:00 INFO service.KinitAuxiliaryService: Service[KinitAuxiliaryService] is initialized.</span><br><span class="line">(2)</span><br><span class="line">21/07/13 14:30:04 INFO metrics.JsonReporterService: Service[JsonReporterService] is initialized.</span><br><span class="line">21/07/13 14:41:19 INFO metrics.MetricsSystem: Service[MetricsSystem] is initialized.</span><br><span class="line">(3)</span><br><span class="line">KyuubiBackendService</span><br><span class="line"></span><br><span class="line">-&gt; AbstractBackendService -&gt; KyuubiSessionManager -&gt; KyuubiOperationManager</span><br><span class="line"></span><br><span class="line">backend.server.exec.pool.size</span><br><span class="line">Number of threads in the operation execution thread pool of Kyuubi server</span><br><span class="line"></span><br><span class="line">backend.engine.exec.pool.size</span><br><span class="line">Number of threads in the operation execution thread pool of SQL engine applications</span><br><span class="line"></span><br><span class="line">initialized：</span><br><span class="line">21/07/13 15:00:54 INFO util.ThreadUtils: KyuubiSessionManager-exec-pool: pool size: 100, wait queue size: 100, thread keepalive time: 60000 m</span><br><span class="line">21/07/13 15:05:22 INFO operation.KyuubiOperationManager: Service[KyuubiOperationManager] is initialized</span><br><span class="line">21/07/13 15:06:30 INFO session.KyuubiSessionManager: Service[KyuubiSessionManager] is initialized.</span><br><span class="line">21/07/13 15:09:07 INFO server.KyuubiBackendService: Service[KyuubiBackendService] is initialized.</span><br><span class="line">(4) 21/07/13 15:23:18 INFO service.FrontendService: Initializing FrontendService on host localhost at port 10009 with [9, 999] worker threads</span><br><span class="line">21/07/13 15:23:52 INFO service.FrontendService: Service[FrontendService] is initialized.</span><br><span class="line">(5)</span><br><span class="line">21/07/13 15:45:26 INFO client.KyuubiServiceDiscovery: Service[ServiceDiscovery] is initialized.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">started:</span><br><span class="line">21/07/13 15:46:42 INFO service.KinitAuxiliaryService: Service[KinitAuxiliaryService] is started.</span><br><span class="line">21/07/13 15:59:33 INFO metrics.JsonReporterService: Service[JsonReporterService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:06:57 INFO operation.KyuubiOperationManager: Service[KyuubiOperationManager] is started.</span><br><span class="line">21/07/13 16:07:13 INFO session.KyuubiSessionManager: Service[KyuubiSessionManager] is started.</span><br><span class="line">21/07/13 16:07:13 INFO server.KyuubiBackendService: Service[KyuubiBackendService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:19:14 INFO service.FrontendService: Service[FrontendService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:23:48 INFO client.ServiceDiscovery: Created a /kyuubi/serviceUri=localhost:10009;version=1.2.0;sequence=0000000000 on ZooKeeper for KyuubiServer uri: localhost:10009</span><br><span class="line">21/07/13 16:24:41 INFO client.KyuubiServiceDiscovery: Service[ServiceDiscovery] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 16:25:27 INFO server.KyuubiServer: Service[KyuubiServer] is started.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="sparksqlengine">SparkSQLEngine</span></h4><p>提交jar到集群，获取资源 -&gt; 初始化服务  -&gt; 启动服务(FrontendService 对外暴露的地址注册到zk)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">SparkSQLEngine -&gt; SparkSQLBackendService + FrontendService + EngineServiceDiscovery</span><br><span class="line">(命令行独立进程里先提交任务， 再启动driver端上述三个服务)</span><br><span class="line"></span><br><span class="line">21/07/13 20:26:15 INFO ThreadUtils: SparkSQLSessionManager-exec-pool: pool size: 100, wait queue size: 100, thread keepalive time: 60000 ms</span><br><span class="line">21/07/13 20:27:22 INFO SparkSQLOperationManager: Service[SparkSQLOperationManager] is initialized.</span><br><span class="line">21/07/13 20:27:24 INFO SparkSQLSessionManager: Service[SparkSQLSessionManager] is initialized.</span><br><span class="line">21/07/13 20:27:25 INFO SparkSQLBackendService: Service[SparkSQLBackendService] is initialized.</span><br><span class="line">21/07/13 20:27:39 INFO FrontendService: Initializing FrontendService on host localhost at port 50764 with [9, 999] worker threads</span><br><span class="line">21/07/13 20:27:59 INFO FrontendService: Service[FrontendService] is initialized.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">21/07/13 20:28:29 INFO EngineServiceDiscovery: Service[ServiceDiscovery] is initialized.</span><br><span class="line">21/07/13 20:28:37 INFO SparkSQLEngine: Service[SparkSQLEngine] is initialized.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">21/07/13 20:34:33 INFO SparkSQLOperationManager: Service[SparkSQLOperationManager] is started.</span><br><span class="line">21/07/13 20:34:33 INFO SparkSQLSessionManager: Service[SparkSQLSessionManager] is started.</span><br><span class="line">21/07/13 20:34:33 INFO SparkSQLBackendService: Service[SparkSQLBackendService] is started.</span><br><span class="line">21/07/13 20:34:36 INFO FrontendService: Service[FrontendService] is started.</span><br><span class="line"></span><br><span class="line">21/07/13 20:35:12 INFO EngineServiceDiscovery: Zookeeper client connection state changed to: RECONNECTED</span><br><span class="line">21/07/13 20:35:12 INFO ServiceDiscovery: Created a /kyuubi_USER/zhouqingfeng/serviceUri=localhost:50764;version=1.2.0;sequence=0000000000 on ZooKeeper for KyuubiServer uri: localhost:50764</span><br><span class="line"></span><br><span class="line">21/07/13 20:35:12 INFO EngineServiceDiscovery: Service[ServiceDiscovery] is started.</span><br><span class="line">21/07/13 20:35:12 INFO SparkSQLEngine: Service[SparkSQLEngine] is started.</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SparkProcessBuilder</span><br></pre></td></tr></table></figure>



<h4><span id="servicediscovery">ServiceDiscovery</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ServiceDiscovery  服务发现</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">KyuubiServiceDiscovery</span><br><span class="line">A service for service discovery used by kyuubi server side. 找到kyuubiserver(默认10009端口)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">EngineServiceDiscovery   找到对应的spark-submit 任务 (地址+端口监听spark服务)</span><br><span class="line">A service for service discovery used by engine side. </span><br></pre></td></tr></table></figure>



<h3><span id="debug">Debug</span></h3><p>（<a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/technotes/guides/jpda/conninv.html#Transports">jpda调试工具</a>）</p>
<h4><span id="server-debug">server debug</span></h4><p>Kyuubi 启动脚本kyuubi添加：</p>
<p>-Xdebug -Xrunjdwp:transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5006</p>
<p>cmd&#x3D;”${RUNNER}  -Xdebug -Xrunjdwp:transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5006 ${KYUUBI_JAVA_OPTS}  -cp ${KYUUBI_CLASSPATH} $CLASS”</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5006</p>
<p>启动server,  debug idea remote, ok!</p>
<h4><span id="app-debug">App debug</span></h4><p>change kyuubi-defaults.conf.template to  kyuubi-defaults.conf:</p>
<p>kyuubi-defaults.conf add:</p>
<p>spark.driver.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>spark.executor.extraJavaOptions   -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;5007</p>
<p>IDEA remote add: -agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;n,address&#x3D;5007</p>
<p>正常启动server,</p>
<p>发起一个jdbc 连接，</p>
<p>debug idea remote, ok!</p>
<h2><span id="references">References</span></h2><p><a target="_blank" rel="noopener" href="https://github.com/yaooqinn/kyuubi/tree/master">Github</a></p>
<p><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">spark docs</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1440251">如何在CDH5中使用Spark2.4 Thrift</a></p>
<p><a target="_blank" rel="noopener" href="https://yaooqinn.github.io/kyuubi/docs/authentication.html">kyuubi 文档 适用与0.8版本(包括) 以前</a></p>
<p><a target="_blank" rel="noopener" href="https://kyuubi.readthedocs.io/en/stable/quick_start/quick_start.html#installation">kyuubi文档 使用于spark3.0 以后</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/354654936">知乎作者总结</a></p>
<p><a target="_blank" rel="noopener" href="https://kyuubi.readthedocs.io/en/stable/tools/debugging.html">1.* 文档</a></p>
<p><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-yarn.html#spark-properties">Spark official doc</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/05/bigdata/spark/sparksql/kyuubi/kyuubi_simple_usage/" data-id="cljp81lem0000239afmc93krp" data-title="kyuubi_simple_usage" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark3/20230704_spark_dateframe" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/spark/spark3/20230704_spark_dateframe/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T03:08:56.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/bigdata/spark/spark3/20230704_spark_dateframe/">20230704_spark_dateframe</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#example">Example</a><ul>
<li><a href="#wordcount">wordcount</a></li>
<li><a href="#sparklauncher">SparkLauncher</a></li>
</ul>
</li>
<li><a href="#dataset">DataSet</a><ul>
<li><a href="#dataset-api">Dataset API</a><ul>
<li><a href="#common">Common</a></li>
<li><a href="#cube-%E6%95%B0%E6%8D%AE%E7%AB%8B%E6%96%B9">Cube 数据立方</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sparksql">SparkSql</a><ul>
<li><a href="#group-by-%E5%88%86%E7%BB%84">Group by 分组</a><ul>
<li><a href="#cube%E7%94%A8%E6%B3%95">cube用法</a></li>
</ul>
</li>
<li><a href="#orgapachesparksqlfunctions">org.apache.spark.sql.functions</a><ul>
<li><a href="#collect_list">collect_list</a></li>
<li><a href="#min">min</a></li>
<li><a href="#percentile_approx">percentile_approx</a></li>
<li><a href="#kurtosis">kurtosis</a></li>
<li><a href="#product">Product</a></li>
<li><a href="#skewness">skewness</a></li>
<li><a href="#stddev_pop">stddev_pop</a></li>
<li><a href="#stddev">stddev</a></li>
<li><a href="#var_pop">var_pop</a></li>
<li><a href="#var_samp">var_samp</a></li>
<li><a href="#variance">variance</a></li>
<li><a href="#collection">Collection</a></li>
<li><a href="#%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0">日期函数</a></li>
<li><a href="#others">Others</a></li>
<li><a href="#%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%87%BD%E6%95%B0">字符串函数</a></li>
<li><a href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0">窗口函数</a></li>
<li><a href="#scalar-function">Scalar Function</a><ul>
<li><a href="#udf">UDF</a></li>
</ul>
</li>
<li><a href="#aggregate-function">Aggregate Function</a><ul>
<li><a href="#udaf">UDAF</a></li>
</ul>
</li>
<li><a href="#hive-udf">Hive UDF</a><ul>
<li><a href="#spark-%E5%AE%98%E6%96%B9hive-udf-example">Spark 官方hive udf Example</a></li>
<li><a href="#hive-udf-%E5%9C%A8hive%E4%B8%AD%E4%BD%BF%E7%94%A8">Hive UDF 在Hive中使用</a></li>
<li><a href="#hive-udf-%E5%9C%A8spark-%E4%B8%AD%E4%BD%BF%E7%94%A8">HIVE UDF 在Spark 中使用</a></li>
<li><a href="#hive-udf%E5%85%B6%E4%BB%96%E6%96%87%E6%A1%A3">HIVE UDF其他文档</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#dataset-%E7%94%9F%E6%88%90">Dataset 生成</a><ul>
<li><a href="#%E9%9B%86%E5%90%88%E7%94%9F%E6%88%90dataset">集合生成Dataset</a></li>
<li><a href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90dataset">加载数据集生成Dataset</a></li>
<li><a href="#rdd-%E7%94%9F%E6%88%90dataset">RDD 生成Dataset</a><ul>
<li><a href="#inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</a></li>
<li><a href="#%E7%BC%96%E7%A0%81%E6%8C%87%E5%AE%9Aschema">编码指定schema</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#spark-%E6%95%B0%E6%8D%AE%E6%BA%90">Spark 数据源</a><ul>
<li><a href="#bucket">Bucket</a><ul>
<li><a href="#%E5%88%86%E6%A1%B6%E5%88%92%E5%88%86">分桶划分</a></li>
<li><a href="#%E5%88%86%E6%A1%B6%E6%B5%8B%E8%AF%95">分桶测试</a></li>
<li><a href="#%E5%88%86%E6%A1%B6%E7%9B%AE%E7%9A%84">分桶目的</a><ul>
<li><a href="#%E8%81%9A%E5%90%88%E6%9F%A5%E8%AF%A2shuffle-%E8%A7%84%E9%81%BF%E6%B5%8B%E8%AF%95"><strong>聚合查询shuffle 规避测试</strong></a></li>
</ul>
</li>
<li><a href="#sql%E5%86%99%E6%B3%95">SQL写法</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
</ul>
</li>
<li><a href="#partition">Partition</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="example">Example</span></h2><h3><span id="wordcount">wordcount</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val textFile = spark.read.textFile(&quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/README.md&quot;)</span><br><span class="line"></span><br><span class="line">//    textFile.show()</span><br><span class="line">//    textFile.count()</span><br><span class="line">//    textFile.first()</span><br><span class="line">    //word最多的行有多少个word</span><br><span class="line">//    textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; if (a &gt; b) a else b)</span><br><span class="line">    //identity : scala predef预定义的一个函数，返回值等于传入参数</span><br><span class="line">    val wordCounts = textFile</span><br><span class="line">            .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">            .groupByKey(identity)</span><br><span class="line">            .count()</span><br><span class="line">            .selectExpr(&quot;key as value&quot;, &quot;`count(1)` as count&quot;)</span><br><span class="line"></span><br><span class="line">    wordCounts.printSchema()</span><br><span class="line">    wordCounts.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /* wordCounts2 等价于 上面的 wordCounts，</span><br><span class="line">       等价于sql中的 select value, count(1) count from tb1 group by value</span><br><span class="line">       等价于mapreduce中的wordcount</span><br><span class="line"></span><br><span class="line">    val wordCounts2 = textFile</span><br><span class="line">      .flatMap(line =&gt; line.split(&quot; &quot;))</span><br><span class="line">      .groupBy(&quot;value&quot;)</span><br><span class="line">      .count()</span><br><span class="line"></span><br><span class="line">    wordCounts2.show()</span><br><span class="line">     */</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="sparklauncher">SparkLauncher</span></h3><p>java代码里提交spark任务到指定集群，</p>
<p>（官方说法：The <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/launcher/package-summary.html">org.apache.spark.launcher</a> package provides classes for launching Spark jobs as child processes using a simple Java API.）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.launcher.SparkAppHandle;</span><br><span class="line">import org.apache.spark.launcher.SparkLauncher;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line"></span><br><span class="line">public class TestSparkLauncher &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; envParams = new HashMap&lt;&gt;();</span><br><span class="line">//        envParams.put(&quot;YARN_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">//        envParams.put(&quot;HADOOP_CONF_DIR&quot;, &quot;/Users/zhouqingfeng/Desktop/software/hadoop-2.7.7&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_HOME&quot;, &quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/&quot;);</span><br><span class="line">        envParams.put(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;, &quot;1&quot;);</span><br><span class="line">//        envParams.put(&quot;JAVA_HOME&quot;, &quot;/usr/java&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SparkLauncher launcher = new SparkLauncher(envParams)</span><br><span class="line">                .setAppResource(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;)</span><br><span class="line">                .setMainClass(&quot;org.apache.spark.examples.SparkPi&quot;)</span><br><span class="line">                .setMaster(&quot;local[2]&quot;)</span><br><span class="line">                .setDeployMode(&quot;client&quot;)</span><br><span class="line">                .addAppArgs(&quot;20&quot;);</span><br><span class="line"></span><br><span class="line">        String appName = &quot;sparklancherTest&quot;;</span><br><span class="line">        launcher.setConf(&quot;spark.executor.memory&quot;, &quot;1g&quot;);</span><br><span class="line">        launcher.setAppName( appName );</span><br><span class="line"></span><br><span class="line">//        launcher.addJar(&quot;/Users/zhouqingfeng/Desktop/software/spark-2.4.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.0.jar&quot;);</span><br><span class="line">//        launcher.redirectOutput(new File(&quot;/Users/zhouqingfeng/Desktop/mydirect/data/spark/redir/test1&quot;) );</span><br><span class="line"></span><br><span class="line">        SparkAppHandle sparkHandle = launcher.startApplication();</span><br><span class="line"></span><br><span class="line">        while(!sparkHandle.getState().isFinal()) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(1000);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                Thread.currentThread().interrupt();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2><span id="dataset">DataSet</span></h2><p>dataframe是dataset的一种特殊形式，所有元素类型被泛化为row（untyped）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type DataFrame = Dataset[org.apache.spark.sql.Row]</span><br></pre></td></tr></table></figure>





<h3><span id="dataset-api">Dataset API</span></h3><h4><span id="common">Common</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetUsageExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val df  = spark.read.json(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">    val ds1 = df.as[Person]</span><br><span class="line"></span><br><span class="line">    ds1.cache()</span><br><span class="line"></span><br><span class="line">//    ds1.checkpoint()</span><br><span class="line">    ds1.explain(true)</span><br><span class="line"></span><br><span class="line">    val ds2 = ds1.where(&quot;name = &#x27;Andy&#x27;&quot;)</span><br><span class="line">    //hint 提示词</span><br><span class="line">    ds1.join(ds2.hint(&quot;broadcast&quot;))</span><br><span class="line"></span><br><span class="line">    //ds1.repartition($&quot;age&quot;).write.save(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/out2/&quot;)</span><br><span class="line"></span><br><span class="line">    //添加一列</span><br><span class="line">    ds1.withColumn(&quot;age1&quot;, $&quot;age&quot; + 1 ).show</span><br><span class="line"></span><br><span class="line">    //重命名</span><br><span class="line">    ds1.select($&quot;name&quot;.as(&quot;name1&quot;))</span><br><span class="line"></span><br><span class="line">    //删除一列</span><br><span class="line">    ds1.withColumn(&quot;age1&quot;, $&quot;age&quot; + 1).drop(&quot;age&quot;)</span><br><span class="line"></span><br><span class="line">    //join</span><br><span class="line">    val ds3 = ds1</span><br><span class="line">    ds1.join(ds3, ds1.col(&quot;age&quot;) === ds3.col(&quot;age&quot;) &amp;&amp; ds1.col(&quot;name&quot;) === ds3.col(&quot;name&quot;), &quot;inner&quot;)</span><br><span class="line">       .show()</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    ds1.agg( max($&quot;age&quot;) ).show()</span><br><span class="line"></span><br><span class="line">    //statistics for numeric and string columns</span><br><span class="line">    ds1.describe(&quot;age&quot;)</span><br><span class="line">    ds1.summary()</span><br><span class="line"></span><br><span class="line">    // 替换字段中的null为11</span><br><span class="line">    ds1.na.fill(11).show</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="cube-数据立方">Cube 数据立方</span></h4><p>Cube  + rollup + pivot透视</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object DatasetCubeExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val df1 = Seq((&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)</span><br><span class="line">                 ).toDF(&quot;dept&quot;, &quot;sex&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line">    //rollup 分组,  这里等价于 group by 1 (不分组) + group by dept + group by dept, sex,  agg指明分组后的聚合函数</span><br><span class="line">    df1.rollup(&quot;dept&quot;, &quot;sex&quot;).agg(sum(&quot;salary&quot;).as(&quot;salary_sum&quot;)).show(20)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    分组聚合结果如下</span><br><span class="line">    +----+----+----------+</span><br><span class="line">    |dept| sex|salary_sum|</span><br><span class="line">    +----+----+----------+</span><br><span class="line">    |sale|null|       180|</span><br><span class="line">    |sale|   m|        85|</span><br><span class="line">    | dev|   m|       232|</span><br><span class="line">    | dev|null|       349|</span><br><span class="line">    |null|null|       529|</span><br><span class="line">    |sale|   f|        95|</span><br><span class="line">    | dev|   f|       117|</span><br><span class="line">    +----+----+----------+</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    //cube 分组,  表示取dept和sex两个维度，任意维度组合 分组</span><br><span class="line">    // 这里等价于 group by 1 (不分组) + group by dept + group by sex + group by dept, sex</span><br><span class="line">    df1.cube(&quot;dept&quot;, &quot;sex&quot;).agg(&quot;salary&quot; -&gt; &quot;sum&quot;).show(20)</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">    分组聚合结果如下</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">    |dept| sex|sum(salary)|</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">    |sale|null|        180|</span><br><span class="line">    |null|   m|        317|</span><br><span class="line">    |sale|   m|         85|</span><br><span class="line">    | dev|   m|        232|</span><br><span class="line">    | dev|null|        349|</span><br><span class="line">    |null|null|        529|</span><br><span class="line">    |null|   f|        212|</span><br><span class="line">    |sale|   f|         95|</span><br><span class="line">    | dev|   f|        117|</span><br><span class="line">    +----+----+-----------+</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    //分组之后的 数据透视pivot, group by dept分组之后, 以sex为透视列, sex的取值(m\f)作为透视结果表中列的名字</span><br><span class="line">    val df_pivot = df1.groupBy(&quot;dept&quot;)</span><br><span class="line">      .pivot(&quot;sex&quot;, Seq(&quot;m&quot;, &quot;f&quot;))</span><br><span class="line">      .sum(&quot;salary&quot;)</span><br><span class="line">    df_pivot.show()</span><br><span class="line">    /*</span><br><span class="line">    分组之后的透视结果如下, sex</span><br><span class="line">    +----+---+---+</span><br><span class="line">    |dept|  m|  f|</span><br><span class="line">    +----+---+---+</span><br><span class="line">    | dev|232|117|</span><br><span class="line">    |sale| 85| 95|</span><br><span class="line">    +----+---+---+</span><br><span class="line"></span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">    //数据反透视，数据透视的逆向</span><br><span class="line">    val df_unpivot = df_pivot.unpivot(Array($&quot;dept&quot;), Array($&quot;m&quot;, $&quot;f&quot;), &quot;sex&quot;, &quot;sum&quot;)</span><br><span class="line">    df_unpivot.show()</span><br><span class="line">    /*</span><br><span class="line">    逆向透视结果如下,</span><br><span class="line">    +----+---+---+</span><br><span class="line">    |dept|sex|sum|</span><br><span class="line">    +----+---+---+</span><br><span class="line">    | dev|  m|232|</span><br><span class="line">    | dev|  f|117|</span><br><span class="line">    |sale|  m| 85|</span><br><span class="line">    |sale|  f| 95|</span><br><span class="line">    +----+---+---+</span><br><span class="line"></span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>参考资料</strong>：</p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">spark dataset api</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html">org.apache.spark.sql.functions</a></p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html">spark中的透视函数</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36785151">excel数据透视表制作</a></p>
<h2><span id="sparksql">SparkSql</span></h2><h3><span id="group-by-分组">Group by 分组</span></h3><h4><span id="cube用法">cube用法</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">GROUP BY city, car_model WITH ROLLUP</span><br><span class="line">GROUP BY city, car_model WITH CUBE</span><br><span class="line">GROUP BY CUBE(city, car_model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE dealer (id INT, city STRING, car_model STRING, quantity INT);</span><br><span class="line">INSERT INTO dealer VALUES</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda Civic&#x27;, 10),</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda Accord&#x27;, 15),</span><br><span class="line">    (100, &#x27;Fremont&#x27;, &#x27;Honda CRV&#x27;, 7),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda Civic&#x27;, 20),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda Accord&#x27;, 10),</span><br><span class="line">    (200, &#x27;Dublin&#x27;, &#x27;Honda CRV&#x27;, 3),</span><br><span class="line">    (300, &#x27;San Jose&#x27;, &#x27;Honda Civic&#x27;, 5),</span><br><span class="line">    (300, &#x27;San Jose&#x27;, &#x27;Honda Accord&#x27;, 8);</span><br><span class="line"></span><br><span class="line">-- Sum of quantity per dealership. Group by `id`.</span><br><span class="line">SELECT id, sum(quantity) FROM dealer GROUP BY id ORDER BY id;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           32|</span><br><span class="line">|200|           33|</span><br><span class="line">|300|           13|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Use column position in GROUP by clause.</span><br><span class="line">SELECT id, sum(quantity) FROM dealer GROUP BY 1 ORDER BY 1;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           32|</span><br><span class="line">|200|           33|</span><br><span class="line">|300|           13|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Multiple aggregations.</span><br><span class="line">-- 1. Sum of quantity per dealership.</span><br><span class="line">-- 2. Max quantity per dealership.</span><br><span class="line">SELECT id, sum(quantity) AS sum, max(quantity) AS max FROM dealer GROUP BY id ORDER BY id;</span><br><span class="line">+---+---+---+</span><br><span class="line">| id|sum|max|</span><br><span class="line">+---+---+---+</span><br><span class="line">|100| 32| 15|</span><br><span class="line">|200| 33| 20|</span><br><span class="line">|300| 13|  8|</span><br><span class="line">+---+---+---+</span><br><span class="line"></span><br><span class="line">-- Count the number of distinct dealer cities per car_model.</span><br><span class="line">SELECT car_model, count(DISTINCT city) AS count FROM dealer GROUP BY car_model;</span><br><span class="line">+------------+-----+</span><br><span class="line">|   car_model|count|</span><br><span class="line">+------------+-----+</span><br><span class="line">| Honda Civic|    3|</span><br><span class="line">|   Honda CRV|    2|</span><br><span class="line">|Honda Accord|    3|</span><br><span class="line">+------------+-----+</span><br><span class="line"></span><br><span class="line">-- Sum of only &#x27;Honda Civic&#x27; and &#x27;Honda CRV&#x27; quantities per dealership.</span><br><span class="line">SELECT id, sum(quantity) FILTER (</span><br><span class="line">            WHERE car_model IN (&#x27;Honda Civic&#x27;, &#x27;Honda CRV&#x27;)</span><br><span class="line">        ) AS `sum(quantity)` FROM dealer</span><br><span class="line">    GROUP BY id ORDER BY id;</span><br><span class="line">+---+-------------+</span><br><span class="line">| id|sum(quantity)|</span><br><span class="line">+---+-------------+</span><br><span class="line">|100|           17|</span><br><span class="line">|200|           23|</span><br><span class="line">|300|            5|</span><br><span class="line">+---+-------------+</span><br><span class="line"></span><br><span class="line">-- Aggregations using multiple sets of grouping columns in a single statement.</span><br><span class="line">-- Following performs aggregations based on four sets of grouping columns.</span><br><span class="line">-- 1. city, car_model</span><br><span class="line">-- 2. city</span><br><span class="line">-- 3. car_model</span><br><span class="line">-- 4. Empty grouping set. Returns quantities for all city and car models.</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">    ORDER BY city;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|     null| HondaAccord| 33|</span><br><span class="line">|     null|    HondaCRV| 10|</span><br><span class="line">|     null|  HondaCivic| 35|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line">-- Group by processing with `ROLLUP` clause.</span><br><span class="line">-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), ())</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY city, car_model WITH ROLLUP</span><br><span class="line">    ORDER BY city, car_model;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line">-- Group by processing with `CUBE` clause.</span><br><span class="line">-- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ())</span><br><span class="line">SELECT city, car_model, sum(quantity) AS sum FROM dealer</span><br><span class="line">    GROUP BY city, car_model WITH CUBE</span><br><span class="line">    ORDER BY city, car_model;</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     city|   car_model|sum|</span><br><span class="line">+---------+------------+---+</span><br><span class="line">|     null|        null| 78|</span><br><span class="line">|     null| HondaAccord| 33|</span><br><span class="line">|     null|    HondaCRV| 10|</span><br><span class="line">|     null|  HondaCivic| 35|</span><br><span class="line">|   Dublin|        null| 33|</span><br><span class="line">|   Dublin| HondaAccord| 10|</span><br><span class="line">|   Dublin|    HondaCRV|  3|</span><br><span class="line">|   Dublin|  HondaCivic| 20|</span><br><span class="line">|  Fremont|        null| 32|</span><br><span class="line">|  Fremont| HondaAccord| 15|</span><br><span class="line">|  Fremont|    HondaCRV|  7|</span><br><span class="line">|  Fremont|  HondaCivic| 10|</span><br><span class="line">| San Jose|        null| 13|</span><br><span class="line">| San Jose| HondaAccord|  8|</span><br><span class="line">| San Jose|  HondaCivic|  5|</span><br><span class="line">+---------+------------+---+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--Prepare data for ignore nulls example</span><br><span class="line">CREATE TABLE person (id INT, name STRING, age INT);</span><br><span class="line">INSERT INTO person VALUES</span><br><span class="line">    (100, &#x27;Mary&#x27;, NULL),</span><br><span class="line">    (200, &#x27;John&#x27;, 30),</span><br><span class="line">    (300, &#x27;Mike&#x27;, 80),</span><br><span class="line">    (400, &#x27;Dan&#x27;, 50);</span><br><span class="line"></span><br><span class="line">--Select the first row in column age</span><br><span class="line">SELECT FIRST(age) FROM person;</span><br><span class="line">+--------------------+</span><br><span class="line">| first(age, false)  |</span><br><span class="line">+--------------------+</span><br><span class="line">| NULL               |</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">--Get the first row in column `age` ignore nulls,last row in column `id` and sum of column `id`.</span><br><span class="line">SELECT FIRST(age IGNORE NULLS), LAST(id), SUM(id) FROM person;</span><br><span class="line">+-------------------+------------------+----------+</span><br><span class="line">| first(age, true)  | last(id, false)  | sum(id)  |</span><br><span class="line">+-------------------+------------------+----------+</span><br><span class="line">| 30                | 400              | 1000     |</span><br><span class="line">+-------------------+------------------+----------+</span><br></pre></td></tr></table></figure>



<h3><span id="orgapachesparksqlfunctions">org.apache.spark.sql.functions</span></h3><p>sql: <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/sql/index.html">https://spark.apache.org/docs/latest/api/sql/index.html</a></p>
<p>Api: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html</span><br></pre></td></tr></table></figure>

<p>资料：<a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1">https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1</a></p>
<h4><span id="collect_list">collect_list</span></h4><p>collect_list | collect_set   返回数组ArrayType</p>
<p>ds1.groupBy(“name”).agg(collect_list(“age”).as(“lis”)).show</p>
<h4><span id="min">min</span></h4><p>min_by(c1, c2)  </p>
<p>返回c2取最小值时，c1对应的值</p>
<p>val df1 &#x3D; Seq((1, 322), (2, 211)).toDF(“val”, “id”)</p>
<p>df1.agg(min_by($”val”, $”id”)).show</p>
<h4><span id="percentile_approx">percentile_approx</span></h4><p>percentile_approx百分位数：</p>
<p>val df1 &#x3D; Seq(1, 2, 3,4,5,6,7,8,9,10).toDF(“val”)</p>
<p>df1.agg(percentile_approx( $”val”, lit(0.1), lit(1000))).show</p>
<h4><span id="kurtosis">kurtosis</span></h4><p>kurtosis）又称<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B3%B0%E6%80%81%E7%B3%BB%E6%95%B0/0?fromModule=lemma_inlink">峰态系数</a>。表征<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%A6%82%E7%8E%87/0?fromModule=lemma_inlink">概率</a><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%AF%86%E5%BA%A6%E5%88%86%E5%B8%83%E6%9B%B2%E7%BA%BF/485777?fromModule=lemma_inlink">密度分布曲线</a>在<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B9%B3%E5%9D%87%E5%80%BC/0?fromModule=lemma_inlink">平均值</a>处<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B3%B0%E5%80%BC/11008657?fromModule=lemma_inlink">峰值</a>高低的特征数。直观看来，峰度反映了峰部的尖度</p>
<h4><span id="product">Product</span></h4><p>分组内元素的乘积</p>
<p>val df1 &#x3D; Seq((1, 322), (2, 211)).toDF(“val”, “id”)</p>
<p>df1.agg(product($”id”)).show</p>
<h4><span id="skewness">skewness</span></h4><p>数据倾斜度</p>
<p>val df1 &#x3D; Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(“val”, “id”)</p>
<p>df1.agg(skewness($”val”)).show</p>
<p>df1.agg(skewness($”id”)).show</p>
<h4><span id="stddev_pop">stddev_pop</span></h4><p>整体标准差 seigema (方差&#x3D;标准差的平方 )</p>
<h4><span id="stddev">stddev</span></h4><p>样本标准差</p>
<h4><span id="var_pop">var_pop</span></h4><p>总体方差</p>
<h4><span id="var_samp">var_samp</span></h4><p>样本方差</p>
<h4><span id="variance">variance</span></h4><p>样本方差</p>
<h4><span id="collection">Collection</span></h4><p>集合函数， 作用对象是一个集合，也就是说，列对应数据类型是一个集合（array, list等）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">aggregate </span><br><span class="line"></span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy(&quot;val&quot;).agg(collect_list(&quot;id&quot;).as(&quot;lis&quot;)).select(aggregate(col(&quot;lis&quot;), lit(0), (acc, x) =&gt; acc + x).as(&quot;lis_sum&quot;), $&quot;val&quot;).show</span><br><span class="line"></span><br><span class="line">array_append</span><br><span class="line">数组追加</span><br><span class="line"></span><br><span class="line">array_distinct</span><br><span class="line">数组去重</span><br><span class="line"></span><br><span class="line">array_join</span><br><span class="line">数组元素  连接:Concatenates the elements of column using the delimiter</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(array_join($&quot;ids&quot;, &quot;||&quot;)).show</span><br><span class="line"></span><br><span class="line">exists  </span><br><span class="line">数组中是否存在满足条件的元素</span><br><span class="line">exists(column: Column, f: (Column) ⇒ Column)</span><br><span class="line">df.select(exists(col(&quot;i&quot;), _ % 2 === 0))</span><br><span class="line"></span><br><span class="line">explode(e: Column): Column</span><br><span class="line">一行变多行，数组中的每个元素对应一个新的行</span><br><span class="line">Creates a new row for each element in the given array or map column</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(explode($&quot;ids&quot;).as(&quot;id&quot;)).show</span><br><span class="line"></span><br><span class="line">filter</span><br><span class="line">数组元素过滤</span><br><span class="line">df.select(filter(col(&quot;s&quot;), (x, i) =&gt; i % 2 === 0)) </span><br><span class="line">i表示对应索引</span><br><span class="line">((col, index) =&gt; predicate, the Boolean predicate to filter the input column given the index. Indices start at 0.)</span><br><span class="line"></span><br><span class="line">get</span><br><span class="line">get(column: Column, index: Column)</span><br><span class="line">返回数组指定索引处的元素 Returns element of array at given (0-based) index.</span><br><span class="line">val df1 = Seq((1, 3), (1, 2), (1, 1), (2, 4)).toDF(&quot;val&quot;, &quot;id&quot;)</span><br><span class="line">df1.groupBy().agg(collect_list(&quot;id&quot;).as(&quot;ids&quot;)).select(get($&quot;ids&quot;, lit(1))).show</span><br><span class="line"></span><br><span class="line">get_json_object(e: Column, path: String)</span><br><span class="line">解析json字符串, 返回key对应值</span><br><span class="line">val df1 = Seq(&quot;&#123;\&quot;a\&quot;:1, \&quot;b\&quot;:2&#125;&quot;, &quot;&#123;\&quot;a\&quot;:11, \&quot;b\&quot;:22&#125;&quot; ).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(get_json_object($&quot;val&quot;, &quot;$.b&quot;)).show</span><br><span class="line"></span><br><span class="line">from_json(e: Column, schema: String, options: Map[String, String]): Column</span><br><span class="line">解析json字符串, 返回一个map (Parses a column containing a JSON string into a MapType)</span><br><span class="line">val df1 = Seq(&quot;&#123;\&quot;a\&quot;:1, \&quot;b\&quot;:2&#125;&quot;, &quot;&#123;\&quot;a\&quot;:11, \&quot;b\&quot;:22&#125;&quot; ).toDF(&quot;val&quot;)</span><br><span class="line">import scala.collection.JavaConverters._</span><br><span class="line">df1.select(from_json($&quot;val&quot;, &quot;a int, b int&quot;, Map.empty[String, String].asJava)).show</span><br><span class="line">df1.select(from_json($&quot;val&quot;, &quot;a int, b int&quot;, Map.empty[String, String].asJava).as(&quot;map1&quot;)).select($&quot;map1.b&quot;)</span><br><span class="line"></span><br><span class="line">json_tuple</span><br><span class="line">json_tuple(json: Column, fields: String*): Column</span><br><span class="line">返回tuple, 输入指定多个字段</span><br><span class="line">Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.</span><br><span class="line">df1.select(lit(0).as(&quot;cc11&quot;), json_tuple($&quot;val&quot;, &quot;b&quot;, &quot;a&quot;)).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">map_contains_key</span><br><span class="line">判断map是否包含key</span><br><span class="line">val df1 = Seq(Map(&quot;a&quot; -&gt; &quot;a&quot;, &quot;b&quot; -&gt; &quot;b&quot;), Map(&quot;aa&quot; -&gt; &quot;aa&quot;, &quot;bb&quot; -&gt; &quot;bb&quot;)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(map_contains_key($&quot;val&quot;, &quot;a&quot;)).show</span><br><span class="line"></span><br><span class="line">map_filter</span><br><span class="line">map进行过滤</span><br><span class="line">df1.select(map_filter(col(&quot;val&quot;), (k, v) =&gt; k !== &#x27;a&#x27;)).show</span><br><span class="line"></span><br><span class="line">map_zip_with</span><br><span class="line">两个map合并</span><br><span class="line"></span><br><span class="line">posexplode</span><br><span class="line">类似explode，一行变多行</span><br><span class="line">df1.select($&quot;val&quot;, posexplode($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">sequence </span><br><span class="line">产生一个序列</span><br><span class="line">df1.select(sequence(lit(1), lit(3)))</span><br><span class="line"></span><br><span class="line">shuffle</span><br><span class="line">产生一个随机排列的数组</span><br><span class="line">val df1 = Seq(Array(1, 2, 3), Array(4,5,6,7)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(shuffle($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">slice</span><br><span class="line">数组切片</span><br><span class="line">df1.select(slice($&quot;val&quot;, 1, 2)).show</span><br><span class="line"></span><br><span class="line">sort_array(e: Column, asc: Boolean)</span><br><span class="line">数组排序</span><br><span class="line"></span><br><span class="line">to_json</span><br><span class="line">Converts a column containing a StructType, ArrayType or a MapType into a JSON string</span><br><span class="line">val df1 = Seq(Map(&quot;a&quot; -&gt; &quot;a&quot;, &quot;b&quot; -&gt; &quot;b&quot;), Map(&quot;aa&quot; -&gt; &quot;aa&quot;, &quot;bb&quot; -&gt; &quot;bb&quot;)).toDF(&quot;val&quot;)</span><br><span class="line">df1.select(to_json($&quot;val&quot;)).show</span><br><span class="line"># 行多个字段 转成json字符串</span><br><span class="line">select to_json(named_struct(&quot;salary&quot;, salary, &quot;dept&quot;, department)) from emp;</span><br><span class="line"></span><br><span class="line">zip_with</span><br><span class="line">按元素合并两个数组</span><br><span class="line">df.select(zip_with(df1(&quot;val1&quot;), df1(&quot;val2&quot;), (x, y) =&gt; x + y))</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h4><span id="日期函数">日期函数</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">val df1 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line"></span><br><span class="line">df1.select($&quot;val&quot;, current_date().as(&quot;curd&quot;), current_timestamp.as(&quot;curt&quot;)).select(add_months($&quot;curd&quot;, 2)).show</span><br><span class="line"></span><br><span class="line">select(date_add($&quot;curd&quot;, 2))</span><br><span class="line"></span><br><span class="line">date_sub</span><br><span class="line">date_diff</span><br><span class="line"></span><br><span class="line">date_format</span><br><span class="line"></span><br><span class="line">窗口函数 </span><br><span class="line">window(timeColumn: Column, windowDuration: String) 滚动窗口  窗口长度 size固定, slide = size</span><br><span class="line">window(timeColumn: Column, windowDuration: String, slideDuration: String) 滑动窗口</span><br><span class="line">滑动窗口以一个步长（Slide）不断向前滑动，窗口的长度size固定, slide &lt; size</span><br><span class="line"></span><br><span class="line">session_window(timeColumn: Column, gapDuration: String) 会话窗口</span><br><span class="line">两个窗口之间有一个间隙，被称为Session Gap。当一个窗口在大于Session Gap的时间内没有接收到新数据时，窗口将关闭。在这种模式下，窗口的长度是可变的，每个窗口的开始和结束时间并不是确定的。</span><br></pre></td></tr></table></figure>

<h4><span id="others">Others</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">def coalesce(e: Column*): Column</span><br><span class="line">返回非空的列对应值</span><br><span class="line">Returns the first column that is not null, or null if all inputs are null.</span><br><span class="line"></span><br><span class="line">def col(colName: String): Column</span><br><span class="line">取对应列</span><br><span class="line"></span><br><span class="line">expr(expr: String): Column</span><br><span class="line">表达式转换为列</span><br><span class="line">df.groupBy(expr(&quot;length(word)&quot;)).count()</span><br><span class="line"></span><br><span class="line">lit(literal: Any): Column</span><br><span class="line">创建一个常量类型的列</span><br><span class="line">Creates a Column of literal value.</span><br><span class="line"></span><br><span class="line">monotonically_increasing_id()</span><br><span class="line">产生一个单调递增列，保证唯一性，但整体不保证是连续的</span><br><span class="line">val df1 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line">df1.select($&quot;val&quot;, monotonically_increasing_id()).show</span><br><span class="line"></span><br><span class="line">negate</span><br><span class="line">取负值</span><br><span class="line">df1.select(negate($&quot;val&quot;)).show</span><br><span class="line"></span><br><span class="line">rand()</span><br><span class="line">随机函数</span><br><span class="line"></span><br><span class="line">_*使用</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select(df1.columns.map(c =&gt; col(c)):_*)</span><br><span class="line"></span><br><span class="line">when使用</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select(when(col(&quot;id&quot;) &gt; 1, 1).otherwise(0), $&quot;id&quot;).show</span><br><span class="line"></span><br><span class="line">struct(colName: String, colNames: String*)</span><br><span class="line">Creates a new struct column  创建一个struct列</span><br><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select($&quot;id&quot;, struct(&quot;id&quot;, &quot;mark&quot;)).printSchema</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h4><span id="字符串函数">字符串函数</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html</span><br></pre></td></tr></table></figure>

<p>concat_ws(sep: String, exprs: <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html">Column</a>*)</p>
<p>字符串连接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val df1 = Seq((1, &quot;a&quot;), (2, &quot;b&quot;)).toDF(&quot;id&quot;, &quot;mark&quot;)</span><br><span class="line">df1.select($&quot;id&quot;, concat_ws(&quot;|&quot;, $&quot;id&quot;, $&quot;mark&quot;)).show</span><br><span class="line"></span><br><span class="line">instr(str: Column, substring: String)</span><br><span class="line">查找substring 位置，如果未找到返回0</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="窗口函数">窗口函数</span></h4><p>关于hive的窗口函数 是类似的，<a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2015/04/190.htm">参考资料hive分析窗口函数</a></p>
<p><a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-sql-window-functions/?expand_article=1">spark窗口函数example</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">row_number()  ( row_number() over(partition by a order by b))</span><br><span class="line">dense_rank()   1 2 2  3  排名rank不会跳级</span><br><span class="line">rank()         1 2 2 4 排名rank会跳级，如果有相等排名</span><br><span class="line">percent_rank()</span><br><span class="line">cume_dist()  //累加分布</span><br><span class="line">ntile()   //窗口分组</span><br><span class="line">lag</span><br><span class="line">lead</span><br><span class="line">nth_value  //当前这一行能看到的窗口对应的第n个值</span><br></pre></td></tr></table></figure>





<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.expressions.Window</span><br><span class="line"></span><br><span class="line">object DatasetWindowExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Michael&quot;, &quot;Sales&quot;, 4600),</span><br><span class="line">      (&quot;Robert&quot;, &quot;Sales&quot;, 4100),</span><br><span class="line">      (&quot;Maria&quot;, &quot;Finance&quot;, 3000),</span><br><span class="line">      (&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Scott&quot;, &quot;Finance&quot;, 3300),</span><br><span class="line">      (&quot;Jen&quot;, &quot;Finance&quot;, 3900),</span><br><span class="line">      (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000),</span><br><span class="line">      (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000),</span><br><span class="line">      (&quot;Saif&quot;, &quot;Sales&quot;, 4100)</span><br><span class="line">    )</span><br><span class="line">    val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    //row_number</span><br><span class="line">    val windowSpec  = Window.partitionBy(&quot;department&quot;).orderBy($&quot;salary&quot;)</span><br><span class="line">    df.withColumn(&quot;row_number&quot;,row_number().over(windowSpec))</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line">    df.withColumn(&quot;dense_rank&quot;,dense_rank().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    df.withColumn(&quot;rank&quot;,rank().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //returns the relative rank (i.e. percentile) of rows within a window partition. 相对排名(取值0到1)</span><br><span class="line">    df.withColumn(&quot;percent_rank&quot;,percent_rank().over(windowSpec)).show()</span><br><span class="line"></span><br><span class="line">    //ntile(2) 表示将窗口 2等分, 对应的分组id，前一部分数据对应分组id 1，后一部分数据对应分组id 2</span><br><span class="line">    df.withColumn(&quot;ntile&quot;,ntile(2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    // 窗口累加值分布</span><br><span class="line">    //   * Window function: returns the cumulative distribution of values within a window partition,</span><br><span class="line">    //   * i.e. the fraction of rows that are below the current row.</span><br><span class="line">    df.withColumn(&quot;cume_dist&quot;,cume_dist().over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //当前行之前 2 行 对应列的值</span><br><span class="line">    df.withColumn(&quot;lag&quot;,lag(&quot;salary&quot;,2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    //当前行之后 2 行 对应列的值</span><br><span class="line">    df.withColumn(&quot;lead&quot;,lead(&quot;salary&quot;,2).over(windowSpec)).show</span><br><span class="line"></span><br><span class="line">    val windowSpecAgg  = Window.partitionBy(&quot;department&quot;)</span><br><span class="line">    val aggDF = df.withColumn(&quot;row&quot;,row_number.over(windowSpec))</span><br><span class="line">      .withColumn(&quot;avg&quot;, avg(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;sum&quot;, sum(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;min&quot;, min(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .withColumn(&quot;max&quot;, max(col(&quot;salary&quot;)).over(windowSpecAgg))</span><br><span class="line">      .where(col(&quot;row&quot;)===1).select(&quot;department&quot;,&quot;avg&quot;,&quot;sum&quot;,&quot;min&quot;,&quot;max&quot;)</span><br><span class="line">      .show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.groupBy(&quot;department&quot;).agg(</span><br><span class="line">      avg($&quot;salary&quot;).as(&quot;avg&quot;),</span><br><span class="line">      sum($&quot;salary&quot;).as(&quot;sum&quot;),</span><br><span class="line">      min($&quot;salary&quot;).as(&quot;min&quot;),</span><br><span class="line">      max($&quot;salary&quot;).as(&quot;max&quot;)).show</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="scalar-function">Scalar Function</span></h4><p>标量函数</p>
<p>一行返回一个值</p>
<p>Scalar functions are functions that return a single value per row</p>
<p>系统内置( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions.html#scalar-functions">Built-in Scalar Functions</a>) +  用户定义UDF ( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html">User Defined Scalar Functions</a>.)</p>
<h5><span id="udf">UDF</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions.udf</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark SQL UDF scalar example&quot;)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">// Define and register a zero-argument non-deterministic UDF</span><br><span class="line">// UDF is deterministic by default, i.e. produces the same result for the same input.</span><br><span class="line">val random = udf(() =&gt; Math.random())</span><br><span class="line">spark.udf.register(&quot;random&quot;, random.asNondeterministic())</span><br><span class="line">spark.sql(&quot;SELECT random()&quot;).show()</span><br><span class="line">// +-------+</span><br><span class="line">// |UDF()  |</span><br><span class="line">// +-------+</span><br><span class="line">// |xxxxxxx|</span><br><span class="line">// +-------+</span><br><span class="line"></span><br><span class="line">// Define and register a one-argument UDF</span><br><span class="line">val plusOne = udf((x: Int) =&gt; x + 1)</span><br><span class="line">spark.udf.register(&quot;plusOne&quot;, plusOne)</span><br><span class="line">spark.sql(&quot;SELECT plusOne(5)&quot;).show()</span><br><span class="line">// +------+</span><br><span class="line">// |UDF(5)|</span><br><span class="line">// +------+</span><br><span class="line">// |     6|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Define a two-argument UDF and register it with Spark in one step</span><br><span class="line">spark.udf.register(&quot;strLenScala&quot;, (_: String).length + (_: Int))</span><br><span class="line">spark.sql(&quot;SELECT strLenScala(&#x27;test&#x27;, 1)&quot;).show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |strLenScala(test, 1)|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |                   5|</span><br><span class="line">// +--------------------+</span><br><span class="line"></span><br><span class="line">// UDF in a WHERE clause</span><br><span class="line">spark.udf.register(&quot;oneArgFilter&quot;, (n: Int) =&gt; &#123; n &gt; 5 &#125;)</span><br><span class="line"></span><br><span class="line">//注册成临时表</span><br><span class="line">spark.range(1, 10).createOrReplaceTempView(&quot;test&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM test WHERE oneArgFilter(id)&quot;).show()</span><br><span class="line">// +---+</span><br><span class="line">// | id|</span><br><span class="line">// +---+</span><br><span class="line">// |  6|</span><br><span class="line">// |  7|</span><br><span class="line">// |  8|</span><br><span class="line">// |  9|</span><br><span class="line">// +---+</span><br></pre></td></tr></table></figure>



<h4><span id="aggregate-function">Aggregate  Function</span></h4><p>聚合函数</p>
<p>多行返回一个值</p>
<p>return a single value on a group of rows</p>
<p>系统内置( <a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions">Built-in Aggregation Functions</a>) + 用户定义UDAF(<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-aggregate.html">User Defined Aggregate Functions</a>.)</p>
<h5><span id="udaf">UDAF</span></h5><p>Reduce  汇总、汇聚</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Type-Safe User-Defined Aggregate Functions</span><br></pre></td></tr></table></figure>

<p>针对dataset操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.expressions.Aggregator</span><br><span class="line"></span><br><span class="line">case class Employee(name: String, salary: Long)</span><br><span class="line">case class Average(var sum: Long, var count: Long)</span><br><span class="line"></span><br><span class="line">object MyAverage extends Aggregator[Employee, Average, Double] &#123;</span><br><span class="line">  // A zero value for this aggregation. Should satisfy the property that any b + zero = b</span><br><span class="line">  def zero: Average = Average(0L, 0L)</span><br><span class="line">  // Combine two values to produce a new value. For performance, the function may modify `buffer`</span><br><span class="line">  // and return it instead of constructing a new object</span><br><span class="line">  def reduce(buffer: Average, employee: Employee): Average = &#123;</span><br><span class="line">    buffer.sum += employee.salary</span><br><span class="line">    buffer.count += 1</span><br><span class="line">    buffer</span><br><span class="line">  &#125;</span><br><span class="line">  // Merge two intermediate values</span><br><span class="line">  def merge(b1: Average, b2: Average): Average = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  // Transform the output of the reduction</span><br><span class="line">  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count</span><br><span class="line">  // Specifies the Encoder for the intermediate value type</span><br><span class="line">  def bufferEncoder: Encoder[Average] = Encoders.product</span><br><span class="line">  // Specifies the Encoder for the final output value type</span><br><span class="line">  def outputEncoder: Encoder[Double] = Encoders.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val ds = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;).as[Employee]</span><br><span class="line">ds.show()</span><br><span class="line">// +-------+------+</span><br><span class="line">// |   name|salary|</span><br><span class="line">// +-------+------+</span><br><span class="line">// |Michael|  3000|</span><br><span class="line">// |   Andy|  4500|</span><br><span class="line">// | Justin|  3500|</span><br><span class="line">// |  Berta|  4000|</span><br><span class="line">// +-------+------+</span><br><span class="line"></span><br><span class="line">// Convert the function to a `TypedColumn` and give it a name</span><br><span class="line">val averageSalary = MyAverage.toColumn.name(&quot;average_salary&quot;)</span><br><span class="line">val result = ds.select(averageSalary)</span><br><span class="line">result.show()</span><br><span class="line">// +--------------+</span><br><span class="line">// |average_salary|</span><br><span class="line">// +--------------+</span><br><span class="line">// |        3750.0|</span><br><span class="line">// +--------------+</span><br></pre></td></tr></table></figure>

<p>针对DataFrame操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Untyped User-Defined Aggregate Functions</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.expressions.Aggregator</span><br><span class="line">import org.apache.spark.sql.functions</span><br><span class="line"></span><br><span class="line">case class Average(var sum: Long, var count: Long)</span><br><span class="line"></span><br><span class="line">object MyAverage extends Aggregator[Long, Average, Double] &#123;</span><br><span class="line">  // A zero value for this aggregation. Should satisfy the property that any b + zero = b</span><br><span class="line">  def zero: Average = Average(0L, 0L)</span><br><span class="line">  // Combine two values to produce a new value. For performance, the function may modify `buffer`</span><br><span class="line">  // and return it instead of constructing a new object</span><br><span class="line">  def reduce(buffer: Average, data: Long): Average = &#123;</span><br><span class="line">    buffer.sum += data</span><br><span class="line">    buffer.count += 1</span><br><span class="line">    buffer</span><br><span class="line">  &#125;</span><br><span class="line">  // Merge two intermediate values</span><br><span class="line">  def merge(b1: Average, b2: Average): Average = &#123;</span><br><span class="line">    b1.sum += b2.sum</span><br><span class="line">    b1.count += b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line">  // Transform the output of the reduction</span><br><span class="line">  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count</span><br><span class="line">  // Specifies the Encoder for the intermediate value type</span><br><span class="line">  def bufferEncoder: Encoder[Average] = Encoders.product</span><br><span class="line">  // Specifies the Encoder for the final output value type</span><br><span class="line">  def outputEncoder: Encoder[Double] = Encoders.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Register the function to access it</span><br><span class="line">spark.udf.register(&quot;myAverage&quot;, functions.udaf(MyAverage))</span><br><span class="line"></span><br><span class="line">val df = spark.read.json(&quot;examples/src/main/resources/employees.json&quot;)</span><br><span class="line">df.createOrReplaceTempView(&quot;employees&quot;)</span><br><span class="line">df.show()</span><br><span class="line">// +-------+------+</span><br><span class="line">// |   name|salary|</span><br><span class="line">// +-------+------+</span><br><span class="line">// |Michael|  3000|</span><br><span class="line">// |   Andy|  4500|</span><br><span class="line">// | Justin|  3500|</span><br><span class="line">// |  Berta|  4000|</span><br><span class="line">// +-------+------+</span><br><span class="line"></span><br><span class="line">val result = spark.sql(&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;)</span><br><span class="line">result.show()</span><br><span class="line">// +--------------+</span><br><span class="line">// |average_salary|</span><br><span class="line">// +--------------+</span><br><span class="line">// |        3750.0|</span><br><span class="line">// +--------------+</span><br></pre></td></tr></table></figure>

<h4><span id="hive-udf">Hive UDF</span></h4><p>Spark SQL supports integration of Hive UDFs, UDAFs and UDTFs. Similar to Spark UDFs and UDAFs, Hive UDFs work on a single row as input and generate a single row as output, while Hive UDAFs operate on multiple rows and return a single aggregated row as a result. In addition, Hive also supports UDTFs (User Defined Tabular Functions) that act on one row as input and return multiple rows as output. </p>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-hive.html">spark 官方</a></p>
<h5><span id="spark-官方hive-udf-example">Spark 官方hive udf Example</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">-- Register `GenericUDFAbs` and use it in Spark SQL.</span><br><span class="line">-- Note that, if you use your own programmed one, you need to add a JAR containing it</span><br><span class="line">-- into a classpath,</span><br><span class="line">-- e.g., ADD JAR yourHiveUDF.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION testUDF AS &#x27;org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs&#x27;;</span><br><span class="line"></span><br><span class="line">SELECT * FROM t;</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">| -1.0|</span><br><span class="line">|  2.0|</span><br><span class="line">| -3.0|</span><br><span class="line">+-----+</span><br><span class="line"></span><br><span class="line">SELECT testUDF(value) FROM t;</span><br><span class="line">+--------------+</span><br><span class="line">|testUDF(value)|</span><br><span class="line">+--------------+</span><br><span class="line">|           1.0|</span><br><span class="line">|           2.0|</span><br><span class="line">|           3.0|</span><br><span class="line">+--------------+</span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf-在hive中使用">Hive UDF 在Hive中使用</span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1 创建一个class 继承UDF class，实现evaluate 方法</span><br><span class="line">(you need to create a new class that extends UDF, with one or more methods named evaluate)</span><br><span class="line"></span><br><span class="line">2 把项目打成jar包，将jar包添加到hive classpath</span><br><span class="line">(After compiling your code to a jar, you need to add this to the Hive classpath)</span><br><span class="line"></span><br><span class="line">3 注册一个函数(可以是临时的、永久的) 对应用户定义的class (register your function as described)</span><br><span class="line">之后，就可以像内置函数一样，使用udf了</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hive0.13似乎支持，注册UDF时 指定用户jar path， for example:</span><br><span class="line"></span><br><span class="line">As of Hive 0.13, UDFs also have the option of being able to specify required jars in the CREATE FUNCTION statement:</span><br><span class="line">`CREATE FUNCTION myfunc AS ``&#x27;myclass&#x27;` `USING JAR ``&#x27;hdfs:///path/to/jar&#x27;``;`</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf-在spark-中使用">HIVE  UDF 在Spark 中使用</span></h5><p>定义class GeohashUtilty 继承udf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class GeohashUtilty extends UDF &#123;</span><br><span class="line"></span><br><span class="line">    public String evaluate(double lat, double lon, int precision) &#123;</span><br><span class="line">        return getGeoHashString(lat, lon, precision);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //get geohash string with precision n</span><br><span class="line">    public static String getGeoHashString(double lat, double lon, int precision) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            GeoHash geoHash = GeoHash.withCharacterPrecision(lat, lon, precision);</span><br><span class="line">            String geoHashString = geoHash.toBase32();</span><br><span class="line">            return geoHashString;</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(1) 经纬度获取geohash值，用于判断哪些点在同一个区域内</span><br><span class="line">spark sql 用法:</span><br><span class="line">add jar /home/etl/until/GenericUDF-1.0-jar-with-dependencies.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION geohashVal AS &#x27;com.weidai.udf.generic.GeohashUtilty&#x27;;</span><br><span class="line">select geohashVal( 28.561104, 121.186142, 8 );</span><br><span class="line">最后一位表示hash值的精度，既字符串长度，上述结果为字符串 wtn4mxmh</span><br><span class="line"></span><br><span class="line">当两个点的geohash值相同，表示这两个点在同一个区域内</span><br><span class="line">geohash值取8位长度，表示区域范围38.2*19m，</span><br><span class="line">geohash值取7位长度，表示区域范围152.9*152.4m，</span><br><span class="line">geohash值取6位长度，表示区域范围1200*609m，</span><br><span class="line">geohash值取5位长度，表示区域范围4.9km*4.9km，</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(2) 经纬度 转换为 地址 （调用百度api, 调用次数有限制，一天不要超过10万次）</span><br><span class="line">spark sql  用法:</span><br><span class="line">add jar /home/etl/until/GenericUDF-1.0-jar-with-dependencies.jar;</span><br><span class="line">CREATE TEMPORARY FUNCTION latlonToAddr AS &#x27;com.weidai.udf.generic.LatLonToAddress&#x27;;</span><br><span class="line">select latlonToAddr( 28.561104, 121.186142 );</span><br></pre></td></tr></table></figure>



<h5><span id="hive-udf其他文档">HIVE UDF其他文档</span></h5><p><strong>hive SQL 目前支持 三种用户自定义的function: UDF, UDAF, UDTF</strong></p>
<p><strong>UDF</strong>: 输入对应一行，输出对应一行，一对一关系</p>
<p>UDFs works on a single row in a table and produces a single row as output. Its one to one relationship between input and output of a function， e.g Hive built in TRIM() function</p>
<p><strong>UDAF</strong>：<strong>User Defined Aggregate Function</strong></p>
<p>输入多行，输出1行，关系对应是多对1</p>
<p>User defined aggregate functions works on more than one row and gives single row as output. e.g Hive built in MAX() or COUNT() functions. here the relation is many to one</p>
<p><strong>UDTF</strong></p>
<p>输入一行，输出多行，关系对应是1对多。UDTF can be used to split a column into multiple column。 </p>
<p>User defined tabular function works on one row as input and returns multiple rows as output. So here the relation in one to many. e.g Hive built in EXPLODE() function(输入一行，输出多行). </p>
<p><strong>references</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.linkedin.com/pulse/hive-functions-udfudaf-udtf-examples-gaurav-singh">linked in的一篇blog：Hive Functions – UDF,UDAF and UDTF with Examples</a></p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inOperators">hive wiki: hive built-in function</a></p>
<p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">Hive wiki: Creating Custom UDFs</a></p>
<h2><span id="dataset-生成">Dataset 生成</span></h2><h3><span id="集合生成dataset">集合生成Dataset</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val df1 = Seq((&quot;dev&quot;, &quot;m&quot;, 115),(&quot;sale&quot;, &quot;f&quot;, 95), (&quot;sale&quot;, &quot;m&quot;, 85), (&quot;dev&quot;, &quot;f&quot;, 117), (&quot;dev&quot;, &quot;m&quot;, 117)).toDF(&quot;dept&quot;, &quot;sex&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line">val df2 = Seq(1, 2, 3).toDF(&quot;val&quot;)</span><br><span class="line"></span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">// Encoders are created for case classes</span><br><span class="line">val caseClassDS = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"></span><br><span class="line">val rdd1 = spark.range(1, 10)</span><br><span class="line">#生成rdd1, rdd1: org.apache.spark.sql.Dataset[Long] = [id: bigint]</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3><span id="加载数据集生成dataset">加载数据集生成Dataset</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span><br><span class="line">import spark.implicits._</span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDS = spark.read.json(path).as[Person]</span><br><span class="line">peopleDS.show()</span><br></pre></td></tr></table></figure>



<h3><span id="rdd-生成dataset">RDD 生成Dataset</span></h3><h4><span id="inferring-the-schema-using-reflection">Inferring the Schema Using Reflection</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于反射推测schema</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line">object DatasetExample2 &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val peopleDF  = spark.sparkContext</span><br><span class="line">      .textFile(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.txt&quot;)</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt)).toDF()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">    //dsl api</span><br><span class="line">    peopleDF.select( $&quot;name&quot;, $&quot;age&quot;, concat_ws(&quot;&quot;, lit(&quot;Name: &quot;), $&quot;name&quot;) ).show()</span><br><span class="line"></span><br><span class="line">    //函数式api The columns of a row in the result can be accessed by field index</span><br><span class="line">    peopleDF.map(teenager =&gt; &quot;Name: &quot; + teenager(0)).show()</span><br><span class="line"></span><br><span class="line">    //函数式api The columns of a row in the result can be accessed by field name</span><br><span class="line">    peopleDF.map(teenager =&gt; &quot;Name: &quot; + teenager.getAs[String](&quot;name&quot;)).show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    // No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span><br><span class="line">    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]</span><br><span class="line">    // Primitive types and case classes can be also defined as</span><br><span class="line">    // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span><br><span class="line"></span><br><span class="line">    // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span><br><span class="line">    peopleDF.map(teenager =&gt; teenager.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="编码指定schema">编码指定schema</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.&#123;Row, SparkSession&#125;</span><br><span class="line">import org.apache.spark.sql.functions._</span><br><span class="line">import org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;</span><br><span class="line"></span><br><span class="line">object DatasetExample3 &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(name: String, age: Long)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val peopleRDD  = spark.sparkContext</span><br><span class="line">      .textFile(&quot;file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.txt&quot;)</span><br><span class="line"></span><br><span class="line">    // The schema is encoded in a string</span><br><span class="line">    val schemaString = &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">    // Generate the schema based on the string of schema</span><br><span class="line">    val fields = schemaString.split(&quot; &quot;)</span><br><span class="line">      .map(fieldName =&gt; StructField(fieldName, StringType, nullable = true))</span><br><span class="line">    val schema = StructType(fields)</span><br><span class="line"></span><br><span class="line">    // Convert records of the RDD (people) to Rows</span><br><span class="line">    val rowRDD = peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes =&gt; Row(attributes(0), attributes(1).trim))</span><br><span class="line"></span><br><span class="line">    // Apply the schema to the RDD</span><br><span class="line">    val peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.printSchema()</span><br><span class="line"></span><br><span class="line">    peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">    // SQL can be run over a temporary view created using DataFrames</span><br><span class="line">    val results = spark.sql(&quot;SELECT name FROM people&quot;)</span><br><span class="line"></span><br><span class="line">    // The results of SQL queries are DataFrames and support all the normal RDD operations</span><br><span class="line">    // The columns of a row in the result can be accessed by field index or by field name</span><br><span class="line">    results.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2><span id="spark-数据源">Spark 数据源</span></h2><p>DataSource</p>
<p>通用写法指定fileformat，比如parquet|orc|json|csv|avro，默认读写格式都为parquet</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># read format | write format</span><br><span class="line">val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># read option | write option</span><br><span class="line">val peopleDFCsv = spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;examples/src/main/resources/people.csv&quot;)</span><br><span class="line">  </span><br><span class="line">usersDF.write.format(&quot;orc&quot;)</span><br><span class="line">  .option(&quot;orc.bloom.filter.columns&quot;, &quot;favorite_color&quot;)</span><br><span class="line">  .option(&quot;orc.dictionary.key.threshold&quot;, &quot;1.0&quot;)</span><br><span class="line">  .option(&quot;orc.column.encoding.direct&quot;, &quot;name&quot;)</span><br><span class="line">  .save(&quot;users_with_options.orc&quot;)  </span><br><span class="line"></span><br><span class="line"># 直接通过sql 读取文件</span><br><span class="line">val sqlDF = spark.sql(&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;)</span><br><span class="line"></span><br><span class="line">SELECT * FROM parquet.`file:/Users/zhouqingfeng/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.parquet`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#saveAsTable</span><br><span class="line">val df  = spark.read.parquet(&quot;file:/Users/**/Desktop/software/spark3/spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">df.write.mode(&quot;overwrite&quot;).saveAsTable(&quot;test_zhou.user1&quot;)</span><br><span class="line">spark.table(&quot;test_zhou.user1&quot;).show</span><br><span class="line">spark.sql(&quot;drop table test_zhou.user1&quot;)   </span><br><span class="line">#默认保存warehouse配置路径，drop table删除表之后，数据对应路径也被删除(类似hive内部表)</span><br><span class="line"></span><br><span class="line">//在write option指定path, drop table删除表之后，数据对应路径不会被删除, 数据被保留, 类似hive外部表</span><br><span class="line">//在这种方式下, 表的分区信息需要手动同步, 执行sql: MSCK REPAIR TABLE tableName</span><br><span class="line">df.write.mode(&quot;overwrite&quot;).option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;).saveAsTable(&quot;test_zhou.user1&quot;)</span><br><span class="line">spark.table(&quot;test_zhou.user1&quot;).show</span><br><span class="line">spark.sql(&quot;drop table test_zhou.user1&quot;)   </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="bucket">Bucket</span></h3><p>分桶只能作用与持久化的表 Bucketing and sorting are applicable only to persistent tables</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.fs.Path</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.expressions.Window</span><br><span class="line"></span><br><span class="line">object DatasetPartitionExample &#123;</span><br><span class="line"></span><br><span class="line">  case class Person(age:Long, name:String)</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .appName(&quot;Spark SQL basic example&quot;)</span><br><span class="line">      .master(&quot;local[3]&quot;)</span><br><span class="line">      .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)</span><br><span class="line">//      .config(&quot;spark.sql.catalogImplementation&quot;, &quot;hive&quot;)</span><br><span class="line">      .enableHiveSupport()  //&quot;spark.sql.catalogImplementation&quot;  &quot;hive&quot;</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.hadoopConfiguration.addResource(new Path(&quot;/Users/zhouqingfeng/Desktop/mydirect/github/distributedStudy/spark/sourcecode/projects/spark_all_test/src/main/resources/hive-site.xml&quot;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    //sql.functions</span><br><span class="line">    import org.apache.spark.sql.functions._</span><br><span class="line">    val simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Michael&quot;, &quot;Sales&quot;, 4600),</span><br><span class="line">      (&quot;Robert&quot;, &quot;Sales&quot;, 4100),</span><br><span class="line">      (&quot;Maria&quot;, &quot;Finance&quot;, 3000),</span><br><span class="line">      (&quot;James&quot;, &quot;Sales&quot;, 3000),</span><br><span class="line">      (&quot;Scott&quot;, &quot;Finance&quot;, 3300),</span><br><span class="line">      (&quot;Jen&quot;, &quot;Finance&quot;, 3900),</span><br><span class="line">      (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000),</span><br><span class="line">      (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000),</span><br><span class="line">      (&quot;Saif&quot;, &quot;Sales&quot;, 4100)</span><br><span class="line">    )</span><br><span class="line">    val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    df.write</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line">    </span><br><span class="line">    //按照分桶个数和分桶列 进行分桶</span><br><span class="line">    df.write</span><br><span class="line">      .bucketBy(3, &quot;department&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line"></span><br><span class="line">		//分区</span><br><span class="line">    df.write</span><br><span class="line">      .partitionBy(&quot;department&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .format(&quot;parquet&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">		//分区之后 分桶</span><br><span class="line">    df.write</span><br><span class="line">      .partitionBy(&quot;department&quot;)</span><br><span class="line">      .bucketBy(3, &quot;salary&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .format(&quot;parquet&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">    df.write</span><br><span class="line">      .partitionBy(&quot;department&quot;)</span><br><span class="line">      .bucketBy(3, &quot;salary&quot;)</span><br><span class="line">      .mode(&quot;overwrite&quot;)</span><br><span class="line">      .format(&quot;csv&quot;)</span><br><span class="line">      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)</span><br><span class="line">      .saveAsTable(&quot;test_zhou.emp&quot;)</span><br><span class="line">      </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4><span id="分桶划分">分桶划分</span></h4><p>计算桶字段hash值， 然后mod取模， 如果结果为负，再加上桶数(保证结果为正)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># value为根据hash算法计算的hash值, n为桶数</span><br><span class="line">b = value mod n</span><br><span class="line">if b &lt; 0:</span><br><span class="line">  b = (b + n) mod n</span><br><span class="line"></span><br><span class="line"># 测试</span><br><span class="line">from pyspark.sql.functions import hash, col, expr</span><br><span class="line">(</span><br><span class="line">  spark.range(100) # this will create a DataFrame with one column id</span><br><span class="line">  .withColumn(&quot;hash&quot;, hash(col(&quot;id&quot;)))</span><br><span class="line">  .withColumn(&quot;bucket&quot;, expr(&quot;pmod(hash, 8)&quot;))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4><span id="分桶测试">分桶测试</span></h4><pre><code>import org.apache.spark.sql.functions._
val simpleData = Seq((&quot;James&quot;, &quot;Sales&quot;, 3000),
  (&quot;Michael&quot;, &quot;Sales&quot;, 4600),
  (&quot;Robert&quot;, &quot;Sales&quot;, 4100),
  (&quot;Maria&quot;, &quot;Finance&quot;, 3000),
  (&quot;James&quot;, &quot;Sales&quot;, 3000),
  (&quot;Scott&quot;, &quot;Finance&quot;, 3300),
  (&quot;Jen&quot;, &quot;Finance&quot;, 3900),
  (&quot;Jeff&quot;, &quot;Marketing&quot;, 3000),
  (&quot;Kumar&quot;, &quot;Marketing&quot;, 2000),
  (&quot;Saif&quot;, &quot;Sales&quot;, 4100)
)
val df = simpleData.toDF(&quot;employee_name&quot;, &quot;department&quot;, &quot;salary&quot;)

        //分区之后 分桶
df.write
      .partitionBy(&quot;department&quot;)
      .bucketBy(3, &quot;salary&quot;)
      .mode(&quot;overwrite&quot;)
      .format(&quot;parquet&quot;)
      .option(&quot;path&quot;,&quot;/tmp/test/spark_ext&quot;)
      .saveAsTable(&quot;test_zhou.emp&quot;)

# 桶号计算方式：如何计算每行数据所属的桶id
spark.table(&quot;test_zhou.emp&quot;).withColumn(&quot;hash&quot;, hash(col(&quot;salary&quot;))).withColumn(&quot;bucket&quot;, expr(&quot;pmod(hash, 3)&quot;)).orderBy(&quot;department&quot;).withColumn(&quot;mod&quot;, expr(&quot;mod(hash, 3)&quot;)).orderBy(&quot;department&quot;).show
</code></pre>
<p>结果产生三个分区，每个分区最多产生三个桶(可能少于3)，桶号可通过文件名或计算得到</p>
<p>三个分区：</p>
<p>对应三个目录</p>
<img src="/2023/07/04/bigdata/spark/spark3/20230704_spark_dateframe/截屏2023-07-07 17.43.39.png">



<p>文件名查看桶号：</p>
<p>最后面的00001、00002表示桶号</p>
<img src="/2023/07/04/bigdata/spark/spark3/20230704_spark_dateframe/截屏2023-07-07 17.44.01.png">



<img src="/2023/07/04/bigdata/spark/spark3/20230704_spark_dateframe/截屏2023-07-07 17.45.02.png">





<p>桶号计算方式如上，查询结果如下，bucket列对应桶号</p>
<img src="/2023/07/04/bigdata/spark/spark3/20230704_spark_dateframe/截屏2023-07-07 17.44.15.png">



<h4><span id="分桶目的">分桶目的</span></h4><p>1、避免在join和聚合查询中，产生shuffle操作</p>
<p>2、数据过滤查询中，应用桶裁剪，减少数据IO开销 (io读取和传输)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">From: https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53</span><br><span class="line">分桶优势：</span><br><span class="line">There are two main areas where bucketing can help, the first one is to avoid shuffle in queries with joins and aggregations, the second one is to reduce the I/O with a feature called bucket pruning.</span><br></pre></td></tr></table></figure>



<h5><span id="聚合查询shuffle-规避测试"><strong>聚合查询shuffle 规避测试</strong></span></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 同一份数据 test_zhou.emp 分别按照 department、salary 进行分桶，生成另外两张表</span><br><span class="line">spark.table(&quot;test_zhou.emp&quot;).write.bucketBy(3, &quot;department&quot;).saveAsTable(&quot;test_zhou.emp1&quot;)</span><br><span class="line">spark.table(&quot;test_zhou.emp&quot;).write.bucketBy(3, &quot;salary&quot;).saveAsTable(&quot;test_zhou.emp2&quot;)</span><br><span class="line"></span><br><span class="line">#对这两张表分别执行 sum(salary) group by department 聚合，</span><br><span class="line">#可以看到test_zhou.emp2 的执行计划比test_zhou.emp1 多了一个exchange (数据产生shuffle操作)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select sum(salary) from test_zhou.emp1 group by department&quot;).explain()</span><br><span class="line"></span><br><span class="line">== Physical Plan ==</span><br><span class="line">AdaptiveSparkPlan isFinalPlan=false</span><br><span class="line">+- HashAggregate(keys=[department#45], functions=[sum(salary#44)])</span><br><span class="line">   +- HashAggregate(keys=[department#45], functions=[partial_sum(salary#44)])</span><br><span class="line">      +- FileScan parquet spark_catalog.test_zhou.emp1[salary#44,department#45] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/user/hive/warehouse/test_zhou.db/emp1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;salary:int,department:string&gt;, SelectedBucketsCount: 3 out of 3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select sum(salary) from test_zhou.emp2 group by department&quot;).explain()</span><br><span class="line"></span><br><span class="line">== Physical Plan ==</span><br><span class="line">AdaptiveSparkPlan isFinalPlan=false</span><br><span class="line">+- HashAggregate(keys=[department#54], functions=[sum(salary#53)])</span><br><span class="line">   +- Exchange hashpartitioning(department#54, 200), ENSURE_REQUIREMENTS, [plan_id=120]</span><br><span class="line">      +- HashAggregate(keys=[department#54], functions=[partial_sum(salary#53)])</span><br><span class="line">         +- FileScan parquet spark_catalog.test_zhou.emp2[salary#53,department#54] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://localhost:9000/user/hive/warehouse/test_zhou.db/emp2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;salary:int,department:string&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4><span id="sql写法">SQL写法</span></h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> users_bucketed_by_name(</span><br><span class="line">  name STRING,</span><br><span class="line">  favorite_color STRING,</span><br><span class="line">  favorite_numbers <span class="keyword">array</span><span class="operator">&lt;</span><span class="type">integer</span><span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">USING</span> parquet</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(name) <span class="keyword">INTO</span> <span class="number">42</span> BUCKETS;</span><br><span class="line"></span><br><span class="line"># 和分区整合</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> users_bucketed_and_partitioned(</span><br><span class="line">  name STRING,</span><br><span class="line">  favorite_color STRING,</span><br><span class="line">  favorite_numbers <span class="keyword">array</span><span class="operator">&lt;</span><span class="type">integer</span><span class="operator">&gt;</span></span><br><span class="line">) <span class="keyword">USING</span> parquet</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (favorite_color)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(name) SORTED <span class="keyword">BY</span> (favorite_numbers) <span class="keyword">INTO</span> <span class="number">42</span> BUCKETS;</span><br></pre></td></tr></table></figure>



<h4><span id="其他">其他</span></h4><p>（1）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#查看分桶信息</span><br><span class="line">spark.sql(&quot;DESCRIBE EXTENDED table_name&quot;).show(n=100, true)</span><br><span class="line"></span><br><span class="line">#默认Spark最后输出结果的每个task针对每个桶都会产生一个文件(如果对应桶文件有数据)</span><br><span class="line">#比如200个分区，200个桶，最终产生文件个数 200*200</span><br><span class="line"></span><br><span class="line">#为避免小文件太多，执行操作如下，先进行repartition, 再分桶，每个分区刚好对应一个桶</span><br><span class="line">  df</span><br><span class="line">  .repartition(200, &quot;created_year&quot;,expr(&quot;pmod(hash(user_id), 200)&quot;))</span><br><span class="line">  .write</span><br><span class="line">  .mode(saving_mode)</span><br><span class="line">  .partitionBy(&quot;created_year&quot;)</span><br><span class="line">  .bucketBy(200, &quot;user_id&quot;)</span><br><span class="line">  .option(&quot;path&quot;, output_path)</span><br><span class="line">  .saveAsTable(table_name)</span><br><span class="line"></span><br><span class="line">#如果是单分区，200个桶</span><br><span class="line">  df.repartition(expr(&quot;pmod(hash(user_id), 200)&quot;))</span><br><span class="line">  .write</span><br><span class="line">  .mode(saving_mode)  # append/overwrite</span><br><span class="line">  .bucketBy(200, &#x27;user_id&#x27;)</span><br><span class="line">  .option(&quot;path&quot;, output_path)</span><br><span class="line">  .saveAsTable(table_name)</span><br><span class="line">  </span><br><span class="line">  等价于</span><br><span class="line">  df.repartition(200, ‘user_id’)</span><br><span class="line">  </span><br><span class="line">  等价于</span><br><span class="line">  insert into B  select * from A distribute by pmod(hash(user_id), 200)</span><br><span class="line">  </span><br></pre></td></tr></table></figure>

<p>（2）</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wypblog/article/details/108786770">和hive分桶区别</a></p>
<p>1、Spark 分桶和 Hive 分桶采用不同的 hash 算法。Hive 用的是 HiveHash，而 Spark 用的是 Murmur3，所以数据的分布是不一样的</p>
<p>2、Hive 在生成分桶的时候会额外进行一个 Reduce 操作，以保证相同分桶的数据都存储在一个文件中。而 Spark SQL 在写分桶文件时不需要 Shuffle 操作，这样就会导致每个分桶最多产生 M 个文件(task个数)</p>
<p>（spark的每个task针对每个桶都会产生一个文件(0个或1个)，结果是每个桶会有多个文件；而hive会自动进行reduce，将多个分桶文件合并）</p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53">spark 分桶详解，非常非常清晰、全面</a></p>
<p>（3）</p>
<p>SQL  DML语句中，也有distribute by A 和 cluster by A ( &#x3D; distribute by A sort by A)</p>
<p>用法大致是: select * from tb1 distribute by A 这里指的是对数据重分区，基本等同于repartition A</p>
<p>这和分桶是有所不同的</p>
<h3><span id="partition">Partition</span></h3><p>分区 </p>
<p>这里的分区指的是一个分区对应一个输出子目录</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark3/20230704_spark_dateframe/" data-id="clk6akm6k0003lk9ahzhqdpip" data-title="20230704_spark_dateframe" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/hadoop/hadoop_env/hadoop本地环境搭建" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="article-date">
  <time class="dt-published" datetime="2023-07-04T02:13:53.000Z" itemprop="datePublished">2023-07-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">hadoop本地环境搭建</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#env">Env</a></li>
<li><a href="#install-hadoop-277">Install hadoop 2.7.7</a><ul>
<li><a href="#start">Start</a></li>
<li><a href="#others">Others</a></li>
</ul>
</li>
<li><a href="#install-mysql">Install mysql</a></li>
<li><a href="#install-hive">Install hive</a><ul>
<li><a href="#start-1">Start</a></li>
<li><a href="#references">References</a></li>
</ul>
</li>
<li><a href="#install-hbase">Install HBase</a><ul>
<li><a href="#pseudo-distributed-local-install">Pseudo-Distributed Local Install</a></li>
<li><a href="#simple-usage">Simple Usage</a></li>
<li><a href="#phoenix">phoenix</a></li>
</ul>
</li>
<li><a href="#install-zookeeper">Install Zookeeper</a></li>
<li><a href="#install-kafka">Install Kafka</a></li>
<li><a href="#install-spark">Install Spark</a><ul>
<li><a href="#spark-local">Spark local</a></li>
<li><a href="#spark-yarn-deploy">Spark yarn deploy</a></li>
<li><a href="#spark-to-hbase">spark to  hbase</a></li>
</ul>
</li>
<li><a href="#hive-to-or-from-hbase">Hive to (or from) Hbase</a><ul>
<li><a href="#hive-to-hbase">Hive to hbase</a></li>
<li><a href="#reference">Reference</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="env">Env</span></h2><p>macOS 12.4</p>
<p>java version “1.8.0_102”</p>
<p>Spark3.4.1  hadoop2.7.7</p>
<h2><span id="install-hadoop-277">Install hadoop 2.7.7</span></h2><p>&#x2F;Users&#x2F;***&#x2F;Desktop&#x2F;software&#x2F;hadoop-2.7.7 </p>
<p>Site:<a target="_blank" rel="noopener" href="http://apache.communilink.net/hadoop/common/hadoop-2.7.7/">http://apache.communilink.net/hadoop/common/hadoop-2.7.7/</a></p>
<p>ref1:<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/de7eb61c983a">https://www.jianshu.com/p/de7eb61c983a</a></p>
<p>ref2:<a target="_blank" rel="noopener" href="https://www.cnblogs.com/landed/p/6831758.html">https://www.cnblogs.com/landed/p/6831758.html</a></p>
<h3><span id="start">Start</span></h3><p>.&#x2F;sbin&#x2F;start-all.sh</p>
<p> .&#x2F;sbin&#x2F;mr-jobhistory-daemon.sh  start historyserver</p>
<p>localhost:50070</p>
<p>localhost:8088</p>
<h3><span id="others">Others</span></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># turn off hdfs safemode</span><br><span class="line">hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure>



<h2><span id="install-mysql">Install mysql</span></h2><p>使用docker安装简单更方便</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql-8.0.12-macos10.13-x86_64.dmg</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MYSQL_HOME=/usr/local/mysql</span><br><span class="line">export MYSQL_HOME</span><br><span class="line"></span><br><span class="line">PATH=.:$MYSQL_HOME/bin:$PATH</span><br><span class="line">export PATH</span><br></pre></td></tr></table></figure>

<p>mysql –version</p>
<p>mysql -uroot -p</p>
<h2><span id="install-hive">Install hive</span></h2><p>cd &#x2F;Users&#x2F;***&#x2F;Desktop&#x2F;software&#x2F;apache-hive-2.3.3-bin</p>
<p>download mirror: <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/">https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/</a></p>
<p>vi conf&#x2F;hive-site.xml</p>
<pre><code>#使用MySQL中的hive数据库，如果没有就创建一个
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
      &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
      &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
    &lt;/property&gt;

#连接mysql驱动
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
      &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
    &lt;/property&gt;

#用户名
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
      &lt;value&gt;hive&lt;/value&gt;
      &lt;description&gt;username to use against metastore database&lt;/description&gt;
    &lt;/property&gt;

#密码:
    &lt;property&gt;
      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
      &lt;value&gt;hive&lt;/value&gt;
      &lt;description&gt;password to use against metastore database&lt;/description&gt;
    &lt;/property&gt;

#日志目录
    &lt;property&gt;
      &lt;name&gt;hive.querylog.location&lt;/name&gt;
      &lt;value&gt;/Users/zhouqingfeng/Desktop/software/tmp/hive/querylog&lt;/value&gt;
      &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;
    &lt;/property&gt;


&lt;property&gt;
  &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
  &lt;value&gt;/Users/**/Desktop/software/tmp/hive/scratchdir&lt;/value&gt;
  &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
  &lt;value&gt;/Users/**/Desktop/software/tmp/hive/resoucesDir&lt;/value&gt;
  &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
  &lt;value&gt;/Users/**/Desktop/software/tmp/hive/operationlog&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<hr>
<h3><span id="start">Start</span></h3><p>启动hive metastore:</p>
<p>首先启动.&#x2F;hive –service metastore</p>
<p>hive client:</p>
<p>.&#x2F;hive</p>
<p>使用hiveserver2，然后使用beeline链接hive</p>
<p>.&#x2F;hive –service hiveserver2  |  hiveserver2   | lsof -i tcp:10000</p>
<p>beeline -u “jdbc:hive2:&#x2F;&#x2F;localhost:10000” </p>
<p>beeline -u “jdbc:hive2:&#x2F;&#x2F;192.168.43.65:10000”</p>
<p>hiveserver2 web url: <a target="_blank" rel="noopener" href="http://localhost:10002/">http://localhost:10002/</a></p>
<p>set hive.execution.engine &#x3D; mr;</p>
<p>beeline远程客户端配置：</p>
<p>修改conf&#x2F;hive-env.sh: HADOOP_HOME&#x3D;&#x2F;opt&#x2F;hadoop-2.7.7</p>
<p>修改hadoop_home配置文件：&#x2F;opt&#x2F;hadoop-2.7.7&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh</p>
<h3><span id="references">References</span></h3><p><a target="_blank" rel="noopener" href="http://lxw1234.com/archives/2016/01/600.htm">hiveserver2用户名和密码设置</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qiaojialin/article/details/55506439">https://blog.csdn.net/qiaojialin/article/details/55506439</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/micrari/p/7067968.html">https://www.cnblogs.com/micrari/p/7067968.html</a></p>
<h2><span id="install-hbase">Install  HBase</span></h2><h3><span id="pseudo-distributed-local-install">Pseudo-Distributed Local Install</span></h3><p>Hbase-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export  JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_102.jdk/Contents/Home </span><br></pre></td></tr></table></figure>

<p>Hbase-site.xml</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;hbase.rootdir&lt;/name&gt;
 &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>bin&#x2F;start-hbase.sh</p>
<p>bin&#x2F;stop-hbase.sh</p>
<p>Ref:<a target="_blank" rel="noopener" href="https://hbase.apache.org/book.html">https://hbase.apache.org/book.html</a></p>
<h3><span id="simple-usage">Simple Usage</span></h3><p>create  ‘test’, ‘cf1’</p>
<p>list  ‘test’  &#x2F;&#x2F;Use the <code>list</code> command to confirm your table exists</p>
<p>describe ‘test’</p>
<p>&#x2F;&#x2F;insert</p>
<p>put ‘test’,  ‘row1’,  ‘cf1:c1’,  ‘c1’</p>
<p>put ‘test’, ‘row1’, ‘cf1:a’, ‘value1’ </p>
<p>put ‘test’,  ‘row1’,   ‘cf1:c2’,  ‘c2’</p>
<p>put ‘test’,  ‘row2’,   ‘cf1:c3’,  ‘value1’</p>
<p>&#x2F;&#x2F;select</p>
<p>scan ‘test’</p>
<p>get ‘test’,  ‘row1’</p>
<p>get ‘test’,  ‘row1’,  ‘cf1’</p>
<p>get ‘test’,  ‘row1’,  ‘cf1:c1’</p>
<h3><span id="phoenix">phoenix</span></h3><h2><span id="install-zookeeper">Install Zookeeper</span></h2><p><a target="_blank" rel="noopener" href="http://apache.website-solution.net/zookeeper/">http://apache.website-solution.net/zookeeper/</a></p>
<h2><span id="install-kafka">Install Kafka</span></h2><h2><span id="install-spark">Install Spark</span></h2><h3><span id="spark-local">Spark  local</span></h3><h3><span id="spark-yarn-deploy">Spark  yarn deploy</span></h3><p>1、<a href="https://qingfengzhou.github.io/2023/07/04/bigdata/spark/spark_env/spark%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/#spark-envsh">转spark 环境搭建</a></p>
<p>2、配置 spark history server:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vi conf/spark-defaults.conf</span><br><span class="line"></span><br><span class="line">spark.driver.memory              2g</span><br><span class="line"></span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://localhost:9000/tmp/spark-history-server</span><br><span class="line">spark.history.fs.logDirectory    hdfs://localhost:9000/tmp/spark-history-server</span><br><span class="line"></span><br><span class="line">spark.yarn.jars                  hdfs://localhost:9000/spark3/jars/*</span><br><span class="line">spark.yarn.historyServer.address localhost:18080</span><br><span class="line"></span><br><span class="line">./sbin/start-history-server.sh </span><br></pre></td></tr></table></figure>

<p>3、测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master  yarn \</span><br><span class="line">    --deploy-mode client \</span><br><span class="line">    --driver-memory 1g \</span><br><span class="line">    --executor-memory 1g \</span><br><span class="line">   --num-executors  3 \</span><br><span class="line">   --executor-cores 1 \</span><br><span class="line">   examples/jars/spark-examples*.jar \</span><br><span class="line">   10</span><br></pre></td></tr></table></figure>



<h3><span id="spark-to-hbase">spark  to  hbase</span></h3><p>ETL</p>
<p>mysql  -&gt;  hive -&gt; spark sql  ?</p>
<p>machine learning  ?</p>
<p>spark + machine learning  ?</p>
<p>spark + AI ?</p>
<h2><span id="hive-to-or-from-hbase">Hive to (or from) Hbase</span></h2><h3><span id="hive-to-hbase">Hive to hbase</span></h3><p>create hive:</p>
<p>CREATE  TABLE tb2 (<br>name  string,<br>age int,<br>addr string<br>)<br>COMMENT ‘test table’</p>
<p>ROW FORMAT DELIMITED</p>
<p>FIELDS TERMINATED BY ‘,’</p>
<p>STORED AS textfile;</p>
<p>load data to hive:</p>
<p>hive  -e  “LOAD DATA LOCAL INPATH ‘&#x2F;Users&#x2F;zhouqingfeng&#x2F;Desktop&#x2F;software&#x2F;tmp&#x2F;hive&#x2F;definedata&#x2F;tb2&#x2F;tb2.txt’ OVERWRITE INTO TABLE default.tb2” </p>
<p>create another hive table:</p>
<p>hbase: create  ‘test_tb2’,  ‘cf’</p>
<p>create external table tb2_cp (<br>name  string,<br>age int,<br>addr string<br>) </p>
<p>STORED BY ‘org.apache.hadoop.hive.hbase.HBaseStorageHandler’</p>
<p>WITH SERDEPROPERTIES (“hbase.columns.mapping” &#x3D; “:key,  cf:age,  cf:addr”)</p>
<p>TBLPROPERTIES (“hbase.table.name” &#x3D; “test_tb2”);</p>
<p>insert into table:  this is slow because of mapreduce job</p>
<p>insert into table  tb2_cp select * from tb2;  </p>
<h3><span id="reference">Reference</span></h3><p><a target="_blank" rel="noopener" href="http://bigdataprogrammers.com/data-migration-from-hive-to-hbase/">hive to hbase</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/04/bigdata/hadoop/hadoop_env/hadoop%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" data-id="cljnns25s0000sq9aca136t58" data-title="hadoop本地环境搭建" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ai-llm-langchain/">ai/llm/langchain</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/ai-llm-prompt/">ai/llm/prompt</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-flink-flink-doc/">bigdata/flink/flink_doc</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-kafka/">bigdata/kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-env/">bigdata/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-tune/">bigdata/spark/spark_tune</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-sql/">bigdata/sql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-concurrent/">java/concurrent</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-designpattern/">java/designpattern</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-jvm/">java/jvm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/llm/" rel="tag">llm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql/" rel="tag">sql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/flink/" style="font-size: 10px;">flink</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/java/" style="font-size: 16.67px;">java</a> <a href="/tags/kafka/" style="font-size: 10px;">kafka</a> <a href="/tags/llm/" style="font-size: 13.33px;">llm</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sql/" style="font-size: 10px;">sql</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/27/ai/llm/prompt/wu_en_da_prompt/">wu_en_da_prompt</a>
          </li>
        
          <li>
            <a href="/2023/07/21/ai/llm/langchain/langchain_usage_20230721/">langchain_usage_20230721</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/kafka/kafka_basic/">kafka_basic</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/spark/spark_tune/spark_conf/">spark_conf</a>
          </li>
        
          <li>
            <a href="/2023/07/21/bigdata/spark/spark_tune/spark_shuffle_usage/">spark_shuffle_usage</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>