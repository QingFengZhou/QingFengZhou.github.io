<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://qingfengzhou.github.io/page/3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qingfengzhou.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/nosql/hbase/HBase概览" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/" class="article-date">
  <time class="dt-published" datetime="2023-07-03T07:44:53.000Z" itemprop="datePublished">2023-07-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/">HBase概览</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#hbase">HBase</a><ul>
<li><a href="#one">One</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="hbase">HBase</span></h2><h3><span id="one">One</span></h3><img src="/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/截屏2023-07-03 15.49.39.png">


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/03/bigdata/nosql/hbase/HBase%E6%A6%82%E8%A7%88/" data-id="cljmkav790000qj9ack1hbz8f" data-title="HBase概览" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_file_read_partition_num" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_file_read_partition_num/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:35:32.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_file_read_partition_num/">spark_file_read_partition_num</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h2><p>文件格式: text、csv、json、parquet、orc、avro</p>
<p>压缩格式：snappy、gzip、bzip2、lzo、lz4 (bzip2和lzo支持切分)</p>
<p>spark 读取数据分区个数计算：</p>
<p>影响因子：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">block.size(默认128m)、openCostInBytes(文件打开开销，比如打开一个parquet文件需要4m)、</span><br><span class="line">minPartitionNum(分区个数，最少为2)、totalBytes(总内存开销:文件大小+openCostInBytes)、</span><br><span class="line">bytesPerCore(每个core的内存开心 totalBytes/core num)</span><br><span class="line">举例：10个核，block.size 128m，parquet文件总大小 = 113.918m，8个文件，</span><br><span class="line">计算最大分片大小maxSplitBytes 过程如下, 会读snappy.parquet进行切分：</span><br><span class="line"></span><br><span class="line">defaultMaxSplitBytes = 128MB</span><br><span class="line">openCostInBytes = 4MB</span><br><span class="line">minPartitionNum = max(10, 2) = 10</span><br><span class="line">totalBytes = 113.918 + 8 * 4MB = 145.918MB</span><br><span class="line">bytesPerCore = 145.918MB / 10 = 14.5918MB</span><br><span class="line">maxSplitBytes = 14.5918MB = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</span><br></pre></td></tr></table></figure>

<p>测试：spark读取parquet 会对自动对小文件合并，大文件拆分，</p>
<p>3个executor(3核)读取4.46m文件对自动进行切分为2个分片，读取54kb的parquet文件不会对文件切分</p>
<p>参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/544905929">parquet 源数据分区个数计算剖析</a>   <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/490729788">文件压缩格式</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_file_read_partition_num/" data-id="cljllibc90003029a6lrz68g3" data-title="spark_file_read_partition_num" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_file_combine" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_file_combine/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:30:09.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_file_combine/">spark_file_combine</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- toc -->

<ul>
<li><a href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6">小文件合并</a><ul>
<li><a href="#%E4%BD%BF%E7%94%A8-spark-sql">使用 Spark sql</a><ul>
<li><a href="#%E5%85%B6%E4%BB%96%E6%80%BB%E7%BB%93">其他总结</a></li>
</ul>
</li>
<li><a href="#spark-code">Spark code</a></li>
<li><a href="#hive-insert-overwrite">hive insert  overwrite</a></li>
<li><a href="#hive-%E5%88%86%E7%89%87%E5%92%8C%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%E5%8F%82%E6%95%B0%E8%B0%83%E8%8A%82">Hive 分片和文件合并参数调节</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h2><span id="小文件合并">小文件合并</span></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">create external table if not exists emp_ext(</span><br><span class="line">    	empno int,</span><br><span class="line">	ename string,</span><br><span class="line">	job string,</span><br><span class="line">	mgr int,</span><br><span class="line">	hiredate string,</span><br><span class="line">	sal double,</span><br><span class="line">	comm double,</span><br><span class="line">	deptno int</span><br><span class="line">) row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">LOCATION &#x27;/user/hive/warehouse/emp&#x27;;</span><br><span class="line"></span><br><span class="line">CREATE EXTERNAL TABLE parquet_test1 (value string) </span><br><span class="line">STORED AS PARQUET </span><br><span class="line">LOCATION &#x27;/tmp/test_out/parquet_big/&#x27;</span><br><span class="line">TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;SNAPPY&quot;)</span><br><span class="line">;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3><span id="使用-spark-sql">使用 Spark sql</span></h3><p>思路：创建临时表，通过查看文件目录大小判断分区个数，通过distribute by num_partitions重分区 写入到临时表，判断临时表和源表数据内容是否一致，如果一致，删除源表，重命名临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># insert overwrite源表 报错</span><br><span class="line">insert overwrite table parquet_test1 select * from parquet_test1;</span><br><span class="line"></span><br><span class="line">create table parquet_test2 like  parquet_test1 LOCATION &#x27;/tmp/test_out/parquet_big3/&#x27;</span><br><span class="line"></span><br><span class="line">insert overwrite table parquet_test2 select * from parquet_test1 distribute by floor(rand()*2);</span><br><span class="line"></span><br><span class="line"># 判断源表parquet_test1 和 parquet_test2是否数据量一致</span><br><span class="line"># 删除源表parquet_test1</span><br><span class="line">drop table parquet_test1;</span><br><span class="line"></span><br><span class="line">ALTER TABLE parquet_test2 RENAME TO parquet_test1</span><br></pre></td></tr></table></figure>

<h4><span id="其他总结">其他总结</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(1) 对于原始数据进行按照分区字段进行shuffle，可以规避小文件问题。但有可能引入数据倾斜的问题；</span><br><span class="line">(2) sql中引入 distribute by ，指定分区字段或分区表达式</span><br><span class="line">(3) 已知倾斜key的情况，将数据分为两部分处理，倾斜部分按rand()函数 重分区，未倾斜部分常规处理</span><br><span class="line">(4) 对于Spark 2.4 以上版本的用户，sql中 可以使用HINT提示</span><br><span class="line">    insert into select /*+ REPARTITION(2) */  id,name  from tb1 where id &gt; 0</span><br><span class="line">    insert into select /*+ COALESCE(2) */  id,name  from tb1 where id &gt; 0</span><br><span class="line">(5) spark3.0以上开启自适应查询执行：</span><br><span class="line">spark.sql.adaptive.enabled=true;</span><br><span class="line">spark.sql.adaptive.coalescePartitions.enabled=true;</span><br><span class="line">spark.sql.adaptive.coalescePartitions.parallelismFirst = false;</span><br><span class="line">spark.sql.adaptive.advisoryPartitionSizeInBytes = 64m;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3><span id="spark-code">Spark code</span></h3><p>思路：数据写入临时文件目录，判断临时文件目录大小设定重分区个数，数据写入正式文件目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.fs.&#123;FileSystem, Path&#125;</span><br><span class="line"></span><br><span class="line">    nowDF.persist()</span><br><span class="line">    val tempPath = &quot;/nativeinfopath/&quot;</span><br><span class="line">    nowDF.write.mode(&quot;overwrite&quot;).parquet(tempPath)</span><br><span class="line">    </span><br><span class="line">    val fs = FileSystem.get(sc.hadoopConfiguration)</span><br><span class="line">    val dirSize = fs.getContentSummary(new Path(tempPath)).getLength</span><br><span class="line">    val fileNum = dirSize / (128 * 1024 * 1024) </span><br><span class="line">    </span><br><span class="line">    val regularPath = &quot;&quot;</span><br><span class="line">    nowDF.coalesce(fileNum.toInt).write.mode(&quot;overwrite&quot;).parquet(regularPath)</span><br><span class="line">    nowDF.unpersist()</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h3><span id="hive-insert-overwrite">hive  insert  overwrite</span></h3><p>使用hive  insert  overwrite  (自动小文件合并，不需要创建临时表，分区表分区也适用)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE parquet_test1 (value string) </span><br><span class="line">STORED AS PARQUET </span><br><span class="line">LOCATION &#x27;/tmp/test_out/parquet_big/&#x27;</span><br><span class="line">TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;SNAPPY&quot;)</span><br><span class="line">;</span><br><span class="line">set hive.execution.engine = mr;</span><br><span class="line">insert overwrite table parquet_test1 select * from parquet_test1;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--分区表</span><br><span class="line">day=`date  +&quot;%Y%m%d&quot; -d  &quot;-1 days&quot;`</span><br><span class="line">hive  -d  day=$&#123;day&#125;  -e  &quot;use gps; </span><br><span class="line">                           alter table gpsinfo  add partition (day=$&#123;day&#125;);</span><br><span class="line">                           insert overwrite table gpsinfo partition(day)  select * from gpsinfo where day=$&#123;day&#125;;&quot; </span><br></pre></td></tr></table></figure>

<h3><span id="hive-分片和文件合并参数调节">Hive 分片和文件合并参数调节</span></h3><p>hive 分片大小和自动小文件合并参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//每个Map最大输入大小(这个值决定了合并后文件的数量)</span><br><span class="line">set mapred.max.split.size=256000000;  </span><br><span class="line">//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span><br><span class="line">set mapred.min.split.size.per.node=100000000;</span><br><span class="line">//一个rack下split的至少的大小(这个值决定了rack上的文件是否需要合并)  </span><br><span class="line">set mapred.min.split.size.per.rack=100000000;</span><br><span class="line">//执行Map前进行小文件合并</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; </span><br><span class="line">//设置map端输出进行合并，默认为true</span><br><span class="line">set hive.merge.mapfiles = true</span><br><span class="line">//设置reduce端输出进行合并，默认为false</span><br><span class="line">set hive.merge.mapredfiles = true</span><br><span class="line">//设置合并文件的大小</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000</span><br><span class="line">//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</span><br><span class="line">set hive.merge.smallfiles.avgsize=16000000</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_file_combine/" data-id="cljllibc40001029a724i9b3s" data-title="spark_file_combine" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_minio" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_minio/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:29:29.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_minio/">spark_minio</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="Mino"><a href="#Mino" class="headerlink" title="Mino"></a>Mino</h3><p><a target="_blank" rel="noopener" href="https://min.io/docs/minio/macos/operations/install-deploy-manage/deploy-minio-single-node-single-drive.html#connect-to-the-minio-deployment">官方文档</a></p>
<p>本地安装Web控制台: <a target="_blank" rel="noopener" href="http://192.168.43.65:9011/">http://192.168.43.65:9011/</a></p>
<p>最小化安装</p>
<p>server start:  minio server –address “:9001”  –console-address :9011</p>
<p>client install and start: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 为搭建的s3 服务设置别名 local</span><br><span class="line">api: http://192.168.43.65:9001  </span><br><span class="line">access key: 4PtooCkozQcSIT5QLJFR  (创建的某个用户对应的key)</span><br><span class="line">secret key: 4PtooCkozQcSIT5QLJFR</span><br><span class="line"></span><br><span class="line">mc alias set local  http://192.168.43.65:9001 4PtooCkozQcSIT5QLJFR  4PtooCkozQcSIT5QLJFR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#查看文件列表</span><br><span class="line">mc ls   local/bucktest111</span><br><span class="line"></span><br><span class="line">#查看某个文件</span><br><span class="line">mc cat   local/bucktest111/tmp/pingan_art0629</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="Spark-读写minio"><a href="#Spark-读写minio" class="headerlink" title="Spark 读写minio"></a>Spark 读写minio</h4><p>Spark 3.4.1  </p>
<p>minio  version RELEASE.2023-06-23T20-26-00Z, 依赖版本一定要匹配，不然会读取出错 </p>
<p>（1） 使用minio jar包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;!-- https://mvnrepository.com/artifact/io.minio/spark-select --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;io.minio&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;spark-select_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-aws&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;3.3.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;com.google.guava&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;guava&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;23.6-jre&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<p>Test Code:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">import com.amazonaws.SDKGlobalConfiguration</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object MinioExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">//    System.setProperty(SDKGlobalConfiguration.DISABLE_CERT_CHECKING_SYSTEM_PROPERTY, &quot;true&quot;)</span><br><span class="line"></span><br><span class="line">    lazy val spark = SparkSession.builder().appName(&quot;MinIOTest&quot;).master(&quot;local[*]&quot;).getOrCreate()</span><br><span class="line"></span><br><span class="line">    val s3accessKeyAws = &quot;4PtooCkozQcSIT5QLJFR&quot;</span><br><span class="line">    val s3secretKeyAws = &quot;4PtooCkozQcSIT5QLJFR&quot;</span><br><span class="line">    val connectionTimeOut = &quot;600000&quot;</span><br><span class="line">    val s3endPointLoc: String = &quot;http://192.168.43.65:9001&quot;</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.endpoint&quot;, s3endPointLoc)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.access.key&quot;, s3accessKeyAws)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.secret.key&quot;, s3secretKeyAws)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.connection.timeout&quot;, connectionTimeOut)</span><br><span class="line"></span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;spark.sql.debug.maxToStringFields&quot;, &quot;100&quot;)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.path.style.access&quot;, &quot;true&quot;)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.impl&quot;, &quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot;)</span><br><span class="line">    spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.connection.ssl.enabled&quot;, &quot;false&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val sourceBucket: String = &quot;bucktest111&quot;</span><br><span class="line">    val inputPath: String = s&quot;s3a://$sourceBucket/tmp/credentials2.json&quot;</span><br><span class="line">    val outputPath = s&quot;s3a://$sourceBucket/tmp/credentials4&quot;</span><br><span class="line"></span><br><span class="line">    val df = spark</span><br><span class="line">      .read</span><br><span class="line">      //.format(&quot;minioSelectJSON&quot;) // minioSelectCSV  for csv or &quot;minioSelectJSON&quot; for JSON or &quot;minioSelectParquet&quot; for Parquet</span><br><span class="line">      .json(inputPath)</span><br><span class="line"></span><br><span class="line">    df.show()</span><br><span class="line">    df.write.mode(&quot;overwrite&quot;).json(outputPath)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（2）使用<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/cloud-integration.html">spark官方</a>提供的包，简单、高效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-hadoop-cloud_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_minio/" data-id="cljllibca0004029aamiueeex" data-title="spark_minio" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-bigdata/spark/spark_connect" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/07/02/bigdata/spark/spark_connect/" class="article-date">
  <time class="dt-published" datetime="2023-07-02T15:28:37.000Z" itemprop="datePublished">2023-07-02</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigdata-spark/">bigdata/spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/07/02/bigdata/spark/spark_connect/">spark_connect</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Spark-Connect"><a href="#Spark-Connect" class="headerlink" title="Spark Connect"></a>Spark Connect</h2><p>Env: spark3.4.1 hadoop2.7.7</p>
<h3 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h3><p>Server: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.4.1 --master yarn  --conf spark.sql.catalogImplementation=hive</span><br><span class="line"></span><br><span class="line">./sbin/stop-connect-server.sh </span><br></pre></td></tr></table></figure>

<p>Client：</p>
<p>1、Pyspark:</p>
<p>python最低版本3.9, 需要安装多个依赖包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/pyspark --remote <span class="string">&quot;sc://localhost&quot;</span></span><br></pre></td></tr></table></figure>

<p>2、maven Scala： 注：目前scala client不太稳定，还没正式对外release，使用方式来自stackoverflow</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-connect-client-jvm --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-connect-client-jvm_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-catalyst_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>Test code:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object SparkConnectTest &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val ip = &quot;192.168.43.65&quot;</span><br><span class="line">    val spark = SparkSession.builder().remote(s&quot;sc://$ip&quot;).build()</span><br><span class="line">//    spark.sql(&quot;show databases&quot;).show()</span><br><span class="line">//    spark.sql(&quot;use test_zhou&quot;)</span><br><span class="line">//    spark.sql(&quot;select addr, count(1) cnt from users group by addr&quot;).show()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._</span><br><span class="line"></span><br><span class="line">    val ds1 = spark.read.text(&quot;/tmp/output.txt&quot;)</span><br><span class="line">    val cnt = ds1.count()</span><br><span class="line"></span><br><span class="line">    println(cnt)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id=""><a href="#" class="headerlink" title=""></a></h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/07/02/bigdata/spark/spark_connect/" data-id="cljllibbx0000029aavti3wcr" data-title="spark_connect" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-life/thought/20230629" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/30/life/thought/20230629/" class="article-date">
  <time class="dt-published" datetime="2023-06-30T11:58:06.000Z" itemprop="datePublished">2023-06-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/life-thought/">life/thought</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/30/life/thought/20230629/">20230629</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="One"><a href="#One" class="headerlink" title="One"></a>One</h1><p>What is important ?</p>
<h1 id="Two"><a href="#Two" class="headerlink" title="Two"></a>Two</h1><p>Time flies</p>
<h1 id="Three"><a href="#Three" class="headerlink" title="Three"></a>Three</h1><p>Do the right thing</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/06/30/life/thought/20230629/" data-id="cljiiyeep00029o9ah0lc6tm1" data-title="20230629" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-life/daily/20230630" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/30/life/daily/20230630/" class="article-date">
  <time class="dt-published" datetime="2023-06-30T09:39:44.000Z" itemprop="datePublished">2023-06-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/life-daily/">life/daily</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/30/life/daily/20230630/">20230630</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Morning"><a href="#Morning" class="headerlink" title="Morning"></a>Morning</h2><p>8:10 - 8:35</p>
<p>8:40 - 11:30</p>
<h2 id="loon"><a href="#loon" class="headerlink" title="loon"></a>loon</h2><p>11:30 - 13:30</p>
<h2 id="Afternoon"><a href="#Afternoon" class="headerlink" title="Afternoon"></a>Afternoon</h2><p>13:30 - 18:00<br>18:00 - 18:40</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qingfengzhou.github.io/2023/06/30/life/daily/20230630/" data-id="cljieq5zk0001bt9a4yle9w6t" data-title="20230630" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-flink-flink-doc/">bigdata/flink/flink_doc</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-hadoop-hadoop-env/">bigdata/hadoop/hadoop_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-nosql-hbase/">bigdata/nosql/hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark/">bigdata/spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark3/">bigdata/spark/spark3</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-spark-env/">bigdata/spark/spark_env</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-spark-sparksql-kyuubi/">bigdata/spark/sparksql/kyuubi</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/bigdata-sql/">bigdata/sql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java-concurrent/">java/concurrent</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-daily/">life/daily</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/life-thought/">life/thought</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-ansible/">system/linux/ansible</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-command/">system/linux/command</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-host-monitor/">system/linux/host/monitor</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/system-linux-prometheus/">system/linux/prometheus</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-draw-%E5%9C%A8%E7%BA%BF%E7%94%BB%E5%9B%BE/">tools/draw/在线画图</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-github-hexo/">tools/github/hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools-java-maven/">tools/java/maven</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sql/" rel="tag">sql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/flink/" style="font-size: 10px;">flink</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sql/" style="font-size: 10px;">sql</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/19/java/concurrent/jiketime_java%E5%B9%B6%E5%8F%91/">jiketime_java并发</a>
          </li>
        
          <li>
            <a href="/2023/07/17/bigdata/sql/doc_sql/">doc_sql</a>
          </li>
        
          <li>
            <a href="/2023/07/13/bigdata/flink/flink_doc/flink_sql_20230713/">flink_sql_20230713</a>
          </li>
        
          <li>
            <a href="/2023/07/13/bigdata/spark/spark_env/spark_terminal_tips/">spark_terminal_tips</a>
          </li>
        
          <li>
            <a href="/2023/07/13/bigdata/spark/spark3/20230712_spark_delta_lake/">20230713_delta_lake</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>